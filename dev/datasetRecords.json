[
  {
    "id": 1,
    "canonicalId": "n6l94ql8oga1ggbu1ku80kvc",
    "datasetId": "n6l94ql8oga1ggbu1ku80kvc",
    "doi": "10.17632/trghs22fpg.1",
    "title": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders",
    "description": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/trghs22fpg.1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Taimur Hassan",
            "nameType": "Personal"
          },
          {
            "creatorName": "Muhammad Usman Akram",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Sciences and Technology"
              }
            ]
          },
          {
            "creatorName": "Muhammad Noman Nazir",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Armed Forces Institute of Ophthalmology"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-03-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal Fundus and OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/9049083",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/s10916-018-1078-3",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/8425990",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1016/j.compbiomed.2018.12.015",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/AO.55.000454",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/JOSAA.33.000455",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/7294517",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          }
        ],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "Central Serous Chorioretinopathy"
          },
          {
            "subjectValue": "Cystoid Macular Edema"
          },
          {
            "subjectValue": "Age-Related Macular Degeneration"
          },
          {
            "subjectValue": "Optical Coherence Tomography"
          },
          {
            "subjectValue": "Fundus Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "1.44 GB",
          "1000 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1430000,
      "fileCount": 1,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/trghs22fpg/1",
    "created": "1616050800"
  },
  {
    "id": 2,
    "canonicalId": "n6l94ql8oga1ggbu1ku80kvc",
    "datasetId": "shcf0nviaoabxwx6rfo1za7i",
    "doi": "10.17632/trghs22fpg.2",
    "title": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders",
    "description": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/trghs22fpg.2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Taimur Hassan",
            "nameType": "Personal"
          },
          {
            "creatorName": "Muhammad Usman Akram",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Sciences and Technology"
              }
            ]
          },
          {
            "creatorName": "Muhammad Noman Nazir",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Armed Forces Institute of Ophthalmology"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-03-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal Fundus and OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/7294517",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/9049083",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/s10916-018-1078-3",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/8425990",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1016/j.compbiomed.2018.12.015",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/AO.55.000454",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/JOSAA.33.000455",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/978-3-319-93000-8_79",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          }
        ],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "Central Serous Chorioretinopathy"
          },
          {
            "subjectValue": "Cystoid Macular Edema"
          },
          {
            "subjectValue": "Age-Related Macular Degeneration"
          },
          {
            "subjectValue": "Optical Coherence Tomography"
          },
          {
            "subjectValue": "Fundus Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "1.44 GB",
          "1000 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1430000,
      "fileCount": 1,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/trghs22fpg/1",
    "created": "1616310000"
  },
  {
    "id": 3,
    "canonicalId": "n6l94ql8oga1ggbu1ku80kvc",
    "datasetId": "aromplgot5ob7svcxw7vbkrw",
    "doi": "10.17632/trghs22fpg.3",
    "title": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders",
    "description": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
    "versionTitle": "3",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/trghs22fpg.4",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders"
          }
        ],
        "version": "4",
        "creator": [
          {
            "creatorName": "Taimur Hassan",
            "nameType": "Personal"
          },
          {
            "creatorName": "Muhammad Usman Akram",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Sciences and Technology"
              }
            ]
          },
          {
            "creatorName": "Muhammad Noman Nazir",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Armed Forces Institute of Ophthalmology"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-09-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal Fundus and OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/7294517",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/9049083",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/s10916-018-1078-3",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/8425990",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1016/j.compbiomed.2018.12.015",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/AO.55.000454",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/JOSAA.33.000455",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/978-3-319-93000-8_79",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          }
        ],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "Central Serous Chorioretinopathy"
          },
          {
            "subjectValue": "Cystoid Macular Edema"
          },
          {
            "subjectValue": "Age-Related Macular Degeneration"
          },
          {
            "subjectValue": "Optical Coherence Tomography"
          },
          {
            "subjectValue": "Fundus Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "1.44 GB",
          "1000 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1430000,
      "fileCount": 1,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/trghs22fpg/1",
    "created": "1617174000"
  },
  {
    "id": 4,
    "canonicalId": "n6l94ql8oga1ggbu1ku80kvc",
    "datasetId": "aromplgot5ob7svcxw7vbkrw",
    "doi": "10.17632/trghs22fpg.4",
    "title": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders",
    "description": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
    "versionTitle": "4",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/trghs22fpg.3",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A Composite Retinal Fundus and OCT Dataset along with Detailed Clinical Markings for Extracting Retinal Layers, Retinal Lesions and Screening Macular and Glaucomatous Disorders"
          }
        ],
        "version": "4",
        "creator": [
          {
            "creatorName": "Taimur Hassan",
            "nameType": "Personal"
          },
          {
            "creatorName": "Muhammad Usman Akram",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Sciences and Technology"
              }
            ]
          },
          {
            "creatorName": "Muhammad Noman Nazir",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Armed Forces Institute of Ophthalmology"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-03-31",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal Fundus and OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This repository contains composite retinal fundus and OCT dataset for analyzing retinal layers, retinal lesions, and to diagnose normal and abnormal retinal diseases like Macular Edema, Central Serous Retinopathy, Age-related Macular Degeneration, and Glaucoma.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/7294517",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/9049083",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/s10916-018-1078-3",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://ieeexplore.ieee.org/document/8425990",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1016/j.compbiomed.2018.12.015",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/AO.55.000454",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1364/JOSAA.33.000455",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "JournalArticle"
          },
          {
            "relatedIdentifierValue": "https://doi.org/10.1007/978-3-319-93000-8_79",
            "relatedIdentifierType": "URL",
            "relationType": "Cites",
            "resourceTypeGeneral": "ConferencePaper"
          }
        ],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "Central Serous Chorioretinopathy"
          },
          {
            "subjectValue": "Cystoid Macular Edema"
          },
          {
            "subjectValue": "Age-Related Macular Degeneration"
          },
          {
            "subjectValue": "Optical Coherence Tomography"
          },
          {
            "subjectValue": "Fundus Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "1.44 GB",
          "1000 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1430000,
      "fileCount": 1,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/trghs22fpg/1",
    "created": "1632294000"
  },
  {
    "id": 5,
    "canonicalId": "wz4jmtu7byn2eh5dod8aqe1i",
    "datasetId": "wz4jmtu7byn2eh5dod8aqe1i",
    "doi": "10.5683/SP/YEM3RA",
    "title": "Age-related Macular Degeneration Retinal OCT images",
    "description": "Fovea-centered OCT images of adult retina diagnosed with Age-related Macular Degeneration.",
    "versionTitle": "1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Fovea-centered OCT images of adult retina diagnosed with Age-related Macular Degeneration",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5683/SP/YEM3RA",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Age-related Macular Degeneration Retinal OCT images"
          }
        ],
        "version": "1.0",
        "creator": [
          {
            "creatorName": "Peyman Gholami",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Waterloo"
              }
            ]
          },
          {
            "creatorName": "Priyanka Roy",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Waterloo"
              }
            ]
          },
          {
            "creatorName": "Vasudevan Lakshminarayanan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Waterloo"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-12-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Borealis Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Fovea-centered OCT images of adult retina diagnosed with Age-related Macular Degeneration.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Engineering"
          },
          {
            "subjectValue": "Medicine"
          },
          {
            "subjectValue": "Health and Life Sciences"
          },
          {
            "subjectValue": "Computer and Information Science"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 1.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/1.0/",
              "rightsIdentifierScheme": "CC BY 1.0",
              "schemeURI": "https://creativecommons.org/licenses/by/1.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Borealis Data"
        },
        "size": [
          "5.8MB",
          "57 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/jpeg"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 5800000,
      "fileCount": 57,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP/YEM3RA",
    "created": "1545072000"
  },
  {
    "id": 6,
    "canonicalId": "j215vdvtjyqihxb41mrg4t3h",
    "datasetId": "j215vdvtjyqihxb41mrg4t3h",
    "doi": "10.17632/2wxnrd832j.1",
    "title": "The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment",
    "description": "This dataset is an upgraded version of the dataset used in the latter part of the article 'The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment' in Medical & Biological Engineering & Computing volume 57, pages677-687(2019). Images including both OCT and matched fundus photograph was crawled from Google Image Search using keywords related to normal retina, drusen, AMD, and OCT. The dataset was built by harvesting images manually for normal and AMD categories. Finally, 59 normal eyes, 26 drusens, and 98 AMD eyes including both OCT and matched fundus images were obtained.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This dataset is an upgraded version of the dataset used in the latter part of the article 'The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment' in Medical & Biological Engineering & Computing volume 57, pages677-687(2019). \n\n Images including both OCT and matched fundus photograph was crawled from Google Image Search using keywords related to normal retina, drusen, AMD, and OCT. The dataset was built by harvesting images manually for normal and AMD categories. Finally, 59 normal eyes, 26 drusens, and 98 AMD eyes including both OCT and matched fundus images were obtained.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/2wxnrd832j.1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "TaeKeun Yoo",
            "nameType": "Personal"
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-10-13",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Borealis Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT and Fundus Images Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This dataset is an upgraded version of the dataset used in the latter part of the article 'The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment' in Medical & Biological Engineering & Computing volume 57, pages677-687(2019). \n\n Images including both OCT and matched fundus photograph was crawled from Google Image Search using keywords related to normal retina, drusen, AMD, and OCT. The dataset was built by harvesting images manually for normal and AMD categories. Finally, 59 normal eyes, 26 drusens, and 98 AMD eyes including both OCT and matched fundus images were obtained.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Optical Coherence Tomography"
          },
          {
            "subjectValue": "Retinal Examination"
          },
          {
            "subjectValue": "Fundus Imaging"
          },
          {
            "subjectValue": "Multimodality Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "17.8MB",
          "178 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 17800000,
      "fileCount": 178,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/2wxnrd832j/1",
    "created": "1602572400"
  },
  {
    "id": 7,
    "canonicalId": "l64eb3wab5vtjbi7cud2vrfr",
    "datasetId": "l64eb3wab5vtjbi7cud2vrfr",
    "doi": "10.17632/8kt969dhx6.1",
    "title": "Labeled Retinal Optical Coherence Tomography Dataset for Classification of Normal, Drusen, and CNV Cases",
    "description": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.\n\nThe structure of the folders are as below:\n- CNV, DRUSEN, NORMAL folders\n- Within each class, folders are separated patient-wise with numbers from 1 to <number_of_patients>.\n- Within each patient folder, images (B-scans) are labeled with <0XX_LABEL> format where <XX> is the B-scan number, and <LABEL> is the specialist's selected label for that specific B-scan.\n\nThe excel spreadsheet (data_information.csv) includes information such as \"Patient ID\", \"Class\", \"Eye\", \"B-scan\", \"Label\", and \"Directory\" for all images (16823 rows, 6 columns).\n\nThe python code (read_data.py) includes code for loading images and labels as NumPy arrays. The written function outputs the input data as an array with shape (number_of_images, imageSize, imageSize, 3) and output data as a list of labels (Normal: 0, Drusen: 1, CNV: 2). There are two different options for reading the files:\n- Option 1: Reading all images. This would result in 16822 images.\n- Option 2: Reading the worst-case condition images for each volume (i.e., if a patient was detected as a CNV case, only CNV-appearing B-scans were included for training procedure and normal and drusen B-scans of that patient are excluded from the dataset). This would result in 12649 images.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/8kt969dhx6.1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Labeled Retinal Optical Coherence Tomography Dataset for Classification of Normal, Drusen, and CNV Cases"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Saman Sotoudeh-Paima",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Tehran"
              }
            ]
          },
          {
            "creatorName": "Fedra Hajizadeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Noor Eye Hospital"
              }
            ]
          },
          {
            "creatorName": "Hamid Soltanian-Zadeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Henry Ford Health System"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-10-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "3.92 GB",
          "16823 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 392000000,
      "fileCount": 16823,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/2wxnrd832j/1",
    "created": "1633503600"
  },
  {
    "id": 8,
    "canonicalId": "l64eb3wab5vtjbi7cud2vrfr",
    "datasetId": "jk9lmp3qhmk6j1pbw3gs0x7o",
    "doi": "10.17632/8kt969dhx6.2",
    "title": "Labeled Retinal Optical Coherence Tomography Dataset for Classification of Normal, Drusen, and CNV Cases",
    "description": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.\n\nThe structure of the folders are as below:\n- CNV, DRUSEN, NORMAL folders\n- Within each class, folders are separated patient-wise with numbers from 1 to <number_of_patients>.\n- Within each patient folder, images (B-scans) are labeled with <0XX_LABEL> format where <XX> is the B-scan number, and <LABEL> is the specialist's selected label for that specific B-scan.\n\nThe excel spreadsheet (data_information.csv) includes information such as \"Patient ID\", \"Class\", \"Eye\", \"B-scan\", \"Label\", and \"Directory\" for all images (16823 rows, 6 columns).\n\nThe python code (read_data.py) includes code for loading images and labels as NumPy arrays. The written function outputs the input data as an array with shape (number_of_images, imageSize, imageSize, 3) and output data as a list of labels (Normal: 0, Drusen: 1, CNV: 2). There are two different options for reading the files:\n- Option 1: Reading all images. This would result in 16822 images.\n- Option 2: Reading the worst-case condition images for each volume (i.e., if a patient was detected as a CNV case, only CNV-appearing B-scans were included for training procedure and normal and drusen B-scans of that patient are excluded from the dataset). This would result in 12649 images.\n\n*****************************************************************************************************************************************************\n*****************************************************************************************************************************************************\n\nIf you utilize the dataset, kindly acknowledge and cite our work by referencing the following publication:\nSotoudeh-Paima, S., Jodeiri, A., Hajizadeh, F., & Soltanian-Zadeh, H. (2022). Multi-scale convolutional neural network for automated AMD classification using retinal OCT images. Computers in biology and medicine, 144, 105368.\n\nThe following repository contains implementation of the abovementioned publication:\nhttps://github.com/SamanSotoudeh/Multi-scale-convolutional-neural-network-for-automated-AMD-classification-using-retinal-OCT-images",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.17632/8kt969dhx6.2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Labeled Retinal Optical Coherence Tomography Dataset for Classification of Normal, Drusen, and CNV Cases"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Saman Sotoudeh-Paima",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Tehran"
              }
            ]
          },
          {
            "creatorName": "Fedra Hajizadeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Noor Eye Hospital"
              }
            ]
          },
          {
            "creatorName": "Hamid Soltanian-Zadeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Henry Ford Health System"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-11-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Mendeley Data"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal Optical Coherence Tomography Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by/4.0/",
              "rightsIdentifierScheme": "CC BY 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "Mendeley Data"
        },
        "size": [
          "3.92 GB",
          "16822 images"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/DICOM"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 392000000,
      "fileCount": 16822,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://data.mendeley.com/datasets/jk9lmp3qhmk6j1pbw3gs0x7o/2",
    "created": "1637481600"
  },
  {
    "id": 9,
    "canonicalId": "l9xq04s6khjwoy8lvpy1j65v",
    "datasetId": "l9xq04s6khjwoy8lvpy1j65v",
    "doi": "10.5522/04/22128671.v1",
    "title": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers",
    "description": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5522/04/22128671.v1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Mustafa Arikan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University College London"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-04-14",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on figshare"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Knowledge Representation and Machine Learning"
          },
          {
            "subjectValue": "Artificial Intelligence and Image Processing"
          },
          {
            "subjectValue": "Image Processing"
          },
          {
            "subjectValue": "deep learning"
          },
          {
            "subjectValue": "Medical Imaging"
          },
          {
            "subjectValue": "OCT Annotations"
          },
          {
            "subjectValue": "Retinal Layer Annotations"
          },
          {
            "subjectValue": "Retinal Layers"
          },
          {
            "subjectValue": "Object Detection"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution No Derivatives 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by-nd/4.0/",
              "rightsIdentifierScheme": "CC BY-ND 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by-nd/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "figshare"
        },
        "size": [
          "47.33 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/png"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 47330000,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://rdr.ucl.ac.uk/articles/dataset/OCT5k_A_dataset_of_multi-disease_and_multi-graded_annotations_for_retinal_layers/22128671/1",
    "created": "1681455600"
  },
  {
    "id": 10,
    "canonicalId": "l9xq04s6khjwoy8lvpy1j65v",
    "datasetId": "cy8vis4umaa7tkwk44io966c",
    "doi": "10.5522/04/22128671.v2",
    "title": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers",
    "description": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5522/04/22128671.v2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Mustafa Arikan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University College London"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-11-24",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on figshare"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Knowledge Representation and Machine Learning"
          },
          {
            "subjectValue": "Artificial Intelligence and Image Processing"
          },
          {
            "subjectValue": "Image Processing"
          },
          {
            "subjectValue": "deep learning"
          },
          {
            "subjectValue": "Medical Imaging"
          },
          {
            "subjectValue": "OCT Annotations"
          },
          {
            "subjectValue": "Retinal Layer Annotations"
          },
          {
            "subjectValue": "Retinal Layers"
          },
          {
            "subjectValue": "Object Detection"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution No Derivatives 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by-nd/4.0/",
              "rightsIdentifierScheme": "CC BY-ND 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by-nd/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "figshare"
        },
        "size": [
          "48.33 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/png"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 48330000,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://rdr.ucl.ac.uk/articles/dataset/OCT5k_A_dataset_of_multi-disease_and_multi-graded_annotations_for_retinal_layers/22128671/2",
    "created": "1700812800"
  },
  {
    "id": 11,
    "canonicalId": "l9xq04s6khjwoy8lvpy1j65v",
    "datasetId": "ejyvkb9428pudqx0mife6tkl",
    "doi": "10.5522/04/22128671.v3",
    "title": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers",
    "description": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
    "versionTitle": "3",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5522/04/22128671.v3",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers"
          }
        ],
        "version": "3",
        "creator": [
          {
            "creatorName": "Mustafa Arikan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University College London"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-02-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on figshare"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Retinal OCT Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": ""
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ophthalmology"
          },
          {
            "subjectValue": "Knowledge Representation and Machine Learning"
          },
          {
            "subjectValue": "Artificial Intelligence and Image Processing"
          },
          {
            "subjectValue": "Image Processing"
          },
          {
            "subjectValue": "deep learning"
          },
          {
            "subjectValue": "Medical Imaging"
          },
          {
            "subjectValue": "OCT Annotations"
          },
          {
            "subjectValue": "Retinal Layer Annotations"
          },
          {
            "subjectValue": "Retinal Layers"
          },
          {
            "subjectValue": "Object Detection"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "Creative Commons Attribution No Derivatives 4.0 International",
            "rightsIdentifier": {
              "rightsIdentifierValue": "https://creativecommons.org/licenses/by-nd/4.0/",
              "rightsIdentifierScheme": "CC BY-ND 4.0",
              "schemeURI": "https://creativecommons.org/licenses/by-nd/4.0/"
            }
          }
        ],
        "publisher": {
          "publisherName": "figshare"
        },
        "size": [
          "48.43 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": [
          "image/png"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 48430000,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://rdr.ucl.ac.uk/articles/dataset/OCT5k_A_dataset_of_multi-disease_and_multi-graded_annotations_for_retinal_layers/22128671/3",
    "created": "1707465600"
  },
  {
    "id": 12,
    "canonicalId": "l9xq04s6khjwoy8lvpy1j65v",
    "datasetId": "x5k19qbg8k27f6qqdvl7y64f",
    "doi": "10.5522/04/22128671.v4",
    "title": "OCT5k: A dataset of multi-disease and multi-graded annotations for retinal layers",
    "description": "The thickness and appearance of retinal layers are essential markers for diagnosing and studying eye diseases. Despite the increasing availability of imaging devices to scan and store large amounts of data, analyzing retinal images and generating trial endpoints has remained a manual, error-prone, and time-consuming task. In particular, the lack of large amounts of high-quality labels for different diseases hinders the development of automated algorithms. Therefore, we have compiled 5016 pixel-wise manual labels for 1672 optical coherence tomography (OCT) scans featuring two different diseases as well as healthy subjects to help democratize the process of developing novel automatic techniques. We also collected 4698 bounding box annotations for a subset of 566 scans across 9 classes of disease biomarker. Due to variations in retinal morphology, intensity range, and changes in contrast and brightness, designing segmentation and detection methods that can generalize to different disease types is challenging. While machine learning-based methods can overcome these challenges, high-quality expert annotations are necessary for training. Publicly available annotated image datasets typically contain few images and/or only cover a single type of disease, and most are only annotated by a single grader. To address this gap, we present a comprehensive multi-grader and multi-disease dataset fortraining machine learning-based algorithms. The proposed dataset covers three subsets of scans (Age-related Macular Degeneration, Diabetic Macular Edema, and healthy) and annotations for two types of tasks (semantic segmentation and object detection).",
    "versionTitle": "4",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/study_description.json",
        "identificationModule": {
          "officialTitle": "AI Ready and Equitable Atlas for Diabetes Insights",
          "acronym": "AI-READI",
          "orgStudyIdInfo": {
            "orgStudyId": "OT2OD032644",
            "orgStudyIdType": "U.S. National Institutes of Health (NIH) Grant/Contract Award Number",
            "orgStudyIdLink": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
          },
          "secondaryIdInfoList": [
            {
              "secondaryId": "NCT06002048",
              "secondaryIdType": "ClinicalTrials.gov",
              "secondaryIdLink": "https://classic.clinicaltrials.gov/ct2/show/NCT06002048"
            }
          ]
        },
        "statusModule": {
          "overallStatus": "Enrolling by invitation",
          "startDateStruct": {
            "startDate": "2023-07-19",
            "startDateType": "Actual"
          },
          "completionDateStruct": {
            "completionDate": "2027-01-01",
            "completionDateType": "Anticipated"
          }
        },
        "sponsorCollaboratorsModule": {
          "leadSponsor": {
            "leadSponsorName": "University of Washington",
            "leadSponsorIdentifier": {
              "leadSponsorIdentifierValue": "https://ror.org/00cvxb145",
              "leadSponsorIdentifierScheme": "ROR",
              "schemeURI": "https://ror.org/"
            }
          },
          "responsibleParty": {
            "responsiblePartyType": "Principal Investigator",
            "responsiblePartyInvestigatorFirstName": "Aaron",
            "responsiblePartyInvestigatorLastName": "Lee",
            "responsiblePartyInvestigatorTitle": "Associate Professor",
            "responsiblePartyInvestigatorIdentifier": [
              {
                "responsiblePartyInvestigatorIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                "responsiblePartyInvestigatorIdentifierScheme": "ORCID",
                "schemeURI": "https://orcid.org/"
              }
            ],
            "responsiblePartyInvestigatorAffiliation": {
              "responsiblePartyInvestigatorAffiliationName": "University of Washington",
              "responsiblePartyInvestigatorAffiliationIdentifier": {
                "responsiblePartyInvestigatorAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                "responsiblePartyInvestigatorAffiliationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          },
          "collaboratorList": [
            {
              "collaboratorName": "National Institutes of Health",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/01cwqze88",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "California Medical Innovations Institute",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0156zyn36",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Johns Hopkins University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00za53h95",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Oregon Health & Science University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/009avj582",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Stanford University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00f54p054",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of Alabama at Birmingham",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/008s83205",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of California, San Diego",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0168r3w48",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        },
        "oversightModule": {
          "isFDARegulatedDrug": "No",
          "isFDARegulatedDevice": "No",
          "humanSubjectReviewStatus": "Submitted, approved",
          "oversightHasDMC": "No"
        },
        "descriptionModule": {
          "briefSummary": "The study will collect a cross-sectional dataset of 4000 people across the US from diverse racial/ethnic groups who are either 1) healthy, or 2) belong in one of the three stages of diabetes severity (pre-diabetes/lifestyle controlled, oral medication and/or non-insulin-injectable medication controlled, or insulin dependent), forming a total of four groups of patients. Clinical data (social determinants of health surveys, continuous glucose monitoring data, biomarkers, genetic data, retinal imaging, cognitive testing, etc.) will be collected. The purpose of this project is data generation to allow future creation of artificial intelligence/machine learning (AI/ML) algorithms aimed at defining disease trajectories and underlying genetic links in different racial/ethnic cohorts. A smaller subgroup of participants will be invited to come for a follow-up visit in year 4 of the project (longitudinal arm of the study). Data will be placed in an open-source repository and samples will be sent to the study sample repository and used for future research.",
          "detailedDescription": "The Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available. This project will also create a roadmap for ethical and equitable research that focuses on the diversity of the research participants and the workforce involved at all stages of the research process (study design and data collection, curation, analysis, and sharing and collaboration)."
        },
        "conditionsModule": {
          "conditionList": [
            {
              "conditionName": "Type 2 Diabetes",
              "conditionIdentifier": {
                "conditionClassificationCode": "D003924",
                "conditionScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "conditionURI": "https://meshb.nlm.nih.gov/record/ui?ui=D003924"
              }
            }
          ],
          "keywordList": [
            {
              "keywordValue": "Retinal Imaging"
            },
            {
              "keywordValue": "Data Sharing",
              "keywordIdentifier": {
                "keywordClassificationCode": "D033181",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D033181"
              }
            },
            {
              "keywordValue": "Equitable Data Collection"
            },
            {
              "keywordValue": "Machine Learning",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000069550",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
              }
            },
            {
              "keywordValue": "Artificial Intelligence",
              "keywordIdentifier": {
                "keywordClassificationCode": "D001185",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
              }
            },
            {
              "keywordValue": "Electrocardiography",
              "keywordIdentifier": {
                "keywordClassificationCode": "D004562",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
              }
            },
            {
              "keywordValue": "Continuous Glucose Monitoring",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000095583",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
              }
            },
            {
              "keywordValue": "Retinal imaging"
            },
            {
              "keywordValue": "Eye exam"
            }
          ]
        },
        "designModule": {
          "studyType": "Observational",
          "isPatientRegistry": "No",
          "designInfo": {
            "designObservationalModelList": [
              "Cohort"
            ],
            "designTimePerspectiveList": [
              "Cross-sectional"
            ]
          },
          "bioSpec": {
            "bioSpecRetention": "Samples With DNA",
            "bioSpecDescription": "Participants who consent are asked to provide both blood and urine for research use. There are three distinct elements that follow from these collections; first, a small amount of the collected blood is sent to the local collection site hospital lab for a complete blood cell count on the day of the encounter. Second, an additional portion of the blood and the urine sample are processed and stored at the collection site for batch shipment to the University of Washington Nutrition and Obesity Research Center for specialized lab tests. Third, the remaining majority of the collected blood is processed into a variety of derivatives that are being used to establish a large repository of biospecimens at the University of Alabama at Birmingham to be used in research to further our understanding of diabetes, diabetic associated eye disease and other applications related to health and related diseases. These derivatives include isolated plasma, serum, DNA, buffy coats (white blood cells), blood stabilized for future isolation of RNA and peripheral blood mononuclear cells (PBMC). These derivatives are separated into small aliquots to facilitate future approved research test and are stored at either -80oC or at cryogenic temperatures (PBMC)."
          },
          "enrollmentInfo": {
            "enrollmentCount": "4000",
            "enrollmentType": "Anticipated"
          }
        },
        "armsInterventionsModule": {
          "armGroupList": [
            {
              "armGroupLabel": "Healthy",
              "armGroupDescription": "Participants who do not have Type 1 or Type 2 Diabetes",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Pre-diabetes/Lifestyle Controlled",
              "armGroupDescription": "Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifesyle adjustments",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Oral Medication and/or Non-insulin-injectable Medication Controlled",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Insulin Dependent",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            }
          ],
          "interventionList": [
            {
              "interventionType": "Other",
              "interventionName": "AI-READI protocol",
              "interventionDescription": "Custom protocol followed for the AI-READI study. Details are available at in the documentation of v1.0.0 of the dataset at https://docs.aireadi.org/"
            }
          ]
        },
        "eligibilityModule": {
          "sex": "All",
          "genderBased": "No",
          "minimumAge": "40 Years",
          "maximumAge": "85 Years",
          "healthyVolunteers": "No",
          "eligibilityCriteria": {
            "eligibilityCriteriaInclusion": [
              "Adults (\u2265 40 years old)",
              "Patients with and without type 2 diabetes",
              "Able to provide consent",
              "Must be able to read and speak English"
            ],
            "eligibilityCriteriaExclusion": [
              "Adults older than 85 years of age",
              "Pregnancy",
              "Gestational diabetes",
              "Type 1 diabetes"
            ]
          },
          "studyPopulation": "Adult patients will be recruited into one of four groups: 1) healthy/no diabetes, 2) pre-diabetes/borderline diabetes/lifestyle-controlled diabetes, 3) oral medication and/or non-insulin injectable medication controlled type 2 diabetes, or 4) insulin dependent type 2 diabetes. The investigators aim to recruit approximately 1000 patients into each of the four groups. Patients will be recruited from University of Washington (UW), University of California at San Diego (UCSD), and University of Alabama at Birmingham (UAB). The study aims to recruit 1,000 subjects from each of the following racial and ethnic groups: White, Asian, Hispanic, and Black. Subjects will be age- and sex-matched within and between groups.",
          "samplingMethod": "Non-Probability Sample"
        },
        "contactsLocationsModule": {
          "centralContactList": [
            {
              "centralContactFirstName": "Aaron",
              "centralContactLastName": "Lee",
              "centralContactDegree": "MD",
              "centralContactIdentifier": [
                {
                  "centralContactIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "centralContactIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "centralContactAffiliation": {
                "centralContactAffiliationName": "University of Washington",
                "centralContactAffiliationIdentifier": {
                  "centralContactAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "centralContactAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "centralContactEMail": "contact@aireadi.org"
            }
          ],
          "overallOfficialList": [
            {
              "overallOfficialFirstName": "Aaron",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Washington",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cecilia",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-1994-7213",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Washington",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Amir",
              "overallOfficialLastName": "Bahmani",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-4533-9334",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sally L.",
              "overallOfficialLastName": "Baxter",
              "overallOfficialDegree": "MD, MSc",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-5271-7690",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Christopher G.",
              "overallOfficialLastName": "Chute",
              "overallOfficialDegree": "MD, DrPH",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-5437-2545",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Megan",
              "overallOfficialLastName": "Collins",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-2067-0848",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Kadija",
              "overallOfficialLastName": "Ferryman",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-8552-1822",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Samantha",
              "overallOfficialLastName": "Hurst",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9843-2845",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Hiroshi",
              "overallOfficialLastName": "Ishikawa",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-6310-5748",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Oregon Health & Science University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/009avj582",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "T. Y. Alvin",
              "overallOfficialLastName": "Liu",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-2957-0755",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Gerald",
              "overallOfficialLastName": "McGwin",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9592-1133",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Shannon",
              "overallOfficialLastName": "McWeeney",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-8333-6607",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Oregon Health & Science University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/009avj582",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Camille",
              "overallOfficialLastName": "Nebeker",
              "overallOfficialDegree": "EdD, MS",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-6819-1796",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cynthia",
              "overallOfficialLastName": "Owsley",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-3424-011X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Bhavesh",
              "overallOfficialLastName": "Patel",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-0307-262X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "California Medical Innovations Institute",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0156zyn36",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Michael",
              "overallOfficialLastName": "Snyder",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-0784-7987",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sara J.",
              "overallOfficialLastName": "Singer",
              "overallOfficialDegree": "MBA, PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "http://orcid.org/0000-0002-3374-1177",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Linda M.",
              "overallOfficialLastName": "Zangwill",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-1143-5224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            }
          ],
          "locationList": [
            {
              "locationFacility": "University of Alabama at Birmingham",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Birmingham",
              "locationZip": "35233",
              "locationState": "Alabama",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/008s83205",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of California, San Diego",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "San Diego",
              "locationZip": "92093",
              "locationState": "California",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/0168r3w48",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of Washington",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Seattle",
              "locationZip": "98109",
              "locationState": "Washington",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/00cvxb145",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        }
      },
      "readme": "## Overview of the study\n\nThe Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available. This project will also create a roadmap for ethical and equitable research that focuses on the diversity of the research participants and the workforce involved at all stages of the research process (study design and data collection, curation, analysis, and sharing and collaboration).\n\n## Description of the dataset\n\nThis dataset contains data from 204 participants from the pilot period of the AI-READI project (July 19, 2023 to November 30, 2023). Data from multiple modalities are included. A full list is provided in the `Data Standards` section below. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed.\n\nThe dataset contains 21,669 files and is around 310 GB in size.\n\nA detailed description of the dataset is available in the AI-READI documentation for v1.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Protocol\n\nThe protocol followed for collecting the data can be found in the AI-READI documentation for v1.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Dataset access/restrictions\n\nAccessing the dataset requires several steps, including:\n\n- Login in through a verified ID system\n- Agreeing to use the data only for type 2 diabetes related research.\n- Agreeing to the license terms which set certain restrictions and obligations for data usage (see `License` section below).\n\n## Data standards followed\n\nThis dataset is organized following the [Clinical Dataset Structure (CDS) v0.1.0](https://cds-specification.readthedocs.io/en/v0.1.0/). We refer to the CDS documentation for more details. Briefly, data is organized at the root level into one directory per datatype (c.f. Table below). Within each datatype folder, there is one folder per modality. Within each modality folder, there is one folder per device used to collect that modality. Within each device folder, there is one folder per participant. Each datatype, modality, and device folder is named using a name that best defines it. Each participant folder is named after the participant's ID number used in the study. For each datatype, the data files follow the standards listed in the Table below. More details are available in the dataset_structure_description.json metadata file included in this dataset.\n\n| Datatype directory name   | Description                                                                                                                                                                                      | File format standard followed                                                                                                                                     |\n| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cardiac_ecg               | This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.      | [WaveForm DataBase (WFDB)](https://wfdb.readthedocs.io/en/latest/wfdb.html)                                                                                       |\n| clinical_data             | This directory contains clinical data collected through REDCap. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.                                                  | [Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi)                                               |\n| environment               | This directory contains data collected through an environmental sensor device custom built for the AI-READI project.                                                                             | [Earth Science Data Systems (ESDS) format](https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data) |\n| retinal_flio              | This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores. | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_oct               | This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.                                   | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_octa              | This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.                     | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_photography       | This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.                                                                          | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| wearable_activity_monitor | This directory contains data collected through a wearable fitness tracker.                                                                                                                       | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n| wearable_blood_glucose    | This directory contains data collected through a continuous glucose monitoring (CGM) device.                                                                                                     | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n\n## Resources\n\nAll of our data files are in formats that are accessible with free software commonly used for such data types so no specific software is required. Some useful resources related to this dataset are listed below:\n\n- Documentation of the dataset: [docs.aireadi.org](https://docs.aireadi.org/) (see 'Dataset v1.0.0' for this version of the dataset)\n- AI-READI project website: [aireadi.org](https://aireadi.org/)\n- Zenodo community of the AI-READI project: [zenodo.org/communities/aireadi](https://zenodo.org/communities/aireadi)\n- GitHub organization of the AI-READI project: [github.com/AI-READI](https://github.com/AI-READI)\n\n## License\n\nThis work is licensed under a custom license specifically tailored to enable the reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purposes while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications. More details are available in the License file included in the dataset and also available at https://doi.org/10.5281/zenodo.10642459.\n\n## How to cite\n\nIf you use this dataset for any purpose, please cite the resources specified in the AI-READI documentation for version 1.0.0 of the dataset at https://docs.aireadi.org.\n\n## Contact\n\nFor any questions, suggestions, or feedback related to this dataset, please email contact@aireadi.org. We refer to the study_description.json and dataset_description.json metadata files included in this dataset for additional information about the contact person/entity, authors, and contributors of the dataset.\n\n## Acknowledgement\n\nThe AI-READI project is supported by NIH grant [1OT2OD032644](https://reporter.nih.gov/search/1ADgncihCk6fdMRJdCnBjg/project-details/10471118) through the NIH Bridge2AI Common Fund program.",
      "datasetDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/dataset_description.json",
        "identifier": {
          "identifierValue": "10.60775/fairhub.1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project"
          },
          {
            "titleValue": "AI-READI dataset",
            "titleType": "AlternativeTitle"
          }
        ],
        "version": "1.0.0",
        "creator": [
          {
            "creatorName": "AI-READI Consortium",
            "nameType": "Organizational"
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-05-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made availabe on FAIRhub"
          },
          {
            "dateValue": "2023-07-19/2023-11-30",
            "dateType": "Collected",
            "dateInformation": "Period of the pilot phase when the data was collected"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Type 2 Diabetes",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No identifiers were collected so no active de-identification was necessary but we checked that no identifiable data per US HIPAA were present in the data."
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": "The public version of the dataset can only be used for type 2 diabetes related research. A private version will allow for more generic use."
        },
        "description": [
          {
            "descriptionValue": "This dataset contain data from 204 participants from the pilot period of the AI-READI project (July 19, 2023 to November 30, 2023). Data from multiple modalities are included. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed. A detailed description of the dataset is available in the AI-READI documentation for v1.0.0 of the dataset at https://docs.aireadi.org",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://docs.aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          },
          {
            "relatedIdentifierValue": "https://aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          }
        ],
        "subject": [
          {
            "subjectValue": "Diabetes mellitus",
            "subjectIdentifier": {
              "classificationCode": "45636-8",
              "subjectScheme": "Logical Observation Identifier Names and Codes (LOINC)",
              "schemeURI": "https://loinc.org/",
              "valueURI": "https://loinc.org/45636-8"
            }
          },
          {
            "subjectValue": "Machine Learning",
            "subjectIdentifier": {
              "classificationCode": "D000069550",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/record/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
            }
          },
          {
            "subjectValue": "Artificial Intelligence",
            "subjectIdentifier": {
              "classificationCode": "D001185",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
            }
          },
          {
            "subjectValue": "Electrocardiography",
            "subjectIdentifier": {
              "classificationCode": "D004562",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
            }
          },
          {
            "subjectValue": "Continuous Glucose Monitoring",
            "subjectIdentifier": {
              "classificationCode": "D000095583",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
            }
          },
          {
            "subjectValue": "Retinal imaging"
          },
          {
            "subjectValue": "Eye exam"
          }
        ],
        "managingOrganization": {
          "name": "University of Washington",
          "managingOrganizationIdentifier": {
            "managingOrganizationIdentifierValue": "https://ror.org/00cvxb145",
            "managingOrganizationScheme": "ROR",
            "schemeURI": "https://ror.org"
          }
        },
        "accessType": "PublicDownloadSelfAttestationRequired",
        "accessDetails": {
          "description": "Accessing the dataset requires several steps, including: Login in through a verified ID system, Agreeing to use the data only for type 2 diabetes related research, Agreeing to the license terms which set certain restrictions and obligations for data usage (see 'rights' property)"
        },
        "rights": [
          {
            "rightsName": "AI-READI custom license v1.0",
            "rightsURI": "https://doi.org/10.5281/zenodo.10642459"
          }
        ],
        "publisher": {
          "publisherName": "FAIRhub"
        },
        "size": [
          "310 GB",
          "21,669 files"
        ],
        "fundingReference": [
          {
            "funderName": "National Institutes of Health",
            "funderIdentifier": {
              "funderIdentifierValue": "https://ror.org/01cwqze88",
              "funderIdentifierType": "ROR",
              "schemeURI": "https://ror.org"
            },
            "awardNumber": {
              "awardNumberValue": "OT2OD032644",
              "awardURI": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
            },
            "awardTitle": "Bridge2AI: Salutogenesis Data Generation Project"
          }
        ],
        "format": [
          "image/DICOM",
          "text/markdown",
          "table/csv"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/dataset_structure_description.json",
        "directoryList": [
          {
            "directoryName": "cardiac_ecg",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Electrocardiogram",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C168186",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C168186"
                  }
                ]
              },
              {
                "relatedTermValue": "Electrocardiography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D004562",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "WaveForm DataBase (WFDB)",
                "standardDescription": "Set of file standards designed for reading and storing physiologic signal data, and associated annotations.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://wfdb.readthedocs.io/en/latest/wfdb.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "ecg_12lead",
                "directoryType": "modality",
                "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Electrocardiography",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other",
                    "relatedIdentifierDescription": "This page describes 3 types of ECG protocol including the 12 lead (standard) protocol."
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "philips_tc30",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol using the Philips PageWriter TC30 device.",
                    "relatedIdentifier": [
                      {
                        "relatedIdentifierValue": "https://www.documents.philips.com/doclib/enc/fetch/2000/4504/577242/577243/577246/581601/711562/DXL_ECG_Algorithm_Physician_s_Guide_(ENG)_Ed.2.pdf",
                        "relatedIdentifierType": "URL",
                        "relationType": "IsDocumentedBy",
                        "resourceTypeGeneral": "Other",
                        "relatedIdentifierDescription": "The 'Philips DXL ECG Algorithm Physician' Guide' contains information on the fields that are printed on an ECG report."
                      }
                    ]
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS) v0.1.0",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "clinical_data",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains clinical data collected through REDCap, including blood/urine lab values and survey data. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Clinical Data",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C15783",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C15783"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory is named following the specification of this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "The Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)",
                "standardDescription": "Standard designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence.",
                "standardUse": "All the data files within this directory follow this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.qk984b",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "dqd_omop.json",
                "metadataFileDescription": "The dqd_omop.json file supports OMOP CDM data quality analysis using the OMOP CDM Data Quality Dashboard (DQD). The OMOP CDM DQD tool (https://ohdsi.github.io/DataQualityDashboard/) runs a set of > 3500 data quality checks against an OMOP CDM instance",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/DataQualityDashboard/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "environment",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through an environmental sensor device custom built for the AI-READI project.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Environmental sensor data"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "ASCII File Format Guidelines for Earth Science Data",
                "standardDescription": "NASA recommended practices for formatting and describing ASCII encoded data files",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "environmental_sensor_variables",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "leelab_anura",
                    "directoryDescription": "You can learn more about the data in this directory by consulting relevant documentation. Search 'AS7431' with Type set as 'Datasheet' at https://ams-osram.com/support/download-center. Search 'DS3231 Precision RTC' at https://www.adafruit.com, look for product ID 5188, select this link and scroll down to Technical Details to find a link for the Datasheet (as of this writing, you may find the datasheet at https://www.analog.com/media/en/technical-documentation/data-sheets/DS3231.pdf). Search 'Datasheet SEN5x' in the search box at https://sensirion.com/ to get the document titled 'Datasheet SEN5x' (the name of the downloaded file may be 'Sensirion_Datasheet_Environmental_Node_SEN5x.pdf'). Search 'NOx Index' in the search box at https://sensirion.com to get the document titled 'What is Sensirion's NOx Index?' (the name of the downloaded file may be 'Info_Note NOx_Index.pdf').",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "retinal_flio",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Fuorescence Lifetime Imaging Ophthalmoscopy"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "flio",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_flio",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "retinal_oct",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C20828",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C20828"
                  }
                ]
              },
              {
                "relatedTermValue": "Tomography, Optical Coherence",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D041623",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D041623"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "structural_oct",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Triton device, manufactured by Topcon"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "retinal_octa",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography"
              },
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography Images"
              },
              {
                "relatedTermValue": "OCTA Images"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "enface",
                "directoryType": "modality",
                "directoryDescription": "This directory contains en face data collected using OCTA. En face images, derived from 3D volume scans, are also referred to as C-scan OCT. These images provide a 2D view of the retina layers and have an orientation siimilar to fundus photographs.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Triton device, manufactured by Topcon."
                  }
                ]
              },
              {
                "directoryName": "flow_cube",
                "directoryType": "modality",
                "directoryDescription": "This directory contains flow cube data collected using OCTA. Flow cube provides information on blood flow in a 3D view.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Maestro2 device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Triton device, manufacture by Topcon."
                  }
                ]
              },
              {
                "directoryName": "segmentation",
                "directoryType": "modality",
                "directoryDescription": "This directory contains segmentation data collected using OCTA. The segmentation information is presented in the form of heightmaps and includes information about the associated layers.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Maestro device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Triton device, manufactured by Topcon."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "retinal_photography",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              },
              {
                "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Fundus_photography",
                "relatedIdentifierType": "URL",
                "relationType": "IsDescribedBy",
                "resourceTypeGeneral": "Other"
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Eye Fundus Photography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C147467",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C147467"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "cfp",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ophthalmology.med.ubc.ca/patient-care/ophthalmic-photography/color-fundus-photography/#:~:text=Color%20Fundus%20Retinal%20Photography%20uses,monitor%20their%20change%20over%20time",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "optomed_aurora",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Aurora device, manufactured by Optomed."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Triton device, manufactured by Topcon."
                  }
                ]
              },
              {
                "directoryName": "faf",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specificallly fundus autofluorescence photographs that uses the fluorescent characteristics of lipofuscin in an non-invasive way.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://eyewiki.aao.org/Fundus_Autofluorescence",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains fundus autofluorescence photographs from the Eidon device, manufactured by iCare."
                  }
                ]
              },
              {
                "directoryName": "ir",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data using near-infrared reflectance (IR).",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8349282/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Maestro2 device, manufactured by Topcon."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "wearable_activity_monitor",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a wearable fitness tracker.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "smartwatch"
              },
              {
                "relatedTermValue": "activity monitoring"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "heart_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "oxygen_saturation",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "respiratory_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "sleep",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "stress",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          },
          {
            "directoryName": "wearable_blood_glucose",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a continuous glucose monitoring (CGM) device.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Continuous Glucose Monitoring System",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C159776",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C159776"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.0",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "continuous_glucose_monitoring",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "dexcom_g6",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ]
          }
        ],
        "metadataFileList": [
          {
            "metadataFileName": "CHANGELOG.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_structure_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "healthsheet.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "LICENSE.txt",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.tsv",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "README.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "study_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.0/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          }
        ]
      },
      "healthsheet": {
        "generalInformation": [
          {
            "id": 1,
            "question": "Provide a 2 sentence summary of this dataset.",
            "response": "The Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) is a dataset consisting of data collected from individuals with and without `Type 2 Diabetes Mellitus (T2DM)` and harmonized across 3 data collection sites. The composition of the dataset was designed with future studies using AI/Machine Learning in mind. This included recruitment sampling procedures aimed at achieving approximately equal distribution of participants across sex, race, and diabetes severity, as well as the design of a data acquisition protocol across multiple domains (survey data, physical measurements, clinical data, imaging data, wearable device data, etc.) to enable downstream AI/ML analyses that may not be feasible with existing data sources such as claims or electronic health records data. The goal is to better understand salutogenesis (the pathway from disease to health) in T2DM. Some data that are not considered to be sensitive personal health data will be available to the public for download upon agreement with a license that defines how the data can be used. The full dataset will be accessible by entering into a data use agreement. The public dataset will include survey data, blood and urine lab results, fitness activity levels, clinical measurements (e.g. monofilament and cognitive function testing), retinal images, ECG, blood sugar levels, and home air quality. The data held under controlled access include 5-digit zip code, sex, race, ethnicity, genetic sequencing data, past health records, and traffic and accident reports. Of note, the overall enrollment goal is to have balanced distribution between different racial groups. As enrollment is ongoing, the pilot data release and periodic updates to data releases may not have achieved balanced distribution across groups."
          },
          {
            "id": 2,
            "question": "Has the dataset been audited before? If yes, by whom and what are the results?",
            "response": "The dataset has not undergone any formal external audits. However, the dataset has been reviewed internally by AI-READI team members for quality checks and to ensure that no personally identifiable information was accidentally included."
          }
        ],
        "versioning": [
          {
            "id": 1,
            "question": "Does the dataset get released as static versions or is it dynamically updated?\n\n   a. If static, how many versions of the dataset exist?\n\n   b. If dynamic, how frequently is the dataset updated?",
            "response": "The dataset gets released as static versions. This is the first version of the dataset and consists of data collected during the pilot data collection phase. After that, there are plans to release new versions of the dataset approximately once a year with additional data from participants who have been enrolled since the last dataset version release."
          },
          {
            "id": 2,
            "question": "Is this datasheet created for the original version of the dataset? If not, which version of the dataset is this datasheet for?",
            "response": "Yes, this datasheet is created for the first version of the dataset."
          },
          {
            "id": 3,
            "question": "Are there any datasheets created for any versions of this dataset?",
            "response": "No, there are no previous datasheets created, since this is the first version of the dataset."
          },
          {
            "id": 4,
            "question": "Does the current version/subversion of the dataset come with predefined task(s), labels, and recommended data splits (e.g., for training, development/validation, testing)? If yes, please provide a high-level description of the introduced tasks, data splits, and labeling, and explain the rationale behind them. Please provide the related links and references. If not, is there any resource (website, portal, etc.) to keep track of all defined tasks and/or associated label definitions? (please note that more detailed questions w.r.t labeling is provided in further sections)",
            "response": "See response to question #6 under \u201cLabeling and subjectivity of labeling\u201d."
          },
          {
            "id": 5,
            "question": "If the dataset has multiple versions, and this datasheet represents one of them, answer the following questions:\n\n   a. What are the characteristics that have been changed between different versions of the dataset?\n\n   b. Explain the motivation/rationale for creating the current version of the dataset.\n\n   c. Does this version have more subjects/patients represented in the data, or fewer?\n\n   d. Does this version of the dataset have extended data or new data from the same patients as the older versions? Were any patients, data fields, or data points removed? If so, why?\n\n   e. Do we expect more versions of the dataset to be released?\n\n   f. Is this datasheet for a version of the dataset? If yes, does this sub-version of the dataset introduce a new task, labeling, and/or recommended data splits? If the answer to any of these questions is yes, explain the rationale behind it.\n\n   g. Are you aware of any widespread version(s)/subversion(s) of the dataset? If yes, what is the addressed task, or application that is addressed?",
            "response": "N/A, since this is the first version of the dataset."
          }
        ],
        "motivation": [
          {
            "id": 2,
            "question": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.",
            "response": "The purpose for creating the dataset was to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. T2DM is a growing public health threat. Yet, the current understanding of T2DM, especially in the context of salutogenesis, is limited. Given the complexity of T2DM, AI-based approaches may help with improving our understanding but a key issue is the lack of data ready for training AI models. The AI-READI dataset is intended to fill this gap."
          },
          {
            "id": 3,
            "question": "What are the applications that the dataset is meant to address? (e.g., administrative applications, software applications, research)",
            "response": "The multi-modal dataset being collected is being gathered to facilitate downstream pseudotime manifolds and various applications in artificial intelligence."
          },
          {
            "id": 4,
            "question": "Are there any types of usage or applications that are discouraged from using this dataset? If so, why?",
            "response": "The AI READI dataset License imposes certain restrictions on the usage of the data. The restrictions are described in the License files available at https://doi.org/10.5281/zenodo.10642459. Briefly, the Licensee shall not: \u201c(i) make clinical treatment decisions based on the Data, as it is intended solely as a research resource, or (ii) use or attempt to use the Data, alone or in concert with other information, to compromise or otherwise infringe the confidentiality of information on an individual person who is the source of any Data or any clinical data or biological sample from which Data has been generated (a 'Data Subject' and their right to privacy, to identify or contact any individual Data Subject or group of Data Subjects, to extract or extrapolate any identifying information about a Data Subject, to establish a particular Data Subject's membership in a particular group of persons, or otherwise to cause harm or injury to any Data Subject.\u201d"
          },
          {
            "id": 5,
            "question": "Who created this dataset (e.g., which team, research group), and on behalf of which entity (e.g., company, institution, organization)?",
            "response": "This dataset was created by members of the AI-READI project, hereby referred to as the AI-READI Consortium. Details about each member and their institutions are available on the project website at https://aireadi.org."
          },
          {
            "id": 6,
            "question": "Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. If the funding institution differs from the research organization creating and managing the dataset, please state how.",
            "response": "The creation of the dataset was funded by the National Institutes of Health (NIH) through their Bridge2AI Program (https://commonfund.nih.gov/bridge2ai). The grant number is OT2ODO32644 and more information about the funding is available at https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481. Note that the funding institution is not creating or managing the dataset. The dataset is created and managed by the awardees of the grant (c.f. answer to the previous question)."
          },
          {
            "id": 7,
            "question": "What is the distribution of backgrounds and experience/expertise of the dataset curators/generators?",
            "response": "There is a wide range of experience within the project team, including senior, mid-career, and early career faculty members as well as clinical research coordinators, staff, and interns. They collectively cover many areas of expertise including clinical research, data collection, data management, data standards, bioinformatics, team science, and ethics, among others. Visit https://aireadi.org/team for more information."
          }
        ],
        "composition": [
          {
            "id": 1,
            "question": "What do the instances that comprise the dataset represent (e.g., documents, images, people, countries)? Are there multiple types of instances? Please provide a description.",
            "response": "Each instance represents an individual patient."
          },
          {
            "id": 2,
            "question": "How many instances are there in total (of each type, if appropriate) (breakdown based on schema, provide data stats)?",
            "response": "There are 204 instances in this current version of the dataset (version 1, released spring 2024)."
          },
          {
            "id": 3,
            "question": "How many patients / subjects does this dataset represent? Answer this for both the preliminary dataset and the current version of the dataset.",
            "response": "See previous question."
          },
          {
            "id": 4,
            "question": "Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). Answer this question for the preliminary version and the current version of the dataset in question.",
            "response": "The dataset contains all possible instances. More specifically, the dataset contains data from all participants who have been enrolled during the pilot data collection phase for AI-READI."
          },
          {
            "id": 5,
            "question": "What data modality does each patient data consist of? If the data is hierarchical, provide the modality details for all levels (e.g: text, image, physiological signal). Break down in all levels and specify the modalities and devices.",
            "response": "Multiple modalities of data are collected for each participant, including survey data, clinical data, retinal imaging data, environmental sensor data, continuous glucose monitor data, and wearable activity monitor data. These encompass tabular data, imaging data, and physiological signal/waveform data. There is no unstructured text data included in this dataset. The exact forms used for data collection in REDCap are available here. Furthermore, all modalities, file formats, and devices are detailed in the dataset documentation at https://docs.aireadi.org/."
          },
          {
            "id": 6,
            "question": "What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.",
            "response": "Each instance consists of all of the data available for an individual participating in the study. See answer to question 5 for the data types associated with each instance."
          },
          {
            "id": 7,
            "question": "Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).?",
            "response": "Yes, not all modalities are available for all participants. Some participants elected not to participate in some study elements. In a few cases, the data collection device did not have any stored results or was returned too late to retrieve the results (e.g. battery died, data was lost). In a few cases, there may have been a data collision at some point in the process and data has been lost."
          },
          {
            "id": 8,
            "question": "Are relationships between individual instances made explicit? (e.g., They are all part of the same clinical trial, or a patient has multiple hospital visits and each visit is one instance)? If so, please describe how these relationships are made explicit.",
            "response": "Yes - all instances are part of the same prospective data generation project (AI-READI). There is currently only one visit per participant."
          },
          {
            "id": 9,
            "question": "Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. (e.g., losing data due to battery failure, or in survey data subjects skip the question, radiological sources of noise).",
            "response": "In cases of survey data, skipped questions or incomplete responses are expected. In cases of using wearables, improper use, technical failure such as battery failure or system malfunction are expected. In cases of imaging data, patient uncooperation, noise that may obscure the images and technical failure such as system malfunction, and data transfer failures are expected."
          },
          {
            "id": 10,
            "question": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, other datasets)? If it links to or relies on external resources,\n\n a. are there guarantees that they will exist, and remain constant, over time;\n\n b. are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created);\n\n c. are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.",
            "response": "The dataset is self-contained but does rely on the dataset documentation for users requiring additional information about the provenance of the dataset. The documentation is available at https://docs.aireadi.org. The documentation is shared under the CC-BY 4.0 license, so there are no restrictions associated with its use."
          },
          {
            "id": 11,
            "question": "Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications that is confidential)? If so, please provide a description.",
            "response": "No, the dataset does not contain data that might be considered confidential. No personally identifiable information is included in the dataset."
          },
          {
            "id": 12,
            "question": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise pose any safety risk (such as psychological safety and anxiety)? If so, please describe why.",
            "response": "No."
          },
          {
            "id": 13,
            "question": "If the dataset has been de-identified, were any measures taken to avoid the re-identification of individuals? Examples of such measures: removing patients with rare pathologies or shifting time stamps.",
            "response": ""
          },
          {
            "id": 14,
            "question": "Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.",
            "response": "No, the public dataset will not contain data that is considered sensitive. However, the controlled access dataset will contain data regarding racial and ethnic origins, location (5-digit zip code), as well as motor vehicle accident reports."
          }
        ],
        "devices": [
          {
            "id": 1,
            "question": "For data that requires a device or equipment for collection or the context of the experiment, answer the following additional questions or provide relevant information based on the device or context that is used (for example)\n\n   a. If there was an MRI machine used, what is the MRI machine and model used?\n\n   b. If heart rate was measured what is the device for heart rate variation that is used?\n\n   c. If cortisol measurement is reported at multi site, provide details,\n\n   d. If smartphones were used to collect the data, provide the names of models.\n\n   e. And so on,..",
            "response": "The devices included in the study are as follows, and more details can be found at [https://docs.aireadi.org](https://docs.aireadi.org):\n\n   **Environmental sensor device**\n\n   Participants will be sent home with an environmental sensor (a custom-designed sensor unit called the LeeLab Anura), which they will use for 10 continuous days before returning the devices to the clinical research coordinators for data download.\n\n   **Continuous glucose monitor (Dexcom G6)**\n\n   The Dexcom G6 is a real-time, integrated continuous glucose monitoring system (iCGM) that directly monitors blood glucose levels without requiring finger sticks. It must be worn continuously in order to collect data.\n\n   **Wearable accelerometer (Physical activity monitor)**\n\n   The Garmin Vivosmart 5 Fitness Activity tracker will be used to measure data related to physical activity.\n\n   **Heart rate**\n\n   Heart rate can be read from EKG or blood pressure measurement devices.\n\n   **Blood pressure**\n\n   Blood pressure devices used for the study across the various data acquisition sites are: OMRON HEM 907XL Blood Pressure Monitor, Medline MDS4001 Automatic Digital Blood Pressure Monitor, and Welch Allyn 6000 series Vital signs monitor with Welch Allyn FlexiPort Reusable Blood Pressure Cuff.\n\n   **Visual acuity**\n\n   M&S Technologies EVA device to test visual acuity. The test is administered at a distance of 4 meters from a touch-screen monitor that is 12x20 inches. Participants will read letters from the screen. Photopic Conditions: No neutral density filters are used. A general occluder will be used for photopic testing. The participant wears their own prescription spectacles or trial frames. For Mesopic conditions, a neutral density (ND) filter will be used. The ND filter will either be a lens added to trial frames to reduce incoming light on the tested eye, OR a handheld occluder with a neutral density\n   filter (which we will designate as \u201cND-occluder) over the glasses will be used. The ND-occluder is different from a standard occluder and is used only for vision testing under mesopic conditions.\n\n   **Contrast sensitivity**\n\n   The MARS Letter Contrast Sensitivity test (Perceptrix) was conducted monocularly under both Photopic conditions (with a general occluder) and Mesopic conditions (using a Neutral Density occluder with a low luminance filter lens). The standardized order of MARS cards was as follows: Photopic OD, Photopic OS, Mesopic OD, and Mesopic OS. The background luminance of the charts fell within the range of 60 to 120 cd/m2, with an optimal level of 85 cd/m2. Illuminance was recommended to be between 189 to 377 lux, with an optimal level of 267 lux. While the designed viewing distance was 50 cm, it could vary between 40 to 59 cm. Patients were required to wear their appropriate near correction: reading glasses or trial frames with +2.00D lenses. All testing was carried out under undilated conditions. Patients were instructed to read the letter left to right across each line on the chart. Patients were encouraged to guess, even if they perceived the letters as too faint. Testing was terminated either when the patient made two consecutive errors or reached the end of the chart. The log contrast sensitivity (log CS) values were recorded by multiplying the number of errors prior to the final correct letter by 0.04 and subtracting the result from the log CS value at the final correct letter. If a patient reached the end of the chart without making two consecutive errors, the final correct letter was simply the last one correctly identified.\n\n   **Autorefraction**\n\n   KR 800 Auto Keratometer/Refractor.\n\n   **EKG**\n\n   Philips (manufacturer of Pagewriter TC30 Cardiograph)\n\n   **Lensometer**\n\n   Lensometer devices used at data acquisition sites across the study include: NIDEK LM-600P Auto Lensometer, Topcon-CL-200 computerized Lensometer, and Topcon-CL-300 computerized Lensometer\n\n   **Undilated fundus photography - Optomed Aurora**\n\n   The Optomed Aurora IQ is a handheld fundus camera that can take non-mydriatic images of the ocular fundus. It has a 50\u00b0 field of view, 5 Mpix sensor, and high-contrast optical design. The camera is non-mydriatic, meaning it doesn't require the pupil to be dilated, so it can be used for detailed viewing of the retina. Images taken during the AI-READI visit, are undilated images taken in a dark room while a patient is sitting on a comfortable chair, laying back. As it becomes challenging to get a good view because of the patients not being dilated and the handheld nature of this imaging modality, the quality of the images vary from patient to patient and within the same patient.\n\n   **Dilated fundus photography - Eidon**\n\n   The iCare EIDON is a widefield TrueColor confocal fundus imaging system that can capture images up to 200\u00b0. It comes with multiple imaging modalities, including TrueColor, blue, red, Red-Free, and infrared confocal images. The system offers widefield, ultra-high-resolution imaging and the capability to image through cataract and media opacities. It operates without dilation (minimum pupil 2.5 mm) and provides the flexibility of both fully automated and fully manual modes. Additionally, the iCare EIDON features an all-in-one compact design, eliminating the need for an additional PC. AI READI images using EIDON include two main modalities: 1. Single Field Central IR/FAF 2. Smart Horizontal Mosaic. Imaging is done in fully automated mode in a dark room with the machine moving and positioning according to the patient's head aiming at optimizing the view and minimizing operator's involvement/operator induced noise.\n\n   **Spectralis HRA (Heidelberg Engineering)**\n\n   The Heidelberg Spectralis HRA+OCT is an ophthalmic imaging system that combines optical coherence tomography (OCT) with retinal angiography. It is a modular, upgradable platform that allows clinicians to configure it for their specific diagnostic workflow. It has the confocal scanning laser ophthalmoscope (cSLO) technology that not only offers documentation of clinical findings but also often highlights critical diagnostic details that are not visible on traditional clinical ophthalmoscopy. Since cSLO imaging minimizes the effects of light scatter, it can be used effectively even in patients with cataracts. For AI READI subjects, imaging is done in a dark room using the following modalities: ONH-RC, PPole-H, and OCTA of the macula. As the machine is operated by the imaging team and is not fully automated, quality issues may arise, which may lead to skipping this modality and missing data.\n\n   **Triton DRI OCT (Topcon Healthcare)**\n\n   The DRI OCT Triton is a device from Topcon Healthcare that combines swept-source OCT technology with multimodal fundus imaging. The DRI OCT Triton uses swept-source technology to visualize the deepest layers of the eye, including through cataracts. It also enhances visualization of outer retinal structures and deep pathologies. The DRI OCT Triton has a 1,050 nm wavelength light source and a non-mydriatic color fundus camera. AI READI imaging is done in a dark room with minimal intervention from the imager as the machine positioning is done automatically. This leads to higher quality images with minimal operator induced error. Imaging is done in 12.0X12.0 mm and 6.0X6.0 mm OCTA, and 12.0 mm X9.0 mmX6.0 mm 3D Horizontal and Radial scan modes.\n\n   **Maestro2 3D OCT (Topcon Healthcare)**\n\n   The Maestro2 is a robotic OCT and color fundus camera system from Topcon Healthcare. It can capture a 12 mm x 9 mm wide-field OCT scan that includes the macula and optic disc. The Maestro2 can also capture high-resolution non-mydriatic, true color fundus photography, OCT, and OCTA with a single button press. Imaging is done in a dark room and automatically with minimal involvement of the operator. Protocols include 12.0 mm X9.0 mm widefield, 6.0 mm X 6.0 mm 3D macula scan and 6.0 mm X 6.0 mm OCTA (scan rate: 50 kHz).\n\n   **FLIO (Heidelberg Engineering)**\n\n   Fluorescence Lifetime Imaging Ophthalmoscopy (FLIO) is an advanced imaging technique used in ophthalmology. It is a non-invasive method that provides valuable information about the metabolic and functional status of the retina. FLIO is based on the measurement of fluorescence lifetimes, which is the duration a fluorophore remains in its excited state before emitting a photon and returning to the ground state. FLIO utilizes this fluorescence lifetime information to capture and analyze the metabolic processes occurring in the retina. Different retinal structures and molecules exhibit distinct fluorescence lifetimes, allowing for the visualization of metabolic changes, cellular activity, and the identification of specific biomolecules. The imaging is done by an operator in a dark room analogous to a straightforward heidelberg spectralis OCT. However, as it takes longer than a usual spectralis OCT and exposes patients to uncomfortable levels of light, it is kept to be performed as the last modality of an AI READI visit. Because of this patients may not be at their best possible compliance.\n\n   **Cirrus 5000 Angioplex (Carl Zeiss Meditec)**\n\n   The Zeiss Cirrus 5000 Angioplex is a high-definition optical coherence tomography (OCT) system that offers non-invasive imaging of retinal microvasculature. The imaging is done in a dark room by an operator and it is pretty straightforward and analogous to what is done in the ophthalmology clinics on a day to day basis. Imaging protocols include 512 X 512 and 200 X 200 macula and ONH scans and also OCTA of the macula. Zeiss Cirrus 5000 also provides a 60-degree OCTA widefield view. 8x8mm single scans and 14x14mm automated OCTA montage allow for rapid peripheral assessment of the retina as well.\n\n   **Monofilament testing for peripheral neuropathy**\n\n   Monofilament test is a standard clinical test to monitor peripheral neuropathy in diabetic patients. It is done using a standard 10g monofilament applying pressure to different points on the plantar surface of the feet. If patients sense the monofilament, they confirm by saying \u201cyes\u201d; if patients do not sense the monofilament after it bends, they are considered to be insensate. When the sequence is completed, the insensate area is retested for confirmation. This sequence is further repeated randomly at each of the testing sites on each foot until results are obtained.The results are recorded on an iPad, Laptop, or a paper questionnaire and are directly added to the project's RedCap by the clinical research staff.\n\n   **Montreal Cognitive Assessment (MoCA)**\n\n   The Montreal Cognitive Assessment (MoCA) is a simple, in-office screening tool that helps detect mild cognitive impairment and early onset of dementia. The MoCA evaluates cognitive domains such as: Memory, Executive functioning, Attention, Language, Visuospatial, Orientation, Visuoconstructional skills, Conceptual thinking, Calculations. The MoCA generates a total score and six domain-specific index scores. The maximum score is 30, and anything below 24 is a sign of cognitive impairment. A final total score of 26 and above is considered normal. Some disadvantages of the MoCA include: Professionals require training to score the test, A person's level of education may affect the test, Socioeconomic factors may affect the test, People living with depression or other mental health issues may score similarly to those with mild dementia. AI READI research staff perform this test on an iPad using a pre-installed software (MoCA Duo app downloaded from the app store) that captures all the patients responses in an interactive manner."
          }
        ],
        "challenge": [
          {
            "id": 1,
            "question": "Which factors in the data might limit the generalization of potentially derived models? Is this information available as auxiliary labels for challenge tests? For instance:\n\n   a. Number and diversity of devices included in the dataset.\n\n   b. Data recording specificities, e.g., the view for a chest x-ray image.\n\n   c. Number and diversity of recording sites included in the dataset.\n\n   d. Distribution shifts over time.",
            "response": "While the AI-READI's cross-sectional database ultimately aims to achieve balance across race/ethnicity, biological sex, and diabetes presence and severity, the pilot study is not balanced across these parameters.\n\n   Three recording sites were strategically selected to achieve diverse recruitment: the University of Alabama at Birmingham (UAB), the University of California San Diego (UCSD), and the University of Washington (UW). The sites were chosen for geographic diversity across the United States and to ensure diverse representation across various racial and ethnic groups. Individuals from all demographic backgrounds were recruited at all 3 sites.\n\n   Factors influencing the generalization of derived models include the predominantly urban and hospital-based recruitment, which may not fully capture diverse cultural and socioeconomic backgrounds. The study cohort may not provide a comprehensive representation of the population, as it does not include other races/ethnicities such as Pacific Islanders and Native Americans.\n\n   Information on device make and model, including specific modalities like macula scans or wide scans during OCT, were documented to ensure repeatability. Moreover, the study included multiple devices for one measure to enhance generalizability and represent the diverse range of equipment utilized in clinical settings."
          },
          {
            "id": 2,
            "question": "What confounding factors might be present in the data?\n\n   a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another?\n\n   b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another?",
            "response": "a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another? b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another? Uniform data collection protocols were implemented for all subjects, irrespective of their race/ethnicity, biological sex, or diabetes severity, across all study sites. The selection of study sites was intended to ensure equitable representation and minimize the potential for sampling bias."
          }
        ],
        "demographicInformation": [
          {
            "id": 1,
            "question": "Does the dataset identify any demographic sub-populations (e.g., by age, gender, sex, ethnicity)?",
            "response": "No"
          },
          {
            "id": 2,
            "question": "If no,\n\n   a. Is there any regulation that prevents demographic data collection in your study (for example, the country that the data is collected in)?\n\n   b. Are you employing methods to reduce the disparity of error rate between different demographic subgroups when demographic labels are unavailable? Please describe.",
            "response": "No. We are suggesting a split for training/validation/testing models that is aimed at reducing disparities in models developed using this dataset."
          }
        ],
        "preprocessing": [
          {
            "id": 1,
            "question": "Was there any pre-processing for the de-identification of the patients? Provide the answer for the preliminary and the current version of the dataset",
            "response": ""
          },
          {
            "id": 2,
            "question": "Was there any pre-processing for cleaning the data? Provide the answer for the preliminary and the current version of the dataset",
            "response": "There were several quality control measures used at the time of data entry/acquisition. For example, clinical data outside of expected min/max ranges were flagged in REDCap, which was visible in reports viewed by clinical research coordinators (CRCs) and Data Managers. Using these REDCap reports as guides, Data Managers and CRCs examined participant records and determined if an error was likely. Data were checked for the following and edited if errors were detected:\n\n   1. Credibility, based on range checks to determine if all responses fall within a prespecified reasonable range\n\n   2. Incorrect flow through prescribed skip patterns\n\n   3. Missing data that can be directly filed from other portions of an individual's record\n\n   4. The omission and/or duplication of records\n\n   Editing was only done under the guidance and approval of the site PI. If corrected data was available from elsewhere in the respondent's answers, the error was corrected. If there was no logical or appropriate way to correct the data, the Data site PI reviewed the values and made decisions about whether those values should be removed from the data.\n\n   Once data were sent from each of the study sites to the central project team, additional processing steps were conducted in preparation for dissemination. For example, all data were mapped to standardized terminologies when possible, such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, a common data model for observational health data, and the Digital Imaging and Communications in Medicine (DICOM), a commonly used standard for medical imaging data. Details about the data processing approaches for each data domain/modality are described in the dataset documentation at https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Was the \u201craw\u201d data (post de-identification) saved in addition to the preprocessed/cleaned data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data",
            "response": "The raw data is saved and expected to be preserved by the AI-READI project at least for the duration of the project but is not anticipated to be shared outside the project team right now, because it has not been mapped to standardized terminologies and because the raw data may accidentally include personal health information or personally identifiable information (e.g. in free text fields). There is a possibility that raw data may be included in future releases of the controlled access dataset."
          },
          {
            "id": 4,
            "question": "Were instances excluded from the dataset at the time of preprocessing? If so, why? For example, instances related to patients under 18 might be discarded.",
            "response": "No data were excluded from the dataset at the time of preprocessing. However, regarding to study recruitment (i.e. ability to participate in the study), the following eligibility criteria were used:\n\n   Inclusion Criteria:\n\n   - Able to provide consent\n   - \u2265 40 years old\n   - Persons with or without type 2 diabetes\n   - Must speak and read English\n\n   Exclusion Criteria:\n\n   - Must not be pregnant\n   - Must not have gestational diabetes\n   - Must not have Type 1 diabetes"
          },
          {
            "id": 5,
            "question": "If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Answer this question for both the preliminary dataset and the current version of the dataset",
            "response": "N/A"
          }
        ],
        "labeling": [
          {
            "id": 1,
            "question": "Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version.\n\n   a. If yes:\n\n   1. What are the labels provided?\n\n   2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?\n\n   b. What labeling strategy was used?\n\n   1. Gold standard label available in the data (e.g. cancers validated by biopsies)\n\n   2. Proxy label computed from available data:\n\n      1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions)\n\n      2. Which tables and features were considered to compute the label?\n\n   3. Which proportion of the data has gold standard labels?\n\n   c. Human-labeled data\n\n   1. How many labellers were considered?\n\n   2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, \u2026)\n\n   3. What guidelines did they follow?\n\n   4. How many labellers provide a label per instance?\n\n      If multiple labellers per instance:\n\n      1. What is the rater agreement? How was disagreement handled?\n      2. Are all labels provided, or summaries (e.g. maximum vote)?\n\n   5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes)\n\n   6. On average, how much time was required to annotate each instance?\n\n   7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?",
            "response": "No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications."
          },
          {
            "id": 2,
            "question": "What are the human level performances in the applications that the dataset is supposed to address?",
            "response": "N/A"
          },
          {
            "id": 3,
            "question": "Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. ",
            "response": "N/A - no labeling was performed"
          },
          {
            "id": 4,
            "question": "Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?",
            "response": "No, we do not have formal guidelines in place."
          },
          {
            "id": 5,
            "question": "Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.",
            "response": "The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced for sex, race/ethnicity and diabetes status (with and without diabetes)."
          }
        ],
        "collection": [
          {
            "id": 1,
            "question": "Were any REB/IRB approval (e.g., by an institutional review board or research ethics board) received? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "The initial IRB approval at the University of Washington was received on December 20, 2022. The initial approval letter can be found here. Under FWA #00006878, the IRB approved activity for the AI-READI study from 12/16/2022 to 12/15/2023. A modification to the initial IRB application was filed on 5/5/2023 and approved on 5/10/2023. An annual renewal application to the IRB about the status and progress of the study is required and due within 90 days of expiration."
          },
          {
            "id": 2,
            "question": "How was the data associated with each instance acquired? Was the data directly observable (e.g., medical images, labs or vitals), reported by subjects (e.g., survey responses, pain levels, itching/burning sensations), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.",
            "response": "The acquisition of data varied based on the domain; some data were directly observable (such as labs, vitals, and retinal imaging), whereas other data were reported by subjects (e.g. survey responses). Verification of data entry was performed when possible (e.g. cross-referencing entered medications with medications that were physically brought in or photographed by each study participant). Details for each data domain are available in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Provide the answer for all modalities and collected data. Has this information been changed through the process? If so, explain why.",
            "response": "The procedures for data collection and processing is available at https://docs.aireadi.org."
          },
          {
            "id": 4,
            "question": "Who was involved in the data collection process (e.g., patients, clinicians, doctors, ML researchers, hospital staff, vendors, etc.) and how were they compensated (e.g., how much were contributors paid)?",
            "response": "Details about the AI-READI team members involved in the data collection process are available at https://aireadi.org/team. Their effort was supported by the National Institutes of Health award OT2OD032644 based on the percentage of effort contributed, and salaries which aligned with the funding guidelines at each site. Study subjects received a compensation of $200 for the study visit also through the grant funding."
          },
          {
            "id": 5,
            "question": "Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.",
            "response": "The timeline for the overall project spans four years, encompassing one year dedicated to protocol development and training, and years 2-4 allocated for subject recruitment and data collection. Approximately 4% of participants are expected to undergo a follow-up examination in Year 4. The data collection process is specifically tailored to enable downstream pseudotime manifold analysis\u2014an approach used to predict disease trajectories. This involves gathering and learning from complex, multimodal data from participants exhibiting varying disease severity, ranging from normal to insulin-dependent Type 2 Diabetes Mellitus (T2DM). The timeframe also allows for the collection of the highest number of subjects possible to ensure a balanced representation of racial and ethnic groups and mitigate biases in computer vision algorithms.\n\nFor this version of the dataset, the timeframe for data collection was July 18, 2023 to November 30, 2023."
          },
          {
            "id": 6,
            "question": "Does the dataset relate to people? If not, you may skip the remaining questions in this section.",
            "response": "Yes"
          },
          {
            "id": 7,
            "question": "Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., hospitals, app company)?",
            "response": "The data was collected directly from participants across the three recruiting sites. Recruitment pools were identified by screening Electronic Health Records (EHR) for diabetes and prediabetes ICD-10 codes for all patients who have had an encounter with the sites' health systems within the past 2 years.\n\n"
          },
          {
            "id": 8,
            "question": "Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.",
            "response": "Yes, each individual was aware of the data collection, as this was not passive data collection or secondary use of existing data, but rather active data collection directly from participants."
          },
          {
            "id": 9,
            "question": "Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.",
            "response": "Informed consent to participate was required before participation in any part of the protocol (including questionnaires). Potential participants were given the option to read all consent documentation electronically (e-consent) before their visit and give their consent with an electronic signature without verbal communication with a clinical research coordinator. Participants may access e-consent documentation in REDCap and decide at that point they do not want to participate or would like additional information. The approved consent form for the principal project site University of Washington is available here. The other clinical sites had IRB reliance and used the same consent form, with minor institution-specific language incorporated depending on individual institutional requirements."
          },
          {
            "id": 10,
            "question": "If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).",
            "response": "Participants were permitted to withdraw consent at any time and cease study participation. However, any data that had been shared or used up to that point would stay in the dataset. This is clearly communicated in the consent document."
          },
          {
            "id": 11,
            "question": "In which countries was the data collected?",
            "response": "USA"
          },
          {
            "id": 12,
            "question": "Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "No, a data protection impact analysis has not been conducted."
          }
        ],
        "inclusion": [
          {
            "id": 1,
            "question": "Is there any language-based communication with patients (e.g: English, French)? If yes, describe the choices of language(s) for communication. (for example, if there is an app used for communication, what are the language options?)",
            "response": "English language was used for communication with study participants."
          },
          {
            "id": 2,
            "question": "What are the accessibility measurements and what aspects were considered when the study was designed and implemented?",
            "response": "Accessibility measurements were not specifically assessed. However, transportation assistance (rideshare services) was offered to study participants who endorsed barriers to transporting themselves to study visits."
          },
          {
            "id": 3,
            "question": "If data is part of a clinical study, what are the inclusion criteria?",
            "response": "The eligibility criteria for the study were as follows:\n\n Inclusion Criteria:\n\n - Able to provide consent\n - \u2265 40 years old\n - Persons with or without type 2 diabetes\n - Must speak and read English\n\n Exclusion Criteria:\n\n - Must not be pregnant\n - Must not have gestational diabetes\n - Must not have Type 1 diabetes"
          }
        ],
        "uses": [
          {
            "id": 1,
            "question": "Has the dataset been used for any tasks already? If so, please provide a description. ",
            "response": "No"
          },
          {
            "id": 2,
            "question": "Does using the dataset require the citation of the paper or any other forms of acknowledgement? If yes, is it easily accessible through google scholar or other repositories",
            "response": "Yes, use of the dataset requires citation to the resources specified in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. (besides Google scholar)",
            "response": "No"
          },
          {
            "id": 4,
            "question": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?",
            "response": "No, to the extent of our knowledge, we do not currently anticipate any uses of the dataset that could result in unfair treatment or harm. However, there is a theoretical risk of future re-identification."
          },
          {
            "id": 5,
            "question": "Are there tasks for which the dataset should not be used? If so, please provide a description. (for example, dataset creators could recommend against using the dataset for considering immigration cases, as part of insurance policies)",
            "response": "This is answered in a prior question (see details regarding license terms)."
          }
        ],
        "distribution": [
          {
            "id": 1,
            "question": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.",
            "response": "The dataset will be distributed and be available for public use."
          },
          {
            "id": 2,
            "question": "How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?",
            "response": "The dataset will be available through the FAIRhub platform (http://fairhub.io/). The dataset' DOI is https://doi.org/10.60775/fairhub.1"
          },
          {
            "id": 3,
            "question": "When was/will the dataset be distributed?",
            "response": "The dataset was distributed in April 2024."
          },
          {
            "id": 4,
            "question": "Assuming the dataset is available, will it be/is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.",
            "response": "We provide here the license file containing the terms for reusing the AI-READI dataset (https://doi.org/10.5281/zenodo.10642459). These license terms were specifically tailored to enable reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purpose while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications."
          },
          {
            "id": 5,
            "question": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.10642459)"
          },
          {
            "id": 6,
            "question": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.10642459)"
          }
        ],
        "maintenance": [
          {
            "id": 1,
            "question": "Who is supporting/hosting/maintaining the dataset?",
            "response": "The AI-READI team will be supporting and maintaining the dataset. The dataset is hosted on FAIRhub through Microsoft Azure."
          },
          {
            "id": 2,
            "question": "How can the owner/curator/manager of the dataset be contacted (e.g. email address)?",
            "response": "We refer to the README file included with the dataset for contact information."
          },
          {
            "id": 3,
            "question": "Is there an erratum? If so, please provide a link or other access point.",
            "response": ""
          },
          {
            "id": 4,
            "question": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?",
            "response": "The dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants complete the study visit."
          },
          {
            "id": 5,
            "question": "If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.",
            "response": "There are no limits on the retention of the data associated with the instances."
          },
          {
            "id": 6,
            "question": "Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how and for how long. If not, please describe how its obsolescence will be communicated to users.",
            "response": "N/A - This is the first version of the dataset. In the future, when there are newer versions released, the previous versions will continue to be available on FAIRhub."
          },
          {
            "id": 7,
            "question": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?",
            "response": "No, currently there is no mechanism for others to extend or augment the AI-READI dataset outside of those who are involved in the project."
          }
        ]
      }
    },
    "files": [],
    "data": {
      "size": 48430000,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://rdr.ucl.ac.uk/articles/dataset/OCT5k_A_dataset_of_multi-disease_and_multi-graded_annotations_for_retinal_layers/22128671/4",
    "created": "1730966400"
  },
  {
    "id": 13,
    "canonicalId": "agudjafpa77j71jntu5gldy8",
    "datasetId": "agudjafpa77j71jntu5gldy8",
    "doi": "10.60775/fairhub.1",
    "title": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project",
    "description": "The AI-READI project seeks to create and share a flagship ethically-sourced dataset of type 2 diabetes.",
    "versionTitle": "1.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/study_description.json",
        "identificationModule": {
          "officialTitle": "AI Ready and Exploratory Atlas for Diabetes Insights",
          "acronym": "AI-READI",
          "orgStudyIdInfo": {
            "orgStudyId": "OT2OD032644",
            "orgStudyIdType": "U.S. National Institutes of Health (NIH) Grant/Contract Award Number",
            "orgStudyIdLink": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
          },
          "secondaryIdInfoList": [
            {
              "secondaryId": "NCT06002048",
              "secondaryIdType": "ClinicalTrials.gov",
              "secondaryIdLink": "https://classic.clinicaltrials.gov/ct2/show/NCT06002048"
            }
          ]
        },
        "statusModule": {
          "overallStatus": "Enrolling by invitation",
          "startDateStruct": {
            "startDate": "2023-07-19",
            "startDateType": "Actual"
          },
          "completionDateStruct": {
            "completionDate": "2027-01-01",
            "completionDateType": "Anticipated"
          }
        },
        "sponsorCollaboratorsModule": {
          "leadSponsor": {
            "leadSponsorName": "Washington University in St. Louis",
            "leadSponsorIdentifier": {
              "leadSponsorIdentifierValue": "https://ror.org/01yc7t268",
              "leadSponsorIdentifierScheme": "ROR",
              "schemeURI": "https://ror.org/"
            }
          },
          "responsibleParty": {
            "responsiblePartyType": "Principal Investigator",
            "responsiblePartyInvestigatorFirstName": "Aaron",
            "responsiblePartyInvestigatorLastName": "Lee",
            "responsiblePartyInvestigatorTitle": "Associate Professor",
            "responsiblePartyInvestigatorIdentifier": [
              {
                "responsiblePartyInvestigatorIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                "responsiblePartyInvestigatorIdentifierScheme": "ORCID",
                "schemeURI": "https://orcid.org/"
              }
            ],
            "responsiblePartyInvestigatorAffiliation": {
              "responsiblePartyInvestigatorAffiliationName": "Washington University in St. Louis",
              "responsiblePartyInvestigatorAffiliationIdentifier": {
                "responsiblePartyInvestigatorAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                "responsiblePartyInvestigatorAffiliationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          },
          "collaboratorList": [
            {
              "collaboratorName": "National Institutes of Health",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/01cwqze88",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "California Medical Innovations Institute",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0156zyn36",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Johns Hopkins University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00za53h95",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Oregon Health & Science University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/009avj582",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Stanford University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00f54p054",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of Alabama at Birmingham",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/008s83205",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of California, San Diego",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0168r3w48",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        },
        "oversightModule": {
          "isFDARegulatedDrug": "No",
          "isFDARegulatedDevice": "No",
          "humanSubjectReviewStatus": "Submitted, approved",
          "oversightHasDMC": "No"
        },
        "descriptionModule": {
          "briefSummary": "The study will collect a cross-sectional dataset of 4000 people across the US from diverse racial/ethnic groups who are either 1) healthy, or 2) belong in one of the three stages of diabetes severity (pre-diabetes/lifestyle controlled, oral medication and/or non-insulin-injectable medication controlled, or insulin dependent), forming a total of four groups of patients. Clinical data (social determinants of health surveys, continuous glucose monitoring data, biomarkers, genetic data, retinal imaging, cognitive testing, etc.) will be collected. The purpose of this project is data generation to allow future creation of artificial intelligence/machine learning (AI/ML) algorithms aimed at defining disease trajectories and underlying genetic links in different racial/ethnic cohorts. A smaller subgroup of participants will be invited to come for a follow-up visit in year 4 of the project (longitudinal arm of the study). Data will be placed in an open-source repository and samples will be sent to the study sample repository and used for future research.",
          "detailedDescription": "The Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available."
        },
        "conditionsModule": {
          "conditionList": [
            {
              "conditionName": "Type 2 Diabetes",
              "conditionIdentifier": {
                "conditionClassificationCode": "D003924",
                "conditionScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "conditionURI": "https://meshb.nlm.nih.gov/record/ui?ui=D003924"
              }
            }
          ],
          "keywordList": [
            {
              "keywordValue": "Retinal Imaging"
            },
            {
              "keywordValue": "Data Sharing",
              "keywordIdentifier": {
                "keywordClassificationCode": "D033181",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D033181"
              }
            },
            {
              "keywordValue": "Exploratory Data Collection"
            },
            {
              "keywordValue": "Machine Learning",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000069550",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
              }
            },
            {
              "keywordValue": "Artificial Intelligence",
              "keywordIdentifier": {
                "keywordClassificationCode": "D001185",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
              }
            },
            {
              "keywordValue": "Electrocardiography",
              "keywordIdentifier": {
                "keywordClassificationCode": "D004562",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
              }
            },
            {
              "keywordValue": "Continuous Glucose Monitoring",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000095583",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
              }
            },
            {
              "keywordValue": "Retinal imaging"
            },
            {
              "keywordValue": "Eye exam"
            }
          ]
        },
        "designModule": {
          "studyType": "Observational",
          "isPatientRegistry": "No",
          "designInfo": {
            "designObservationalModelList": [
              "Cohort"
            ],
            "designTimePerspectiveList": [
              "Cross-sectional"
            ]
          },
          "bioSpec": {
            "bioSpecRetention": "Samples With DNA",
            "bioSpecDescription": "Participants who consent are asked to provide both blood and urine for research use. There are three distinct elements that follow from these collections; first, a small amount of the collected blood is sent to the local collection site hospital lab for a complete blood cell count on the day of the encounter. Second, an additional portion of the blood and the urine sample are processed and stored at the collection site for batch shipment to the University of Washington Nutrition and Obesity Research Center for specialized lab tests. Third, the remaining majority of the collected blood is processed into a variety of derivatives that are being used to establish a large repository of biospecimens at the University of Alabama at Birmingham to be used in research to further our understanding of diabetes, diabetic associated eye disease and other applications related to health and related diseases. These derivatives include isolated plasma, serum, DNA, buffy coats (white blood cells), blood stabilized for future isolation of RNA and peripheral blood mononuclear cells (PBMC). These derivatives are separated into small aliquots to facilitate future approved research test and are stored at either -80oC or at cryogenic temperatures (PBMC)."
          },
          "enrollmentInfo": {
            "enrollmentCount": "4000",
            "enrollmentType": "Anticipated"
          }
        },
        "armsInterventionsModule": {
          "armGroupList": [
            {
              "armGroupLabel": "Healthy",
              "armGroupDescription": "Participants who do not have Type 1 or Type 2 Diabetes",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Pre-diabetes/Lifestyle Controlled",
              "armGroupDescription": "Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifesyle adjustments",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Oral Medication and/or Non-insulin-injectable Medication Controlled",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Insulin Dependent",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            }
          ],
          "interventionList": [
            {
              "interventionType": "Other",
              "interventionName": "AI-READI protocol",
              "interventionDescription": "Custom protocol followed for the AI-READI study. Details are available at in the documentation of v2.0.0 of the dataset at https://docs.aireadi.org/"
            }
          ]
        },
        "eligibilityModule": {
          "sex": "All",
          "genderBased": "No",
          "minimumAge": "40 Years",
          "maximumAge": "85 Years",
          "healthyVolunteers": "No",
          "eligibilityCriteria": {
            "eligibilityCriteriaInclusion": [
              "Adults (\u2265 40 years old)",
              "Patients with and without type 2 diabetes",
              "Able to provide consent",
              "Must be able to read and speak English"
            ],
            "eligibilityCriteriaExclusion": [
              "Adults older than 85 years of age",
              "Pregnancy",
              "Gestational diabetes",
              "Type 1 diabetes"
            ]
          },
          "studyPopulation": "Adult patients will be recruited into one of four groups: 1) healthy/no diabetes, 2) pre-diabetes/borderline diabetes/lifestyle-controlled diabetes, 3) oral medication and/or non-insulin injectable medication controlled type 2 diabetes, or 4) insulin dependent type 2 diabetes. The investigators aim to recruit approximately 1000 patients into each of the four groups. Patients will be recruited from University of Washington (UW), University of California at San Diego (UCSD), and University of Alabama at Birmingham (UAB). The study aims to recruit 1,000 subjects from each of the following racial and ethnic groups: White, Asian, Hispanic, and Black. Subjects will be age- and sex-matched within and between groups.",
          "samplingMethod": "Non-Probability Sample"
        },
        "contactsLocationsModule": {
          "centralContactList": [
            {
              "centralContactFirstName": "Aaron",
              "centralContactLastName": "Lee",
              "centralContactDegree": "MD",
              "centralContactIdentifier": [
                {
                  "centralContactIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "centralContactIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "centralContactAffiliation": {
                "centralContactAffiliationName": "Washington University in St. Louis",
                "centralContactAffiliationIdentifier": {
                  "centralContactAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "centralContactAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "centralContactEMail": "contact@aireadi.org"
            }
          ],
          "overallOfficialList": [
            {
              "overallOfficialFirstName": "Aaron",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Washington University in St. Louis",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cecilia",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-1994-7213",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Washington University in St. Louis",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Amir",
              "overallOfficialLastName": "Bahmani",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-4533-9334",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sally L.",
              "overallOfficialLastName": "Baxter",
              "overallOfficialDegree": "MD, MSc",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-5271-7690",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Christopher G.",
              "overallOfficialLastName": "Chute",
              "overallOfficialDegree": "MD, DrPH",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-5437-2545",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Jorge",
              "overallOfficialLastName": "Contreras",
              "overallOfficialDegree": "J.D.",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7899-3060",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Utah",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/03r0ha626",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Nicholas",
              "overallOfficialLastName": "Evans",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-3330-0224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Massachusetts Lowell",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/03hamhx47",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Samantha",
              "overallOfficialLastName": "Hurst",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9843-2845",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "T. Y. Alvin",
              "overallOfficialLastName": "Liu",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-2957-0755",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Gerald",
              "overallOfficialLastName": "McGwin",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9592-1133",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Shannon",
              "overallOfficialLastName": "McWeeney",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-8333-6607",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Oregon Health & Science University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/009avj582",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cynthia",
              "overallOfficialLastName": "Owsley",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-3424-011X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Bhavesh",
              "overallOfficialLastName": "Patel",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-0307-262X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "California Medical Innovations Institute",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0156zyn36",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Michael",
              "overallOfficialLastName": "Snyder",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-0784-7987",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sara J.",
              "overallOfficialLastName": "Singer",
              "overallOfficialDegree": "MBA, PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "http://orcid.org/0000-0002-3374-1177",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Linda M.",
              "overallOfficialLastName": "Zangwill",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-1143-5224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            }
          ],
          "locationList": [
            {
              "locationFacility": "University of Alabama at Birmingham",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Birmingham",
              "locationZip": "35233",
              "locationState": "Alabama",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/008s83205",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of California, San Diego",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "San Diego",
              "locationZip": "92093",
              "locationState": "California",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/0168r3w48",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of Washington",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Seattle",
              "locationZip": "98109",
              "locationState": "Washington",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/00cvxb145",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        }
      },
      "readme": "## Overview of the study\n\nThe Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available. This project will also create a roadmap for ethical and equitable research that focuses on the diversity of the research participants and the workforce involved at all stages of the research process (study design and data collection, curation, analysis, and sharing and collaboration).\n\n## Description of the dataset\n\nThis dataset contains data from 204 participants from the pilot period of the AI-READI project (July 19, 2023 to November 30, 2023). Data from multiple modalities are included. A full list is provided in the `Data Standards` section below. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed.\n\nThe dataset contains 21,669 files and is around 310 GB in size.\n\nA detailed description of the dataset is available in the AI-READI documentation for v1.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Protocol\n\nThe protocol followed for collecting the data can be found in the AI-READI documentation for v1.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Dataset access/restrictions\n\nAccessing the dataset requires several steps, including:\n\n- Login in through a verified ID system\n- Agreeing to use the data only for type 2 diabetes related research.\n- Agreeing to the license terms which set certain restrictions and obligations for data usage (see `License` section below).\n\n## Data standards followed\n\nThis dataset is organized following the [Clinical Dataset Structure (CDS) v0.1.0](https://cds-specification.readthedocs.io/en/v0.1.0/). We refer to the CDS documentation for more details. Briefly, data is organized at the root level into one directory per datatype (c.f. Table below). Within each datatype folder, there is one folder per modality. Within each modality folder, there is one folder per device used to collect that modality. Within each device folder, there is one folder per participant. Each datatype, modality, and device folder is named using a name that best defines it. Each participant folder is named after the participant's ID number used in the study. For each datatype, the data files follow the standards listed in the Table below. More details are available in the dataset_structure_description.json metadata file included in this dataset.\n\n| Datatype directory name   | Description                                                                                                                                                                                      | File format standard followed                                                                                                                                     |\n| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cardiac_ecg               | This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.      | [WaveForm DataBase (WFDB)](https://wfdb.readthedocs.io/en/latest/wfdb.html)                                                                                       |\n| clinical_data             | This directory contains clinical data collected through REDCap. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.                                                  | [Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi)                                               |\n| environment               | This directory contains data collected through an environmental sensor device custom built for the AI-READI project.                                                                             | [Earth Science Data Systems (ESDS) format](https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data) |\n| retinal_flio              | This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores. | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_oct               | This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.                                   | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_octa              | This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.                     | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_photography       | This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.                                                                          | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| wearable_activity_monitor | This directory contains data collected through a wearable fitness tracker.                                                                                                                       | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n| wearable_blood_glucose    | This directory contains data collected through a continuous glucose monitoring (CGM) device.                                                                                                     | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n\n## Resources\n\nAll of our data files are in formats that are accessible with free software commonly used for such data types so no specific software is required. Some useful resources related to this dataset are listed below:\n\n- Documentation of the dataset: [docs.aireadi.org](https://docs.aireadi.org/) (see 'Dataset v1.0.0' for this version of the dataset)\n- AI-READI project website: [aireadi.org](https://aireadi.org/)\n- Zenodo community of the AI-READI project: [zenodo.org/communities/aireadi](https://zenodo.org/communities/aireadi)\n- GitHub organization of the AI-READI project: [github.com/AI-READI](https://github.com/AI-READI)\n\n## License\n\nThis work is licensed under a custom license specifically tailored to enable the reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purposes while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications. More details are available in the License file included in the dataset and also available at https://doi.org/10.5281/zenodo.10642459.\n\n## How to cite\n\nIf you use this dataset for any purpose, please cite the resources specified in the AI-READI documentation for version 1.0.0 of the dataset at https://docs.aireadi.org.\n\n## Contact\n\nFor any questions, suggestions, or feedback related to this dataset, please email contact@aireadi.org. We refer to the study_description.json and dataset_description.json metadata files included in this dataset for additional information about the contact person/entity, authors, and contributors of the dataset.\n\n## Acknowledgement\n\nThe AI-READI project is supported by NIH grant [1OT2OD032644](https://reporter.nih.gov/search/1ADgncihCk6fdMRJdCnBjg/project-details/10471118) through the NIH Bridge2AI Common Fund program.",
      "datasetDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/dataset_description.json",
        "identifier": {
          "identifierValue": "10.60775/fairhub.3",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project"
          },
          {
            "titleValue": "AI-READI dataset",
            "titleType": "AlternativeTitle"
          }
        ],
        "version": "3.0.0",
        "creator": [
          {
            "creatorName": "AI-READI Consortium",
            "nameType": "Organizational"
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-11-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on FAIRhub"
          },
          {
            "dateValue": "2023-07-19/2025-05-01",
            "dateType": "Collected",
            "dateInformation": "Period when the data was collected"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Type 2 Diabetes",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No identifiers were collected so no active de-identification was necessary but we checked that no identifiable data per US HIPAA were present in the data."
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": "The public version of the dataset can only be used for type 2 diabetes related research. A private version will allow for more generic use."
        },
        "description": [
          {
            "descriptionValue": "This dataset contains data from 2280 participants that was collected between from July 19, 2023 and May 01, 2025. Data from multiple modalities are included. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed. A detailed description of the dataset is available in the AI-READI documentation for v3.0.0 of the dataset at https://docs.aireadi.org",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://docs.aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          },
          {
            "relatedIdentifierValue": "https://aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          }
        ],
        "subject": [
          {
            "subjectValue": "Diabetes mellitus",
            "subjectIdentifier": {
              "classificationCode": "45636-8",
              "subjectScheme": "Logical Observation Identifier Names and Codes (LOINC)",
              "schemeURI": "https://loinc.org/",
              "valueURI": "https://loinc.org/45636-8"
            }
          },
          {
            "subjectValue": "Machine Learning",
            "subjectIdentifier": {
              "classificationCode": "D000069550",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
            }
          },
          {
            "subjectValue": "Artificial Intelligence",
            "subjectIdentifier": {
              "classificationCode": "D001185",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
            }
          },
          {
            "subjectValue": "Electrocardiography",
            "subjectIdentifier": {
              "classificationCode": "D004562",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
            }
          },
          {
            "subjectValue": "Continuous Glucose Monitoring",
            "subjectIdentifier": {
              "classificationCode": "D000095583",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
            }
          },
          {
            "subjectValue": "Retinal imaging"
          },
          {
            "subjectValue": "Eye exam"
          }
        ],
        "managingOrganization": {
          "name": "Washington University in St. Louis",
          "managingOrganizationIdentifier": {
            "managingOrganizationIdentifierValue": "https://ror.org/01yc7t268",
            "managingOrganizationScheme": "ROR",
            "schemeURI": "https://ror.org"
          }
        },
        "accessType": "PublicDownloadSelfAttestationRequired",
        "accessDetails": {
          "description": "Accessing the dataset requires several steps, including: Login in through a verified ID system, Agreeing to use the data only for type 2 diabetes related research, Agreeing to the license terms which set certain restrictions and obligations for data usage (see 'rights' property)"
        },
        "rights": [
          {
            "rightsName": "AI-READI custom license v2.0",
            "rightsURI": "https://doi.org/10.5281/zenodo.17555036"
          }
        ],
        "publisher": {
          "publisherName": "FAIRhub"
        },
        "size": [
          "3.82 TB",
          "356343 files"
        ],
        "fundingReference": [
          {
            "funderName": "National Institutes of Health",
            "funderIdentifier": {
              "funderIdentifierValue": "https://ror.org/01cwqze88",
              "funderIdentifierType": "ROR",
              "schemeURI": "https://ror.org"
            },
            "awardNumber": {
              "awardNumberValue": "OT2OD032644",
              "awardURI": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
            },
            "awardTitle": "Bridge2AI: Salutogenesis Data Generation Project"
          }
        ],
        "format": [
          "application/dicom",
          "text/markdown",
          "text/csv",
          "application/json"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [
          {
            "directoryName": "cardiac_ecg",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Electrocardiogram",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C168186",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C168186"
                  }
                ]
              },
              {
                "relatedTermValue": "Electrocardiography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D004562",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "WaveForm DataBase (WFDB)",
                "standardDescription": "Set of file standards designed for reading and storing physiologic signal data, and associated annotations.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://wfdb.readthedocs.io/en/latest/wfdb.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "ecg_12lead",
                "directoryType": "modality",
                "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Electrocardiography",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other",
                    "relatedIdentifierDescription": "This page describes 3 types of ECG protocol including the 12 lead (standard) protocol."
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "philips_tc30",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol using the Philips PageWriter TC30 device.",
                    "relatedIdentifier": [
                      {
                        "relatedIdentifierValue": "https://www.documents.philips.com/doclib/enc/fetch/2000/4504/577242/577243/577246/581601/711562/DXL_ECG_Algorithm_Physician_s_Guide_(ENG)_Ed.2.pdf",
                        "relatedIdentifierType": "URL",
                        "relationType": "IsDocumentedBy",
                        "resourceTypeGeneral": "Other",
                        "relatedIdentifierDescription": "The 'Philips DXL ECG Algorithm Physician\u2019s Guide' contains information on the fields that are printed on an ECG report."
                      }
                    ]
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS) v0.1.1",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 302931703,
            "numberOfFiles": 4515
          },
          {
            "directoryName": "clinical_data",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains clinical data collected through REDCap, including blood/urine lab values and survey data. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Clinical Data",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C15783",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C15783"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory is named following the specification of this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "The Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)",
                "standardDescription": "Standard designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence.",
                "standardUse": "All the data files within this directory follow this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.qk984b",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "dqd_omop.json",
                "metadataFileDescription": "The dqd_omop.json file supports OMOP CDM data quality analysis using the OMOP CDM Data Quality Dashboard (DQD). The OMOP CDM DQD tool (https://ohdsi.github.io/DataQualityDashboard/) runs a set of > 3500 data quality checks against an OMOP CDM instance",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/DataQualityDashboard/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 176182781,
            "numberOfFiles": 7
          },
          {
            "directoryName": "environment",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through an environmental sensor device custom built for the AI-READI project.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Environmental sensor data"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "ASCII File Format Guidelines for Earth Science Data",
                "standardDescription": "NASA recommended practices for formatting and describing ASCII encoded data files",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "environmental_sensor",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "leelab_anura",
                    "directoryDescription": "You can learn more about the data in this directory by consulting relevant documentation. Search 'AS7431' with Type set as 'Datasheet' at https://ams-osram.com/support/download-center. Search 'DS3231 Precision RTC' at https://www.adafruit.com, look for product ID 5188, select this link and scroll down to Technical Details to find a link for the Datasheet (as of this writing, you may find the datasheet at https://www.analog.com/media/en/technical-documentation/data-sheets/DS3231.pdf). Search 'Datasheet SEN5x' in the search box at https://sensirion.com/ to get the document titled 'Datasheet SEN5x' (the name of the downloaded file may be 'Sensirion_Datasheet_Environmental_Node_SEN5x.pdf'). Search 'NOx Index' in the search box at https://sensirion.com to get the document titled 'What is Sensirion's NOx Index?' (the name of the downloaded file may be 'Info_Note NOx_Index.pdf').",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 55625676514,
            "numberOfFiles": 2232
          },
          {
            "directoryName": "retinal_flio",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Fuorescence Lifetime Imaging Ophthalmoscopy"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "flio",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_flio",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1069466876718,
            "numberOfFiles": 7969
          },
          {
            "directoryName": "retinal_oct",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C20828",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C20828"
                  }
                ]
              },
              {
                "relatedTermValue": "Tomography, Optical Coherence",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D041623",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D041623"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "structural_oct",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1317625293027,
            "numberOfFiles": 56478
          },
          {
            "directoryName": "retinal_octa",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography"
              },
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography Images"
              },
              {
                "relatedTermValue": "OCTA Images"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "enface",
                "directoryType": "modality",
                "directoryDescription": "This directory contains en face data collected using OCTA. En face images, derived from 3D volume scans, are also referred to as C-scan OCT. These images provide a 2D view of the retina layers and have an orientation siimilar to fundus photographs.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "flow_cube",
                "directoryType": "modality",
                "directoryDescription": "This directory contains flow cube data collected using OCTA. Flow cube provides information on blood flow in a 3D view.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Maestro2 device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Triton device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "segmentation",
                "directoryType": "modality",
                "directoryDescription": "This directory contains segmentation data collected using OCTA. The segmentation information is presented in the form of heightmaps and includes information about the associated layers.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Maestro device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1155908809724,
            "numberOfFiles": 173721
          },
          {
            "directoryName": "retinal_photography",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              },
              {
                "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Fundus_photography",
                "relatedIdentifierType": "URL",
                "relationType": "IsDescribedBy",
                "resourceTypeGeneral": "Other"
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Eye Fundus Photography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C147467",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C147467"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "cfp",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ophthalmology.med.ubc.ca/patient-care/ophthalmic-photography/color-fundus-photography/#:~:text=Color%20Fundus%20Retinal%20Photography%20uses,monitor%20their%20change%20over%20time",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "optomed_aurora",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Aurora device, manufactured by Optomed."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Triton device, manufactured by Topcon."
                  }
                ]
              },
              {
                "directoryName": "faf",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specificallly fundus autofluorescence photographs that uses the fluorescent characteristics of lipofuscin in an non-invasive way.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://eyewiki.aao.org/Fundus_Autofluorescence",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains fundus autofluorescence photographs from the Eidon device, manufactured by iCare."
                  }
                ]
              },
              {
                "directoryName": "ir",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data using near-infrared reflectance (IR).",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8349282/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 174381046406,
            "numberOfFiles": 93921
          },
          {
            "directoryName": "wearable_activity_monitor",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a wearable fitness tracker.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "smartwatch"
              },
              {
                "relatedTermValue": "activity monitoring"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "heart_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "oxygen_saturation",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity_calorie",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "respiratory_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "sleep",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "stress",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 38313536220,
            "numberOfFiles": 15245
          },
          {
            "directoryName": "wearable_blood_glucose",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a continuous glucose monitoring (CGM) device.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Continuous Glucose Monitoring System",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C159776",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C159776"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "continuous_glucose_monitoring",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "dexcom_g6",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 4169006971,
            "numberOfFiles": 2246
          }
        ],
        "metadataFileList": [
          {
            "metadataFileName": "CHANGELOG.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_structure_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "healthsheet.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "LICENSE.txt",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.tsv",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "README.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "study_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          }
        ]
      },
      "healthsheet": {
        "generalInformation": [
          {
            "id": 1,
            "question": "Provide a 2 sentence summary of this dataset.",
            "response": "The Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) is a dataset consisting of data collected from individuals with and without \u201cType 2 Diabetes Mellitus (T2DM)\u201d and harmonized across 3 data collection sites. The composition of the dataset was designed with future studies using AI/Machine Learning in mind. This included recruitment sampling procedures aimed at achieving approximately equal distribution of participants across sex, race, and diabetes severity, as well as the design of a data acquisition protocol across multiple domains (survey data, physical measurements, clinical data, imaging data, wearable device data, etc.) to enable downstream AI/ML analyses that may not be feasible with existing data sources such as claims or electronic health records data. The goal is to better understand salutogenesis (the pathway from disease to health) in T2DM. Some data that are not considered to be sensitive personal health data will be available to the public for download upon agreement with a license that defines how the data can be used. The full dataset will be accessible by entering into a data use agreement. The public dataset will include survey data, blood and urine lab results, fitness activity levels, clinical measurements (e.g. monofilament and cognitive function testing), retinal images, ECG, blood sugar levels, and home air quality. The data held under controlled access include 5-digit zip code, sex, race, ethnicity, genetic sequencing data, past health records, and traffic and accident reports. Of note, the overall enrollment goal is to have balanced distribution between different racial groups. As enrollment is ongoing, the periodic updates to data releases may not have achieved balanced distribution across groups."
          },
          {
            "id": 2,
            "question": "Has the dataset been audited before? If yes, by whom and what are the results?",
            "response": "The dataset has not undergone any formal external audits. However, the dataset has been reviewed internally by AI-READI team members for quality checks and to ensure that no personally identifiable information was accidentally included."
          }
        ],
        "versioning": [
          {
            "id": 1,
            "question": "Does the dataset get released as static versions or is it dynamically updated? a. If static, how many versions of the dataset exist? b. If dynamic, how frequently is the dataset updated?",
            "response": "The dataset gets released as static versions. This is the third version of the dataset and consists of data collected up through the end of the second year of the study, i.e. between July 19, 2023 and May 1st, 2025. There are plans to release new versions of the dataset approximately once a year with additional data from participants who have been enrolled since the last dataset version release."
          },
          {
            "id": 2,
            "question": "Is this datasheet created for the original version of the dataset? If not, which version of the dataset is this datasheet for?",
            "response": "This datasheet is created for the third version of the dataset."
          },
          {
            "id": 3,
            "question": "Are there any datasheets created for any versions of this dataset?",
            "response": "There was a previous datasheet created for the first version of the dataset, which consisted of data collected during the pilot data collection phase. It is available here: https://docs.aireadi.org/docs/1/dataset/healthsheet. There was also a previous datasheet created for the second version of the dataset, which consisted of data collected up to the end of the first full year of the study. It is available here: https://docs.aireadi.org/docs/2/dataset/healthsheet"
          },
          {
            "id": 4,
            "question": "Does the current version/subversion of the dataset come with predefined task(s), labels, and recommended data splits (e.g., for training, development/validation, testing)? If yes, please provide a high-level description of the introduced tasks, data splits, and labeling, and explain the rationale behind them. Please provide the related links and references. If not, is there any resource (website, portal, etc.) to keep track of all defined tasks and/or associated label definitions? (please note that more detailed questions w.r.t labeling is provided in further sections)",
            "response": "See response to question #6 under \u201cLabeling and subjectivity of labeling\u201d."
          },
          {
            "id": 5,
            "question": "If the dataset has multiple versions, and this datasheet represents one of them, answer the following questions: a. What are the characteristics that have been changed between different versions of the dataset? ",
            "response": "This version of the dataset includes more patients than the first version of the dataset."
          },
          {
            "id": 6,
            "question": "b. Explain the motivation/rationale for creating the current version of the dataset.",
            "response": "The current version of the dataset includes data from the second year of the study, rather than only the data collected from the pilot data collection phase (which comprised the first version of the dataset) and the first year of the study (which comprised the second version of the dataset)."
          },
          {
            "id": 7,
            "question": "c. Does this version have more subjects/patients represented in the data, or fewer?",
            "response": "This version has more subjects/patients represented in the data (the first version of the dataset contains data from 204 participants, the second version contains additional data from 863 participants for a total of 1067 participants, and this version contains a total of 2280 participants)."
          },
          {
            "id": 8,
            "question": "d. Does this version of the dataset have extended data or new data from the same patients as the older versions? Were any patients, data fields, or data points removed? If so, why?",
            "response": "The data fields/types are largely the same as the prior version of the dataset. However, the Snellen visual acuity variables were dropped in this version of the dataset, such that now only logMAR visual acuity measurements are available."
          },
          {
            "id": 9,
            "question": "e. Do we expect more versions of the dataset to be released?",
            "response": "Yes, enrollment is ongoing, and future versions of the dataset will be released that will include larger numbers of subjects/patients as enrollment increases."
          },
          {
            "id": 10,
            "question": "f. Is this datasheet for a version of the dataset? If yes, does this sub-version of the dataset introduce a new task, labeling, and/or recommended data splits? If the answer to any of these questions is yes, explain the rationale behind it.",
            "response": "This datasheet does not include new tasks or labeling. It does include a new recommended data split for the 2280 participants balancing training, validation, and testing sets for age, sex, races/ethnicities, and study group."
          },
          {
            "id": 11,
            "question": "g. Are you aware of any widespread version(s)/subversion(s) of the dataset? If yes, what is the addressed task, or application that is addressed?",
            "response": "No"
          }
        ],
        "motivation": [
          {
            "id": 1,
            "question": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.",
            "response": "The purpose for creating the dataset was to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. T2DM is a growing public health threat. Yet, the current understanding of T2DM, especially in the context of salutogenesis, is limited. Given the complexity of T2DM, AI-based approaches may help with improving our understanding but a key issue is the lack of data ready for training AI models. The AI-READI dataset is intended to fill this gap."
          },
          {
            "id": 2,
            "question": "What are the applications that the dataset is meant to address? (e.g., administrative applications, software applications, research)",
            "response": "The multi-modal dataset being collected is being gathered to facilitate downstream pseudotime manifolds and various applications in artificial intelligence."
          },
          {
            "id": 3,
            "question": "Are there any types of usage or applications that are discouraged from using this dataset? If so, why?",
            "response": "The AI READI dataset License imposes certain restrictions on the usage of the data. The restrictions are described in the License files available at https://doi.org/10.5281/zenodo.17555036."
          },
          {
            "id": 4,
            "question": "Who created this dataset (e.g., which team, research group), and on behalf of which entity (e.g., company, institution, organization)?",
            "response": "This dataset was created by members of the AI-READI project, hereby referred to as the AI-READI Consortium. Details about each member and their institutions are available on the project website at https://aireadi.org."
          },
          {
            "id": 5,
            "question": "Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. If the funding institution differs from the research organization creating and managing the dataset, please state how.",
            "response": "The creation of the dataset was funded by the National Institutes of Health (NIH) through their Bridge2AI Program (https://commonfund.nih.gov/bridge2ai). The grant number is OT2ODO32644 and more information about the funding is available at https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481. Note that the funding institution is not creating or managing the dataset. The dataset is created and managed by the awardees of the grant (c.f. answer to the previous question)."
          },
          {
            "id": 6,
            "question": "What is the distribution of backgrounds and experience/expertise of the dataset curators/generators?",
            "response": "There is a wide range of experience within the project team, including senior, mid-career, and early career faculty members as well as clinical research coordinators, staff, and interns. They collectively cover many areas of expertise including clinical research, data collection, data management, data standards, bioinformatics, team science, and ethics, among others. Visit https://aireadi.org/team for more information."
          }
        ],
        "composition": [
          {
            "id": 1,
            "question": "What do the instances that comprise the dataset represent (e.g., documents, images, people, countries)? Are there multiple types of instances? Please provide a description.",
            "response": "Each instance represents an individual patient."
          },
          {
            "id": 2,
            "question": "How many instances are there in total (of each type, if appropriate) (breakdown based on schema, provide data stats)?",
            "response": "There are 2280 instances in this current version of the dataset (version 3, released fall 2025)."
          },
          {
            "id": 3,
            "question": "How many patients / subjects does this dataset represent? Answer this for both the preliminary dataset and the current version of the dataset.",
            "response": "This version of the dataset has data from 2280 participants. The first version of the dataset, composed of data from the pilot data collection phase, had 204 instances. The second version of the dataset, composed of data from the first year of the study, had 1067 instances."
          },
          {
            "id": 4,
            "question": "Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). Answer this question for the preliminary version and the current version of the dataset in question.",
            "response": "The dataset contains all possible instances. More specifically, the dataset contains data from all participants who have been enrolled during the first year of data collection for AI-READI."
          },
          {
            "id": 5,
            "question": "What data modality does each patient data consist of? If the data is hierarchical, provide the modality details for all levels (e.g: text, image, physiological signal). Break down in all levels and specify the modalities and devices.",
            "response": "Multiple modalities of data are collected for each participant, including survey data, clinical data, retinal imaging data, environmental sensor data, continuous glucose monitor data, and wearable activity monitor data. These encompass tabular data, imaging data, and physiological signal/waveform data. There is no unstructured text data included in this dataset. The exact forms used for data collection in REDCap are available [here](https://docs.aireadi.org/v3/REDCap%20surveys%20and%20forms.pdf). Furthermore, all modalities, file formats, and devices are detailed in the dataset documentation at https://docs.aireadi.org/."
          },
          {
            "id": 6,
            "question": "What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.",
            "response": "Each instance consists of all of the data available for an individual participating in the study. See answer to question 5 for the data types associated with each instance."
          },
          {
            "id": 7,
            "question": "Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).?",
            "response": "Yes, not all modalities are available for all participants. Some participants elected not to participate in some study elements. In a few cases, the data collection device did not have any stored results or was returned too late to retrieve the results (e.g. battery died, data was lost). In a few cases, there may have been a data collision at some point in the process and data has been lost."
          },
          {
            "id": 8,
            "question": "Are relationships between individual instances made explicit? (e.g., They are all part of the same clinical trial, or a patient has multiple hospital visits and each visit is one instance)? If so, please describe how these relationships are made explicit.",
            "response": "Yes - all instances are part of the same prospective data generation project (AI-READI). There is currently only one visit per participant."
          },
          {
            "id": 9,
            "question": "Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. (e.g., losing data due to battery failure, or in survey data subjects skip the question, radiological sources of noise).",
            "response": "In cases of survey data, skipped questions or incomplete responses are expected. In cases of using wearables, improper use, technical failure such as battery failure or system malfunction are expected. In cases of imaging data, patient uncooperation, noise that may obscure the images and technical failure such as system malfunction, and data transfer failures are expected."
          },
          {
            "id": 10,
            "question": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, other datasets)? If it links to or relies on external resources, a. are there guarantees that they will exist, and remain constant, over time; b. are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c. are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.",
            "response": "The dataset is self-contained but does rely on the dataset documentation for users requiring additional information about the provenance of the dataset. The documentation is available at https://docs.aireadi.org. The documentation is shared under the CC-BY 4.0 license, so there are no restrictions associated with its use."
          },
          {
            "id": 11,
            "question": "Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications that is confidential)? If so, please provide a description.",
            "response": "No, the dataset does not contain data that might be considered confidential. No personally identifiable information is included in the dataset."
          },
          {
            "id": 12,
            "question": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise pose any safety risk (such as psychological safety and anxiety)? If so, please describe why.",
            "response": "No."
          },
          {
            "id": 13,
            "question": "If the dataset has been de-identified, were any measures taken to avoid the re-identification of individuals? Examples of such measures: removing patients with rare pathologies or shifting time stamps.",
            "response": ""
          },
          {
            "id": 14,
            "question": "Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.",
            "response": "No, the public dataset will not contain data that is considered sensitive. However, the controlled access dataset will contain data regarding racial and ethnic origins, location (5-digit zip code), as well as motor vehicle accident reports."
          }
        ],
        "devices": [
          {
            "id": 1,
            "question": "For data that requires a device or equipment for collection or the context of the experiment, answer the following additional questions or provide relevant information based on the device or context that is used (for example) a. If there was an MRI machine used, what is the MRI machine and model used? b. If heart rate was measured what is the device for heart rate variation that is used? c. If cortisol measurement is reported at multi site, provide details, d. If smartphones were used to collect the data, provide the names of models. e. And so on,..",
            "response": "The devices included in the study are as follows, and more details can be found at [https://docs.aireadi.org](https://docs.aireadi.org): **Environmental sensor device** Participants will be sent home with an environmental sensor (a custom-designed sensor unit called the LeeLab Anura), which they will use for 10 continuous days before returning the devices to the clinical research coordinators for data download. **Continuous glucose monitor (Dexcom G6)** The Dexcom G6 is a real-time, integrated continuous glucose monitoring system (iCGM) that directly monitors blood glucose levels without requiring finger sticks. It must be worn continuously in order to collect data. **Wearable accelerometer (Physical activity monitor)** The Garmin Vivosmart 5 Fitness Activity tracker will be used to measure data related to physical activity. **Heart rate** Heart rate can be read from EKG or blood pressure measurement devices. **Blood pressure** Blood pressure devices used for the study across the various data acquisition sites are: OMRON HEM 907XL Blood Pressure Monitor, Medline MDS4001 Automatic Digital Blood Pressure Monitor, and Welch Allyn 6000 series Vital signs monitor with Welch Allyn FlexiPort Reusable Blood Pressure Cuff. **Visual acuity** M&S Technologies EVA device to test visual acuity. The test is administered at a distance of 4 meters from a touch-screen monitor that is 12x20 inches. Participants will read letters from the screen. Photopic Conditions: No neutral density filters are used. A general occluder will be used for photopic testing. The participant wears their own prescription spectacles or trial frames. For Mesopic conditions, a neutral density (ND) filter will be used. The ND filter will either be a lens added to trial frames to reduce incoming light on the tested eye, OR a handheld occluder with a neutral density filter (which we will designate as \u201cND-occluder) over the glasses will be used. The ND-occluder is different from a standard occluder and is used only for vision testing under mesopic conditions. **Contrast sensitivity** The MARS Letter Contrast Sensitivity test (Perceptrix) was conducted monocularly under both Photopic conditions (with a general occluder) and Mesopic conditions (using a Neutral Density occluder with a low luminance filter lens). The standardized order of MARS cards was as follows: Photopic OD, Photopic OS, Mesopic OD, and Mesopic OS. The background luminance of the charts fell within the range of 60 to 120 cd/m2, with an optimal level of 85 cd/m2. Illuminance was recommended to be between 189 to 377 lux, with an optimal level of 267 lux. While the designed viewing distance was 50 cm, it could vary between 40 to 59 cm. Patients were required to wear their appropriate near correction: reading glasses or trial frames with +2.00D lenses. All testing was carried out under undilated conditions. Patients were instructed to read the letter left to right across each line on the chart. Patients were encouraged to guess, even if they perceived the letters as too faint. Testing was terminated either when the patient made two consecutive errors or reached the end of the chart. The log contrast sensitivity (log CS) values were recorded by multiplying the number of errors prior to the final correct letter by 0.04 and subtracting the result from the log CS value at the final correct letter. If a patient reached the end of the chart without making two consecutive errors, the final correct letter was simply the last one correctly identified. **Autorefraction** KR 800 Auto Keratometer/Refractor. **EKG** Philips (manufacturer of Pagewriter TC30 Cardiograph) **Lensometer** Lensometer devices used at data acquisition sites across the study include: NIDEK LM-600P Auto Lensometer, Topcon-CL-200 computerized Lensometer, and Topcon-CL-300 computerized Lensometer **Undilated fundus photography - Optomed Aurora** The Optomed Aurora IQ is a handheld fundus camera that can take non-mydriatic images of the ocular fundus. It has a 50\u00b0 field of view, 5 Mpix sensor, and high-contrast optical design. The camera is non-mydriatic, meaning it doesn't require the pupil to be dilated, so it can be used for detailed viewing of the retina. Images taken during the AI-READI visit, are undilated images taken in a dark room while a patient is sitting on a comfortable chair, laying back. As it becomes challenging to get a good view because of the patients not being dilated and the handheld nature of this imaging modality, the quality of the images vary from patient to patient and within the same patient. **Dilated fundus photography - Eidon** The iCare EIDON is a widefield TrueColor confocal fundus imaging system that can capture images up to 200\u00b0. It comes with multiple imaging modalities, including TrueColor, blue, red, Red-Free, and infrared confocal images. The system offers widefield, ultra-high-resolution imaging and the capability to image through cataract and media opacities. It operates without dilation (minimum pupil 2.5 mm) and provides the flexibility of both fully automated and fully manual modes. Additionally, the iCare EIDON features an all-in-one compact design, eliminating the need for an additional PC. AI READI images using EIDON include two main modalities: 1. Single Field Central IR/FAF 2. Smart Horizontal Mosaic. Imaging is done in fully automated mode in a dark room with the machine moving and positioning according to the patient's head aiming at optimizing the view and minimizing operator's involvement/operator induced noise. **Spectralis HRA (Heidelberg Engineering)** The Heidelberg Spectralis HRA+OCT is an ophthalmic imaging system that combines optical coherence tomography (OCT) with retinal angiography. It is a modular, upgradable platform that allows clinicians to configure it for their specific diagnostic workflow. It has the confocal scanning laser ophthalmoscope (cSLO) technology that not only offers documentation of clinical findings but also often highlights critical diagnostic details that are not visible on traditional clinical ophthalmoscopy. Since cSLO imaging minimizes the effects of light scatter, it can be used effectively even in patients with cataracts. For AI READI subjects, imaging is done in a dark room using the following modalities: ONH-RC, PPole-H, and OCTA of the macula. As the machine is operated by the imaging team and is not fully automated, quality issues may arise, which may lead to skipping this modality and missing data. **Triton DRI OCT (Topcon Healthcare)** The DRI OCT Triton is a device from Topcon Healthcare that combines swept-source OCT technology with multimodal fundus imaging. The DRI OCT Triton uses swept-source technology to visualize the deepest layers of the eye, including through cataracts. It also enhances visualization of outer retinal structures and deep pathologies. The DRI OCT Triton has a 1,050 nm wavelength light source and a non-mydriatic color fundus camera. AI READI imaging is done in a dark room with minimal intervention from the imager as the machine positioning is done automatically. This leads to higher quality images with minimal operator induced error. Imaging is done in 12.0X12.0 mm and 6.0X6.0 mm OCTA, and 12.0 mm X9.0 mmX6.0 mm 3D Horizontal and Radial scan modes. **Maestro2 3D OCT (Topcon Healthcare)** The Maestro2 is a robotic OCT and color fundus camera system from Topcon Healthcare. It can capture a 12 mm x 9 mm wide-field OCT scan that includes the macula and optic disc. The Maestro2 can also capture high-resolution non-mydriatic, true color fundus photography, OCT, and OCTA with a single button press. Imaging is done in a dark room and automatically with minimal involvement of the operator. Protocols include 12.0 mm X9.0 mm widefield, 6.0 mm X 6.0 mm 3D macula scan and 6.0 mm X 6.0 mm OCTA (scan rate: 50 kHz). **FLIO (Heidelberg Engineering)** Fluorescence Lifetime Imaging Ophthalmoscopy (FLIO) is an advanced imaging technique used in ophthalmology. It is a non-invasive method that provides valuable information about the metabolic and functional status of the retina. FLIO is based on the measurement of fluorescence lifetimes, which is the duration a fluorophore remains in its excited state before emitting a photon and returning to the ground state. FLIO utilizes this fluorescence lifetime information to capture and analyze the metabolic processes occurring in the retina. Different retinal structures and molecules exhibit distinct fluorescence lifetimes, allowing for the visualization of metabolic changes, cellular activity, and the identification of specific biomolecules. The imaging is done by an operator in a dark room analogous to a straightforward heidelberg spectralis OCT. However, as it takes longer than a usual spectralis OCT and exposes patients to uncomfortable levels of light, it is kept to be performed as the last modality of an AI READI visit. Because of this patients may not be at their best possible compliance. **Cirrus 5000 Angioplex (Carl Zeiss Meditec)** The Zeiss Cirrus 5000 Angioplex is a high-definition optical coherence tomography (OCT) system that offers non-invasive imaging of retinal microvasculature. The imaging is done in a dark room by an operator and it is pretty straightforward and analogous to what is done in the ophthalmology clinics on a day to day basis. Imaging protocols include 512 X 512 and 200 X 200 macula and ONH scans and also OCTA of the macula. Zeiss Cirrus 5000 also provides a 60-degree OCTA widefield view. 8x8mm single scans and 14x14mm automated OCTA montage allow for rapid peripheral assessment of the retina as well. **Monofilament testing for peripheral neuropathy** Monofilament test is a standard clinical test to monitor peripheral neuropathy in diabetic patients. It is done using a standard 10g monofilament applying pressure to different points on the plantar surface of the feet. If patients sense the monofilament, they confirm by saying \u201cyes\u201d; if patients do not sense the monofilament after it bends, they are considered to be insensate. When the sequence is completed, the insensate area is retested for confirmation. This sequence is further repeated randomly at each of the testing sites on each foot until results are obtained.The results are recorded on an iPad, Laptop, or a paper questionnaire and are directly added to the project's RedCap by the clinical research staff. **Montreal Cognitive Assessment (MoCA)** The Montreal Cognitive Assessment (MoCA) is a simple, in-office screening tool that helps detect mild cognitive impairment and early onset of dementia. The MoCA evaluates cognitive domains such as: Memory, Executive functioning, Attention, Language, Visuospatial, Orientation, Visuoconstructional skills, Conceptual thinking, Calculations. The MoCA generates a total score and six domain-specific index scores. The maximum score is 30, and anything below 24 is a sign of cognitive impairment. A final total score of 26 and above is considered normal. Some disadvantages of the MoCA include: Professionals require training to score the test, A person's level of education may affect the test, Socioeconomic factors may affect the test, People living with depression or other mental health issues may score similarly to those with mild dementia. AI READI research staff perform this test on an iPad using a pre-installed software (MoCA Duo app downloaded from the app store) that captures all the patients responses in an interactive manner."
          }
        ],
        "challenge": [
          {
            "id": 1,
            "question": "Which factors in the data might limit the generalization of potentially derived models? Is this information available as auxiliary labels for challenge tests? For instance: a. Number and diversity of devices included in the dataset. b. Data recording specificities, e.g., the view for a chest x-ray image. c. Number and diversity of recording sites included in the dataset. d. Distribution shifts over time.",
            "response": "While the AI-READI's cross-sectional database ultimately aims to achieve balance across race/ethnicity, biological sex, and diabetes presence and severity, the pilot study is not balanced across these parameters. Three recording sites were strategically selected to achieve diverse recruitment: the University of Alabama at Birmingham (UAB), the University of California San Diego (UCSD), and the University of Washington (UW). The sites were chosen for geographic diversity across the United States and to ensure diverse representation across various racial and ethnic groups. Individuals from all demographic backgrounds were recruited at all 3 sites. Factors influencing the generalization of derived models include the predominantly urban and hospital-based recruitment, which may not fully capture diverse cultural and socioeconomic backgrounds. The study cohort may not provide a comprehensive representation of the population, as it does not include other races/ethnicities such as Pacific Islanders and Native Americans. Information on device make and model, including specific modalities like macula scans or wide scans during OCT, were documented to ensure repeatability. Moreover, the study included multiple devices for one measure to enhance generalizability and represent the diverse range of equipment utilized in clinical settings."
          },
          {
            "id": 2,
            "question": "What confounding factors might be present in the data? a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another? b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another?",
            "response": "Uniform data collection protocols were implemented for all subjects, irrespective of their race/ethnicity, biological sex, or diabetes severity, across all study sites. The selection of study sites was intended to ensure equitable representation and minimize the potential for sampling bias."
          }
        ],
        "demographicInformation": [
          {
            "id": 1,
            "question": "Does the dataset identify any demographic sub-populations (e.g., by age, gender, sex, ethnicity)?",
            "response": "No"
          },
          {
            "id": 2,
            "question": "If no, a. Is there any regulation that prevents demographic data collection in your study (for example, the country that the data is collected in)?",
            "response": "No."
          },
          {
            "id": 3,
            "question": "b. Are you employing methods to reduce the disparity of error rate between different demographic subgroups when demographic labels are unavailable? Please describe.",
            "response": "We are suggesting a split for training/validation/testing models that is aimed at reducing disparities in models developed using this dataset."
          }
        ],
        "preprocessing": [
          {
            "id": 1,
            "question": "Was there any pre-processing for the de-identification of the patients? Provide the answer for the preliminary and the current version of the dataset",
            "response": ""
          },
          {
            "id": 2,
            "question": "Was there any pre-processing for cleaning the data? Provide the answer for the preliminary and the current version of the dataset",
            "response": "There were several quality control measures used at the time of data entry/acquisition. For example, clinical data outside of expected min/max ranges were flagged in REDCap, which was visible in reports viewed by clinical research coordinators (CRCs) and Data Managers. Using these REDCap reports as guides, Data Managers and CRCs examined participant records and determined if an error was likely. Data were checked for the following and edited if errors were detected: 1. Credibility, based on range checks to determine if all responses fall within a prespecified reasonable range 2. Incorrect flow through prescribed skip patterns 3. Missing data that can be directly filed from other portions of an individual's record 4. The omission and/or duplication of records Editing was only done under the guidance and approval of the site PI. If corrected data was available from elsewhere in the respondent's answers, the error was corrected. If there was no logical or appropriate way to correct the data, the Data site PI reviewed the values and made decisions about whether those values should be removed from the data. Once data were sent from each of the study sites to the central project team, additional processing steps were conducted in preparation for dissemination. For example, all data were mapped to standardized terminologies when possible, such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, a common data model for observational health data, and the Digital Imaging and Communications in Medicine (DICOM), a commonly used standard for medical imaging data. Details about the data processing approaches for each data domain/modality are described in the dataset documentation at https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Was the \u201craw\u201d data (post de-identification) saved in addition to the preprocessed/cleaned data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data",
            "response": "The raw data is saved and expected to be preserved by the AI-READI project at least for the duration of the project but is not anticipated to be shared outside the project team right now, because it has not been mapped to standardized terminologies and because the raw data may accidentally include personal health information or personally identifiable information (e.g. in free text fields). There is a possibility that raw data may be included in future releases of the controlled access dataset."
          },
          {
            "id": 4,
            "question": "Were instances excluded from the dataset at the time of preprocessing? If so, why? For example, instances related to patients under 18 might be discarded.",
            "response": "No data were excluded from the dataset at the time of preprocessing. However, regarding to study recruitment (i.e. ability to participate in the study), the following eligibility criteria were used: Inclusion Criteria: - Able to provide consent - \u2265 40 years old - Persons with or without type 2 diabetes - Must speak and read English Exclusion Criteria: - Must not be pregnant - Must not have gestational diabetes - Must not have Type 1 diabetes"
          },
          {
            "id": 5,
            "question": "If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Answer this question for both the preliminary dataset and the current version of the dataset",
            "response": "N/A"
          }
        ],
        "labeling": [
          {
            "id": 1,
            "question": "Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version. a. If yes: 1. What are the labels provided? 2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?",
            "response": "N/A - no labels are provided"
          },
          {
            "id": 2,
            "question": "b. What labeling strategy was used? 1. Gold standard label available in the data (e.g. cancers validated by biopsies) 2. Proxy label computed from available data: 1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions) 2. Which tables and features were considered to compute the label? 3. Which proportion of the data has gold standard labels?",
            "response": "N/A - no labels are provided"
          },
          {
            "id": 3,
            "question": "c. Human-labeled data 1. How many labellers were considered? 2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, \u2026) 3. What guidelines did they follow? 4. How many labellers provide a label per instance? If multiple labellers per instance: 1. What is the rater agreement? How was disagreement handled? 2. Are all labels provided, or summaries (e.g. maximum vote)? 5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes) 6. On average, how much time was required to annotate each instance? 7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?",
            "response": "N/A - no labels are provided. No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications."
          },
          {
            "id": 4,
            "question": "What are the human level performances in the applications that the dataset is supposed to address?",
            "response": "N/A"
          },
          {
            "id": 5,
            "question": "Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.",
            "response": "N/A - no labeling was performed"
          },
          {
            "id": 6,
            "question": "Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?",
            "response": "No, we do not have formal guidelines in place."
          },
          {
            "id": 7,
            "question": "Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.",
            "response": "The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced as well as possible for sex, race/ethnicity and diabetes status (no diabetes, prediabetes/lifestyle controlled, oral medication controlled, and insulin controlled)."
          }
        ],
        "collection": [
          {
            "id": 1,
            "question": "Were any REB/IRB approval (e.g., by an institutional review board or research ethics board) received? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "The initial IRB approval at the University of Washington was received on December 20, 2022. The initial approval letter can be found [here](https://docs.aireadi.org/v3/Approval_STUDY00016228_Lee_initial.pdf). An annual renewal application to the IRB about the status and progress of the study is required and due within 90 days of expiration."
          },
          {
            "id": 2,
            "question": "How was the data associated with each instance acquired? Was the data directly observable (e.g., medical images, labs or vitals), reported by subjects (e.g., survey responses, pain levels, itching/burning sensations), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.",
            "response": "The acquisition of data varied based on the domain; some data were directly observable (such as labs, vitals, and retinal imaging), whereas other data were reported by subjects (e.g. survey responses). Verification of data entry was performed when possible (e.g. cross-referencing entered medications with medications that were physically brought in or photographed by each study participant). Details for each data domain are available in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Provide the answer for all modalities and collected data. Has this information been changed through the process? If so, explain why.",
            "response": "The procedures for data collection and processing is available at https://docs.aireadi.org."
          },
          {
            "id": 4,
            "question": "Who was involved in the data collection process (e.g., patients, clinicians, doctors, ML researchers, hospital staff, vendors, etc.) and how were they compensated (e.g., how much were contributors paid)?",
            "response": "Details about the AI-READI team members involved in the data collection process are available at https://aireadi.org/team. Their effort was supported by the National Institutes of Health award OT2OD032644 based on the percentage of effort contributed, and salaries which aligned with the funding guidelines at each site. Study subjects received a compensation of $200 for the study visit also through the grant funding."
          },
          {
            "id": 5,
            "question": "Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.",
            "response": "The timeline for the overall project spans four years, encompassing one year dedicated to protocol development and training, and years 2-4 allocated for subject recruitment and data collection. Approximately 4% of participants are expected to undergo a follow-up examination in Year 4. The data collection process is specifically tailored to enable downstream pseudotime manifold analysis\u2014an approach used to predict disease trajectories. This involves gathering and learning from complex, multimodal data from participants exhibiting varying disease severity, ranging from normal to insulin-dependent Type 2 Diabetes Mellitus (T2DM). The timeframe also allows for the collection of the highest number of subjects possible to ensure a balanced representation of racial and ethnic groups and mitigate biases in computer vision algorithms. For this version of the dataset, the timeframe for data collection was July 19, 2023 to May 01, 2025."
          },
          {
            "id": 6,
            "question": "Does the dataset relate to people? If not, you may skip the remaining questions in this section.",
            "response": "Yes"
          },
          {
            "id": 7,
            "question": "Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., hospitals, app company)?",
            "response": "The data was collected directly from participants across the three recruiting sites. Recruitment pools were identified by screening Electronic Health Records (EHR) for diabetes and prediabetes ICD-10 codes for all patients who have had an encounter with the sites' health systems within the past 2 years."
          },
          {
            "id": 8,
            "question": "Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.",
            "response": "Yes, each individual was aware of the data collection, as this was not passive data collection or secondary use of existing data, but rather active data collection directly from participants."
          },
          {
            "id": 9,
            "question": "Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.",
            "response": "Informed consent to participate was required before participation in any part of the protocol (including questionnaires). Potential participants were given the option to read all consent documentation electronically (e-consent) before their visit and give their consent with an electronic signature without verbal communication with a clinical research coordinator. Participants may access e-consent documentation in REDCap and decide at that point they do not want to participate or would like additional information. The approved consent form for the principal project site University of Washington is available [here](https://docs.aireadi.org/v3/AI-READI%20Consent_Form_Standard_Mod17May2023_useforpilot.pdf). The other clinical sites had IRB reliance and used the same consent form, with minor institution-specific language incorporated depending on individual institutional requirements."
          },
          {
            "id": 10,
            "question": "If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).",
            "response": "Participants were permitted to withdraw consent at any time and cease study participation. However, any data that had been shared or used up to that point would stay in the dataset. This is clearly communicated in the consent document."
          },
          {
            "id": 11,
            "question": "In which countries was the data collected?",
            "response": "USA"
          },
          {
            "id": 12,
            "question": "Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "No, a data protection impact analysis has not been conducted."
          }
        ],
        "inclusion": [
          {
            "id": 1,
            "question": "Is there any language-based communication with patients (e.g: English, French)? If yes, describe the choices of language(s) for communication. (for example, if there is an app used for communication, what are the language options?)",
            "response": "English language was used for communication with study participants."
          },
          {
            "id": 2,
            "question": "What are the accessibility measurements and what aspects were considered when the study was designed and implemented?",
            "response": "Accessibility measurements were not specifically assessed. However, transportation assistance (rideshare services) was offered to study participants who endorsed barriers to transporting themselves to study visits."
          },
          {
            "id": 3,
            "question": "If data is part of a clinical study, what are the inclusion criteria?",
            "response": "The eligibility criteria for the study were as follows: Inclusion Criteria: - Able to provide consent - \u2265 40 years old - Persons with or without type 2 diabetes - Must speak and read English Exclusion Criteria: - Must not be pregnant - Must not have gestational diabetes - Must not have Type 1 diabetes"
          }
        ],
        "uses": [
          {
            "id": 1,
            "question": "Has the dataset been used for any tasks already? If so, please provide a description. ",
            "response": "No"
          },
          {
            "id": 2,
            "question": "Does using the dataset require the citation of the paper or any other forms of acknowledgement? If yes, is it easily accessible through google scholar or other repositories",
            "response": "Yes, use of the dataset requires citation to the resources specified in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. (besides Google scholar)",
            "response": "No"
          },
          {
            "id": 4,
            "question": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?",
            "response": "No, to the extent of our knowledge, we do not currently anticipate any uses of the dataset that could result in unfair treatment or harm. However, there is a theoretical risk of future re-identification."
          },
          {
            "id": 5,
            "question": "Are there tasks for which the dataset should not be used? If so, please provide a description. (for example, dataset creators could recommend against using the dataset for considering immigration cases, as part of insurance policies)",
            "response": "This is answered in a prior question (see details regarding license terms)."
          }
        ],
        "distribution": [
          {
            "id": 1,
            "question": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.",
            "response": "The dataset will be distributed and be available for public use."
          },
          {
            "id": 2,
            "question": "How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?",
            "response": "The dataset will be available through the FAIRhub platform (http://fairhub.io/). The dataset' DOI is https://doi.org/10.60775/fairhub.3"
          },
          {
            "id": 3,
            "question": "When was/will the dataset be distributed?",
            "response": "The first version of the dataset was distributed in May 2024, the second version of the dataset was distributed in November 2024, and the third version of the dataset was distributed in November 2025."
          },
          {
            "id": 4,
            "question": "Assuming the dataset is available, will it be/is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.",
            "response": "We provide here the license file containing the terms for reusing the AI-READI dataset (https://doi.org/10.5281/zenodo.17555036). These license terms were specifically tailored to enable reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purpose while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications."
          },
          {
            "id": 5,
            "question": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.17555036)"
          },
          {
            "id": 6,
            "question": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.17555036)"
          }
        ],
        "maintenance": [
          {
            "id": 1,
            "question": "Who is supporting/hosting/maintaining the dataset?",
            "response": "The AI-READI team will be supporting and maintaining the dataset. The dataset is hosted on FAIRhub through Microsoft Azure."
          },
          {
            "id": 2,
            "question": "How can the owner/curator/manager of the dataset be contacted (e.g. email address)?",
            "response": "We refer to the README file included with the dataset for contact information."
          },
          {
            "id": 3,
            "question": "Is there an erratum? If so, please provide a link or other access point.",
            "response": ""
          },
          {
            "id": 4,
            "question": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?",
            "response": "The dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants complete the study visit."
          },
          {
            "id": 5,
            "question": "If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.",
            "response": "There are no limits on the retention of the data associated with the instances."
          },
          {
            "id": 6,
            "question": "Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how and for how long. If not, please describe how its obsolescence will be communicated to users.",
            "response": "N/A - as mentioned in the response to question 4, the dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants are enrolled."
          },
          {
            "id": 7,
            "question": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?",
            "response": "No, currently there is no mechanism for others to extend or augment the AI-READI dataset outside of those who are involved in the project."
          }
        ]
      }
    },
    "files": [
      {
        "children": [
          {
            "label": "ecg_12lead",
            "children": [
              {
                "label": "philips_tc30",
                "children": []
              }
            ]
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "cardiac_ecg"
      },
      {
        "children": [],
        "label": "clinical_data"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "leelab_anura"
              }
            ],
            "label": "environmental_sensor_variables"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "environment"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_flio"
              }
            ],
            "label": "flio"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_flio"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "structural_oct"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_oct"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              }
            ],
            "label": "enface"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              }
            ],
            "label": "flow_cube"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              }
            ],
            "label": "segmentation"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_octa"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "optomed_aurora"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "/topcon_triton"
              }
            ],
            "label": "cfp"
          },
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              }
            ],
            "label": "faf"
          },
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "ir"
          }
        ],
        "label": "retinal_photography"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "heart_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "oxygen_saturation"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "physical_activity"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "respiratory_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "sleep"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "stress"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "wearable_activity_monitor"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "dexcom_g6"
              }
            ],
            "label": "continuous_glucose_monitoring"
          }
        ],
        "label": "wearable_blood_glucose"
      },
      {
        "label": "CHANGELOG.md"
      },
      {
        "label": "LICENSE.txt"
      },
      {
        "label": "README.md"
      },
      {
        "label": "dataset_description.json"
      },
      {
        "label": "dataset_structure_description.json"
      },
      {
        "label": "healthsheet.md"
      },
      {
        "label": "participants.json"
      },
      {
        "label": "participants.tsv"
      },
      {
        "label": "study_description.json"
      }
    ],
    "data": {
      "size": 310000000000,
      "fileCount": 21669,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://fairhub.io/datasets/1",
    "created": "1714546800"
  },
  {
    "id": 14,
    "canonicalId": "agudjafpa77j71jntu5gldy8",
    "datasetId": "b09kil90t8tq4rz6i408cwh3",
    "doi": "10.60775/fairhub.2",
    "title": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project",
    "description": "The AI-READI project seeks to create and share a flagship ethically-sourced dataset of type 2 diabetes.",
    "versionTitle": "2.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/study_description.json",
        "identificationModule": {
          "officialTitle": "AI Ready and Exploratory Atlas for Diabetes Insights",
          "acronym": "AI-READI",
          "orgStudyIdInfo": {
            "orgStudyId": "OT2OD032644",
            "orgStudyIdType": "U.S. National Institutes of Health (NIH) Grant/Contract Award Number",
            "orgStudyIdLink": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
          },
          "secondaryIdInfoList": [
            {
              "secondaryId": "NCT06002048",
              "secondaryIdType": "ClinicalTrials.gov",
              "secondaryIdLink": "https://classic.clinicaltrials.gov/ct2/show/NCT06002048"
            }
          ]
        },
        "statusModule": {
          "overallStatus": "Enrolling by invitation",
          "startDateStruct": {
            "startDate": "2023-07-19",
            "startDateType": "Actual"
          },
          "completionDateStruct": {
            "completionDate": "2027-01-01",
            "completionDateType": "Anticipated"
          }
        },
        "sponsorCollaboratorsModule": {
          "leadSponsor": {
            "leadSponsorName": "University of Washington",
            "leadSponsorIdentifier": {
              "leadSponsorIdentifierValue": "https://ror.org/00cvxb145",
              "leadSponsorIdentifierScheme": "ROR",
              "schemeURI": "https://ror.org/"
            }
          },
          "responsibleParty": {
            "responsiblePartyType": "Principal Investigator",
            "responsiblePartyInvestigatorFirstName": "Aaron",
            "responsiblePartyInvestigatorLastName": "Lee",
            "responsiblePartyInvestigatorTitle": "Associate Professor",
            "responsiblePartyInvestigatorIdentifier": [
              {
                "responsiblePartyInvestigatorIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                "responsiblePartyInvestigatorIdentifierScheme": "ORCID",
                "schemeURI": "https://orcid.org/"
              }
            ],
            "responsiblePartyInvestigatorAffiliation": {
              "responsiblePartyInvestigatorAffiliationName": "University of Washington",
              "responsiblePartyInvestigatorAffiliationIdentifier": {
                "responsiblePartyInvestigatorAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                "responsiblePartyInvestigatorAffiliationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          },
          "collaboratorList": [
            {
              "collaboratorName": "National Institutes of Health",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/01cwqze88",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "California Medical Innovations Institute",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0156zyn36",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Johns Hopkins University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00za53h95",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Oregon Health & Science University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/009avj582",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Stanford University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00f54p054",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of Alabama at Birmingham",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/008s83205",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of California, San Diego",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0168r3w48",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        },
        "oversightModule": {
          "isFDARegulatedDrug": "No",
          "isFDARegulatedDevice": "No",
          "humanSubjectReviewStatus": "Submitted, approved",
          "oversightHasDMC": "No"
        },
        "descriptionModule": {
          "briefSummary": "The study will collect a cross-sectional dataset of 4000 people across the US who are either 1) healthy, or 2) belong in one of the three stages of diabetes severity (pre-diabetes/lifestyle controlled, oral medication and/or non-insulin-injectable medication controlled, or insulin dependent), forming a total of four groups of patients. Clinical data (social determinants of health surveys, continuous glucose monitoring data, biomarkers, genetic data, retinal imaging, cognitive testing, etc.) will be collected. The purpose of this project is data generation to allow future creation of artificial intelligence/machine learning (AI/ML) algorithms aimed at defining disease trajectories and underlying genetic links in different racial/ethnic cohorts. A smaller subgroup of participants will be invited to come for a follow-up visit in year 4 of the project (longitudinal arm of the study). Data will be placed in an open-source repository and samples will be sent to the study sample repository and used for future research.",
          "detailedDescription": "The Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available."
        },
        "conditionsModule": {
          "conditionList": [
            {
              "conditionName": "Type 2 Diabetes",
              "conditionIdentifier": {
                "conditionClassificationCode": "D003924",
                "conditionScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "conditionURI": "https://meshb.nlm.nih.gov/record/ui?ui=D003924"
              }
            }
          ],
          "keywordList": [
            {
              "keywordValue": "Retinal Imaging"
            },
            {
              "keywordValue": "Data Sharing",
              "keywordIdentifier": {
                "keywordClassificationCode": "D033181",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D033181"
              }
            },
            {
              "keywordValue": "Equitable Data Collection"
            },
            {
              "keywordValue": "Machine Learning",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000069550",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
              }
            },
            {
              "keywordValue": "Artificial Intelligence",
              "keywordIdentifier": {
                "keywordClassificationCode": "D001185",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
              }
            },
            {
              "keywordValue": "Electrocardiography",
              "keywordIdentifier": {
                "keywordClassificationCode": "D004562",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
              }
            },
            {
              "keywordValue": "Continuous Glucose Monitoring",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000095583",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
              }
            },
            {
              "keywordValue": "Retinal imaging"
            },
            {
              "keywordValue": "Eye exam"
            }
          ]
        },
        "designModule": {
          "studyType": "Observational",
          "isPatientRegistry": "No",
          "designInfo": {
            "designObservationalModelList": [
              "Cohort"
            ],
            "designTimePerspectiveList": [
              "Cross-sectional"
            ]
          },
          "bioSpec": {
            "bioSpecRetention": "Samples With DNA",
            "bioSpecDescription": "Participants who consent are asked to provide both blood and urine for research use.  There are three distinct elements that follow from these collections; first, a small amount of the collected blood is sent to the local collection site hospital lab for a complete blood cell count on the day of the encounter. Second, an additional portion of the blood and the urine sample are processed and stored at the collection site for batch shipment to the University of Washington Nutrition and Obesity Research Center for specialized lab tests.  Third, the remaining majority of the collected blood is processed into a variety of derivatives that are being used to establish a large repository of biospecimens at the University of Alabama at Birmingham to be used in research to further our understanding of diabetes, diabetic associated eye disease and other applications related to health and related diseases. These derivatives include isolated plasma, serum, DNA, buffy coats (white blood cells), blood stabilized for future isolation of RNA and peripheral blood mononuclear cells (PBMC). These derivatives are separated into small aliquots to facilitate future approved research tests and are stored at either -80oC or at cryogenic temperatures (PBMC)."
          },
          "enrollmentInfo": {
            "enrollmentCount": "4000",
            "enrollmentType": "Anticipated"
          }
        },
        "armsInterventionsModule": {
          "armGroupList": [
            {
              "armGroupLabel": "Healthy",
              "armGroupDescription": "Participants who do not have Type 1 or Type 2 Diabetes",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Pre-diabetes/Lifestyle Controlled",
              "armGroupDescription": "Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifesyle adjustments",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Oral Medication and/or Non-insulin-injectable Medication Controlled",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Insulin Dependent",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            }
          ],
          "interventionList": [
            {
              "interventionType": "Other",
              "interventionName": "AI-READI protocol",
              "interventionDescription": "Custom protocol followed for the AI-READI study. Details are available at in the documentation of v2.0.0 of the dataset at https://docs.aireadi.org/"
            }
          ]
        },
        "eligibilityModule": {
          "sex": "All",
          "genderBased": "No",
          "minimumAge": "40 Years",
          "maximumAge": "85 Years",
          "healthyVolunteers": "No",
          "eligibilityCriteria": {
            "eligibilityCriteriaInclusion": [
              "Adults (\u2265 40 years old)",
              "Patients with and without type 2 diabetes",
              "Able to provide consent",
              "Must be able to read and speak English"
            ],
            "eligibilityCriteriaExclusion": [
              "Adults older than 85 years of age",
              "Pregnancy",
              "Gestational diabetes",
              "Type 1 diabetes"
            ]
          },
          "studyPopulation": "Adult patients will be recruited into one of four groups: 1) healthy/no diabetes, 2) pre-diabetes/borderline diabetes/lifestyle-controlled diabetes, 3) oral medication and/or non-insulin injectable medication controlled type 2 diabetes, or 4) insulin dependent type 2 diabetes. The investigators aim to recruit approximately 1000 patients into each of the four groups. Patients will be recruited from University of Washington (UW), University of California at San Diego (UCSD), and University of Alabama at Birmingham (UAB).",
          "samplingMethod": "Non-Probability Sample"
        },
        "contactsLocationsModule": {
          "centralContactList": [
            {
              "centralContactFirstName": "Aaron",
              "centralContactLastName": "Lee",
              "centralContactDegree": "MD",
              "centralContactIdentifier": [
                {
                  "centralContactIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "centralContactIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "centralContactAffiliation": {
                "centralContactAffiliationName": "University of Washington",
                "centralContactAffiliationIdentifier": {
                  "centralContactAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "centralContactAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "centralContactEMail": "contact@aireadi.org"
            }
          ],
          "overallOfficialList": [
            {
              "overallOfficialFirstName": "Aaron",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Washington",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cecilia",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-1994-7213",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Washington",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00cvxb145",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Amir",
              "overallOfficialLastName": "Bahmani",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-4533-9334",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sally L.",
              "overallOfficialLastName": "Baxter",
              "overallOfficialDegree": "MD, MSc",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-5271-7690",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Christopher G.",
              "overallOfficialLastName": "Chute",
              "overallOfficialDegree": "MD, DrPH",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-5437-2545",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Samantha",
              "overallOfficialLastName": "Hurst",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9843-2845",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "T. Y. Alvin",
              "overallOfficialLastName": "Liu",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-2957-0755",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Gerald",
              "overallOfficialLastName": "McGwin",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9592-1133",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Shannon",
              "overallOfficialLastName": "McWeeney",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-8333-6607",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Oregon Health & Science University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/009avj582",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cynthia",
              "overallOfficialLastName": "Owsley",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-3424-011X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Bhavesh",
              "overallOfficialLastName": "Patel",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-0307-262X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "California Medical Innovations Institute",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0156zyn36",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Michael",
              "overallOfficialLastName": "Snyder",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-0784-7987",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sara J.",
              "overallOfficialLastName": "Singer",
              "overallOfficialDegree": "MBA, PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "http://orcid.org/0000-0002-3374-1177",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Linda M.",
              "overallOfficialLastName": "Zangwill",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-1143-5224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            }
          ],
          "locationList": [
            {
              "locationFacility": "University of Alabama at Birmingham",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Birmingham",
              "locationZip": "35233",
              "locationState": "Alabama",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/008s83205",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of California, San Diego",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "San Diego",
              "locationZip": "92093",
              "locationState": "California",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/0168r3w48",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of Washington",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Seattle",
              "locationZip": "98109",
              "locationState": "Washington",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/00cvxb145",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        }
      },
      "readme": "## Overview of the study\n\nThe Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available. \n\n## Description of the dataset\n\nThis dataset contains data from 1067 participants that was collected between July 19, 2023 and July 31, 2024. Data from multiple modalities are included. A full list is provided in the `Data Standards` section below. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed.\n\nThe dataset contains 165,051 files and is around 2.01 TB in size.\n\nA detailed description of the dataset is available in the AI-READI documentation for v2.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Protocol\n\nThe protocol followed for collecting the data can be found in the AI-READI documentation for v2.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Dataset access/restrictions\n\nAccessing the dataset requires several steps, including:\n\n- Login in through a verified ID system\n- Agreeing to use the data only for type 2 diabetes related research.\n- Agreeing to the license terms which set certain restrictions and obligations for data usage (see `License` section below).\n\n## Data standards followed\n\nThis dataset is organized following the [Clinical Dataset Structure (CDS) v0.1.1](https://cds-specification.readthedocs.io/en/v0.1.1/). We refer to the CDS documentation for more details. Briefly, data is organized at the root level into one directory per datatype (c.f. Table below). Within each datatype folder, there is one folder per modality. Within each modality folder, there is one folder per device used to collect that modality. Within each device folder, there is one folder per participant. Each datatype, modality, and device folder is named using a name that best defines it. Each participant folder is named after the participant's ID number used in the study. For each datatype, the data files follow the standards listed in the Table below. More details are available in the dataset_structure_description.json metadata file included in this dataset.\n\n| Datatype directory name   | Description                                                                                                                                                                                      | File format standard followed                                                                                                                                     |\n| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cardiac_ecg               | This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.      | [WaveForm DataBase (WFDB)](https://wfdb.readthedocs.io/en/latest/wfdb.html)                                                                                       |\n| clinical_data             | This directory contains clinical data collected through REDCap. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.                                                  | [Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi)                                               |\n| environment               | This directory contains data collected through an environmental sensor device custom built for the AI-READI project.                                                                             | [Earth Science Data Systems (ESDS) format](https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data) |\n| retinal_flio              | This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores. | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_oct               | This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.                                   | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_octa              | This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.                     | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_photography       | This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.                                                                          | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| wearable_activity_monitor | This directory contains data collected through a wearable fitness tracker.                                                                                                                       | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n| wearable_blood_glucose    | This directory contains data collected through a continuous glucose monitoring (CGM) device.                                                                                                     | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n\n## Resources\n\nAll of our data files are in formats that are accessible with free software commonly used for such data types so no specific software is required. Some useful resources related to this dataset are listed below:\n\n- Documentation of the dataset: [docs.aireadi.org](https://docs.aireadi.org/) (see `Dataset v2.0.0` for this version of the dataset)\n- AI-READI project website: [aireadi.org](https://aireadi.org/)\n- Zenodo community of the AI-READI project: [zenodo.org/communities/aireadi](https://zenodo.org/communities/aireadi)\n- GitHub organization of the AI-READI project: [github.com/AI-READI](https://github.com/AI-READI)\n\n### Suggested split\n\nThe suggested split for training, validating, and testing AI/ML models is included in the participants.tsv file that will be included in the dataset. A summary is provided in the table below.\n\n|                         | Train        |           |       |         | Val          |           |       |         | Test         |           |       |         | Total       |           |       |         |\n| ----------------------- | ------------ | --------- | ----- | ------- | ------------ | --------- | ----- | ------- | ------------ | --------- | ----- | ------- | ------------ | --------- | ----- | ------- |\n|    |    Hispanic     | Asian     | Black | White   | Hispanic     | Asian     | Black | White   | Hispanic     | Asian     | Black | White   | Hispanic     | Asian     | Black | White   |\n| Race/ethnicity (count)  | 144          | 167       | 211   | 225     | 40           | 40        | 40    | 40      | 40           | 40        | 40    | 40      | 224           | 247        | 291    | 305      |\n|                         | Male         | Female    |       |         | Male         | Female    |       |         | Male         | Female    |       |         | Male         | Female    |       |         |\n| Sex (count)             | 302          | 445       |       |         | 80           | 80        |       |         | 80           | 80        |       |         | 462           | 605        |       |         |\n|                        | No DM        | Lifestyle | Oral  | Insulin | No DM      | Lifestyle | Oral  | Insulin | No DM        | Lifestyle | Oral  | Insulin | No DM        | Lifestyle | Oral  | Insulin | |\n| Diabetes status (count) | 292          | 162       | 235   | 58      | 40           | 40        | 47    | 33      | 40           | 40        | 41    | 39      | 364           | 242        | 331    | 130      | |\n| Mean age (years \u00b1 sd)   | 60.3  \u00b1 11.13 |           |       |         | 60.2 \u00b1  10.5 |           |       |         | 60.4 \u00b1  11.0 |           |       |         | 60.3 \u00b1  11.1 |           |       |         | |\n| Total                   | 747          |           |       |         | 160          |           |       |         | 160          |           |       |         | 1067          |           |       |       |\n\n- No DM : Participants who do not have Type 1 or Type 2 Diabetes\n\n- Lifestyle: Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifestyle adjustments\n\n- Oral: Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin\n\n- Insulin: Participants with Type 2 Diabetes whose blood sugar is controlled by insulin\n\n### Changes between versions of the dataset\n\nChanges between the current version of the dataset and the previous one are provided in details in the CHANGELOG file included in the dataset (also visible at [docs.aireadi.org](https://docs.aireadi.org/)). A summary of the major changes is provided in the table below.\n\n| Dataset   | v1.0.0 pilot              | year 2 data      |v2.0.0 main study |\n| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |-------------------------|\n| Participants               | 204      | 863  |1067|                                                                                   |\n| Data types                | 15+ data types                                                  |+1 image device       |15+ data types  |                                       |\n| Processing               |  custom / ad hoc                                                                             | automated + custom| automated + custom  |\n| Release date               | 5/3/2024    | included in v2.0.0 | 11/8/2024       |\n\n## License\n\nThis work is licensed under a custom license specifically tailored to enable the reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purposes while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications. More details are available in the License file included in the dataset and also available at https://doi.org/10.5281/zenodo.10642459.\n\n## How to cite\n\nIf you use this dataset for any purpose, please cite the resources specified in the AI-READI documentation for version 2.0.0 of the dataset at https://docs.aireadi.org.\n\n## Contact\n\nFor any questions, suggestions, or feedback related to this dataset, please go to https://aireadi.org/contact. We refer to the study_description.json and dataset_description.json metadata files included in this dataset for additional information about the contact person/entity, authors, and contributors of the dataset.\n\n## Acknowledgement\n\nThe AI-READI project is supported by NIH grant [1OT2OD032644](https://reporter.nih.gov/search/1ADgncihCk6fdMRJdCnBjg/project-details/10471118) through the NIH Bridge2AI Common Fund program.",
      "datasetDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/dataset_description.json",
        "identifier": {
          "identifierValue": "10.60775/fairhub.2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project"
          },
          {
            "titleValue": "AI-READI dataset",
            "titleType": "AlternativeTitle"
          }
        ],
        "version": "2.0.0",
        "creator": [
          {
            "creatorName": "AI-READI Consortium",
            "nameType": "Organizational"
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-11-08",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on FAIRhub"
          },
          {
            "dateValue": "2023-07-19/2024-07-31",
            "dateType": "Collected",
            "dateInformation": "Period when the data was collected"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Type 2 Diabetes",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No identifiers were collected so no active de-identification was necessary but we checked that no identifiable data per US HIPAA were present in the data."
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": "The public version of the dataset can only be used for type 2 diabetes related research. A private version will allow for more generic use."
        },
        "description": [
          {
            "descriptionValue": "This dataset contains data from 1067 participants that was collected between from July 19, 2023 and July, 31 2024. Data from multiple modalities are included. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed. A detailed description of the dataset is available in the AI-READI documentation for v2.0.0 of the dataset at https://docs.aireadi.org",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://docs.aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          },
          {
            "relatedIdentifierValue": "https://aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          }
        ],
        "subject": [
          {
            "subjectValue": "Diabetes mellitus",
            "subjectIdentifier": {
              "classificationCode": "45636-8",
              "subjectScheme": "Logical Observation Identifier Names and Codes (LOINC)",
              "schemeURI": "https://loinc.org/",
              "valueURI": "https://loinc.org/45636-8"
            }
          },
          {
            "subjectValue": "Machine Learning",
            "subjectIdentifier": {
              "classificationCode": "D000069550",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
            }
          },
          {
            "subjectValue": "Artificial Intelligence",
            "subjectIdentifier": {
              "classificationCode": "D001185",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
            }
          },
          {
            "subjectValue": "Electrocardiography",
            "subjectIdentifier": {
              "classificationCode": "D004562",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
            }
          },
          {
            "subjectValue": "Continuous Glucose Monitoring",
            "subjectIdentifier": {
              "classificationCode": "D000095583",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
            }
          },
          {
            "subjectValue": "Retinal imaging"
          },
          {
            "subjectValue": "Eye exam"
          }
        ],
        "managingOrganization": {
          "name": "University of Washington",
          "managingOrganizationIdentifier": {
            "managingOrganizationIdentifierValue": "https://ror.org/00cvxb145",
            "managingOrganizationScheme": "ROR",
            "schemeURI": "https://ror.org"
          }
        },
        "accessType": "PublicDownloadSelfAttestationRequired",
        "accessDetails": {
          "description": "Accessing the dataset requires several steps, including: Login in through a verified ID system, Agreeing to use the data only for type 2 diabetes related research, Agreeing to the license terms which set certain restrictions and obligations for data usage (see 'rights' property)"
        },
        "rights": [
          {
            "rightsName": "AI-READI custom license v1.0",
            "rightsURI": "https://doi.org/10.5281/zenodo.10642459"
          }
        ],
        "publisher": {
          "publisherName": "FAIRhub"
        },
        "size": [
          "2.01 TB",
          "165,051 files"
        ],
        "fundingReference": [
          {
            "funderName": "National Institutes of Health",
            "funderIdentifier": {
              "funderIdentifierValue": "https://ror.org/01cwqze88",
              "funderIdentifierType": "ROR",
              "schemeURI": "https://ror.org"
            },
            "awardNumber": {
              "awardNumberValue": "OT2OD032644",
              "awardURI": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
            },
            "awardTitle": "Bridge2AI: Salutogenesis Data Generation Project"
          }
        ],
        "format": [
          "image/DICOM",
          "text/markdown",
          "table/csv"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [
          {
            "directoryName": "cardiac_ecg",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Electrocardiogram",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C168186",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C168186"
                  }
                ]
              },
              {
                "relatedTermValue": "Electrocardiography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D004562",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "WaveForm DataBase (WFDB)",
                "standardDescription": "Set of file standards designed for reading and storing physiologic signal data, and associated annotations.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://wfdb.readthedocs.io/en/latest/wfdb.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "ecg_12lead",
                "directoryType": "modality",
                "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Electrocardiography",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other",
                    "relatedIdentifierDescription": "This page describes 3 types of ECG protocol including the 12 lead (standard) protocol."
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "philips_tc30",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol using the Philips PageWriter TC30 device.",
                    "relatedIdentifier": [
                      {
                        "relatedIdentifierValue": "https://www.documents.philips.com/doclib/enc/fetch/2000/4504/577242/577243/577246/581601/711562/DXL_ECG_Algorithm_Physician_s_Guide_(ENG)_Ed.2.pdf",
                        "relatedIdentifierType": "URL",
                        "relationType": "IsDocumentedBy",
                        "resourceTypeGeneral": "Other",
                        "relatedIdentifierDescription": "The 'Philips DXL ECG Algorithm Physician\u2019s Guide' contains information on the fields that are printed on an ECG report."
                      }
                    ]
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS) v0.1.1",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 142139975,
            "numberOfFiles": 2119
          },
          {
            "directoryName": "clinical_data",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains clinical data collected through REDCap, including blood/urine lab values and survey data. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Clinical Data",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C15783",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C15783"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory is named following the specification of this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "The Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)",
                "standardDescription": "Standard designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence.",
                "standardUse": "All the data files within this directory follow this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.qk984b",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "dqd_omop.json",
                "metadataFileDescription": "The dqd_omop.json file supports OMOP CDM data quality analysis using the OMOP CDM Data Quality Dashboard (DQD). The OMOP CDM DQD tool (https://ohdsi.github.io/DataQualityDashboard/) runs a set of > 3500 data quality checks against an OMOP CDM instance",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/DataQualityDashboard/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 67270057,
            "numberOfFiles": 7
          },
          {
            "directoryName": "environment",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through an environmental sensor device custom built for the AI-READI project.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Environmental sensor data"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "ASCII File Format Guidelines for Earth Science Data",
                "standardDescription": "NASA recommended practices for formatting and describing ASCII encoded data files",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "environmental_sensor",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "leelab_anura",
                    "directoryDescription": "You can learn more about the data in this directory by consulting relevant documentation. Search 'AS7431' with Type set as 'Datasheet' at https://ams-osram.com/support/download-center. Search 'DS3231 Precision RTC' at https://www.adafruit.com, look for product ID 5188, select this link and scroll down to Technical Details to find a link for the Datasheet (as of this writing, you may find the datasheet at https://www.analog.com/media/en/technical-documentation/data-sheets/DS3231.pdf). Search 'Datasheet SEN5x' in the search box at https://sensirion.com/ to get the document titled 'Datasheet SEN5x' (the name of the downloaded file may be 'Sensirion_Datasheet_Environmental_Node_SEN5x.pdf'). Search 'NOx Index' in the search box at https://sensirion.com to get the document titled 'What is Sensirion's NOx Index?' (the name of the downloaded file may be 'Info_Note NOx_Index.pdf').",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 26396580156,
            "numberOfFiles": 1045
          },
          {
            "directoryName": "retinal_flio",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Fuorescence Lifetime Imaging Ophthalmoscopy"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "flio",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_flio",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 504936506725,
            "numberOfFiles": 3763
          },
          {
            "directoryName": "retinal_oct",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C20828",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C20828"
                  }
                ]
              },
              {
                "relatedTermValue": "Tomography, Optical Coherence",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D041623",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D041623"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "structural_oct",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 698763967142,
            "numberOfFiles": 25732
          },
          {
            "directoryName": "retinal_octa",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography"
              },
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography Images"
              },
              {
                "relatedTermValue": "OCTA Images"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "enface",
                "directoryType": "modality",
                "directoryDescription": "This directory contains en face data collected using OCTA. En face images, derived from 3D volume scans, are also referred to as C-scan OCT. These images provide a 2D view of the retina layers and have an orientation siimilar to fundus photographs.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "flow_cube",
                "directoryType": "modality",
                "directoryDescription": "This directory contains flow cube data collected using OCTA. Flow cube provides information on blood flow in a 3D view.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Maestro2 device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Triton device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "segmentation",
                "directoryType": "modality",
                "directoryDescription": "This directory contains segmentation data collected using OCTA. The segmentation information is presented in the form of heightmaps and includes information about the associated layers.",
                "directoryList": [
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Maestro device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 705745431960,
            "numberOfFiles": 81675
          },
          {
            "directoryName": "retinal_photography",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              },
              {
                "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Fundus_photography",
                "relatedIdentifierType": "URL",
                "relationType": "IsDescribedBy",
                "resourceTypeGeneral": "Other"
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Eye Fundus Photography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C147467",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C147467"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "cfp",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ophthalmology.med.ubc.ca/patient-care/ophthalmic-photography/color-fundus-photography/#:~:text=Color%20Fundus%20Retinal%20Photography%20uses,monitor%20their%20change%20over%20time",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "optomed_aurora",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Aurora device, manufactured by Optomed."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Triton device, manufactured by Topcon."
                  }
                ]
              },
              {
                "directoryName": "faf",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specificallly fundus autofluorescence photographs that uses the fluorescent characteristics of lipofuscin in an non-invasive way.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://eyewiki.aao.org/Fundus_Autofluorescence",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains fundus autofluorescence photographs from the Eidon device, manufactured by iCare."
                  }
                ]
              },
              {
                "directoryName": "ir",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data using near-infrared reflectance (IR).",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8349282/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 55737846240,
            "numberOfFiles": 43421
          },
          {
            "directoryName": "wearable_activity_monitor",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a wearable fitness tracker.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "smartwatch"
              },
              {
                "relatedTermValue": "activity monitoring"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "heart_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "oxygen_saturation",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity_calorie",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "respiratory_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "sleep",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "stress",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 17170612349,
            "numberOfFiles": 6230
          },
          {
            "directoryName": "wearable_blood_glucose",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a continuous glucose monitoring (CGM) device.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Continuous Glucose Monitoring System",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C159776",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C159776"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "continuous_glucose_monitoring",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "dexcom_g6",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1938813596,
            "numberOfFiles": 1050
          }
        ],
        "metadataFileList": [
          {
            "metadataFileName": "CHANGELOG.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_structure_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "healthsheet.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "LICENSE.txt",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.tsv",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "README.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "study_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          }
        ]
      },
      "healthsheet": {
        "generalInformation": [
          {
            "id": 1,
            "question": "Provide a 2 sentence summary of this dataset.",
            "response": "The Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) is a dataset consisting of data collected from individuals with and without \u201cType 2 Diabetes Mellitus (T2DM)\u201d and harmonized across 3 data collection sites. The composition of the dataset was designed with future studies using AI/Machine Learning in mind. This included recruitment sampling procedures aimed at achieving approximately equal distribution of participants across sex, race, and diabetes severity, as well as the design of a data acquisition protocol across multiple domains (survey data, physical measurements, clinical data, imaging data, wearable device data, etc.) to enable downstream AI/ML analyses that may not be feasible with existing data sources such as claims or electronic health records data. The goal is to better understand salutogenesis (the pathway from disease to health) in T2DM. Some data that are not considered to be sensitive personal health data will be available to the public for download upon agreement with a license that defines how the data can be used. The full dataset will be accessible by entering into a data use agreement. The public dataset will include survey data, blood and urine lab results, fitness activity levels, clinical measurements (e.g. monofilament and cognitive function testing), retinal images, ECG, blood sugar levels, and home air quality. The data held under controlled access include 5-digit zip code, sex, race, ethnicity, genetic sequencing data, medications, past health records, and traffic and accident reports. Of note, the overall enrollment goal is to have balanced distribution between different racial groups. As enrollment is ongoing, periodic updates to data releases may not have achieved balanced distribution across groups."
          },
          {
            "id": 2,
            "question": "Has the dataset been audited before? If yes, by whom and what are the results?",
            "response": "The dataset has not undergone any formal external audits. However, the dataset has been reviewed internally by AI-READI team members for quality checks and to ensure that no personally identifiable information was accidentally included."
          }
        ],
        "versioning": [
          {
            "id": 1,
            "question": "Does the dataset get released as static versions or is it dynamically updated?\n\n   a. If static, how many versions of the dataset exist?\n\n   b. If dynamic, how frequently is the dataset updated?",
            "response": "The dataset gets released as static versions. This is the second version of the dataset and consists of data collected during the first year of the study, i.e. between July 19, 2023 and July, 31 2024 (the first version of the dataset consisted of data collected only during the pilot data collection phase). There are plans to release new versions of the dataset approximately once a year with additional data from participants who have been enrolled since the last dataset version release."
          },
          {
            "id": 2,
            "question": "Is this datasheet created for the original version of the dataset? If not, which version of the dataset is this datasheet for?",
            "response": "This datasheet is created for the second version of the dataset."
          },
          {
            "id": 3,
            "question": "Are there any datasheets created for any versions of this dataset?",
            "response": "There was a previous datasheet created for the first version of the dataset, which consisted of data collected during the pilot data collection phase. It is available here: https://docs.aireadi.org/docs/1/dataset/healthsheet"
          },
          {
            "id": 4,
            "question": "Does the current version/subversion of the dataset come with predefined task(s), labels, and recommended data splits (e.g., for training, development/validation, testing)? If yes, please provide a high-level description of the introduced tasks, data splits, and labeling, and explain the rationale behind them. Please provide the related links and references. If not, is there any resource (website, portal, etc.) to keep track of all defined tasks and/or associated label definitions? (please note that more detailed questions w.r.t labeling is provided in further sections)",
            "response": "See response to question #6 under \u201cLabeling and subjectivity of labeling\u201d."
          },
          {
            "id": 5,
            "question": "If the dataset has multiple versions, and this datasheet represents one of them, answer the following questions:\n\n   a. What are the characteristics that have been changed between different versions of the dataset?\n\n This version of the dataset includes more patients than the first version of the dataset. \n\n  b. Explain the motivation/rationale for creating the current version of the dataset.\n\n The current version of the dataset includes data from the first year of the study, rather than only the data collected from the pilot data collection phase (which comprised the first version of the dataset). \n\n  c. Does this version have more subjects/patients represented in the data, or fewer?\n\n This version has more subjects/patients represented in the data. \n\n  d. Does this version of the dataset have extended data or new data from the same patients as the older versions? Were any patients, data fields, or data points removed? If so, why?\n\n No, the data fields/types are the same as the prior version of the dataset. \n\n  e. Do we expect more versions of the dataset to be released?\n\n Yes, enrollment is ongoing, and future versions of the dataset will be released that will include larger numbers of subjects/patients as enrollment increases. \n\n  f. Is this datasheet for a version of the dataset? If yes, does this sub-version of the dataset introduce a new task, labeling, and/or recommended data splits? If the answer to any of these questions is yes, explain the rationale behind it.\n\n This datasheet is for the first year of the study and is the second version of the dataset.  It does include a new recommended data split for the 1067 participants balancing training, validation, and testing sets for age, sex, races/ethnicities, and study group.\n\n  g. Are you aware of any widespread version(s)/subversion(s) of the dataset? If yes, what is the addressed task, or application that is addressed?",
            "response": "No"
          }
        ],
        "motivation": [
          {
            "id": 2,
            "question": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.",
            "response": "The purpose for creating the dataset was to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. T2DM is a growing public health threat. Yet, the current understanding of T2DM, especially in the context of salutogenesis, is limited. Given the complexity of T2DM, AI-based approaches may help with improving our understanding but a key issue is the lack of data ready for training AI models. The AI-READI dataset is intended to fill this gap."
          },
          {
            "id": 3,
            "question": "What are the applications that the dataset is meant to address? (e.g., administrative applications, software applications, research)",
            "response": "The multi-modal dataset being collected is being gathered to facilitate downstream pseudotime manifolds and various applications in artificial intelligence."
          },
          {
            "id": 4,
            "question": "Are there any types of usage or applications that are discouraged from using this dataset? If so, why?",
            "response": "The AI READI dataset License imposes certain restrictions on the usage of the data. The restrictions are described in the License files available at https://doi.org/10.5281/zenodo.10642459. Briefly, the Licensee shall not: \u201c(i) make clinical treatment decisions based on the Data, as it is intended solely as a research resource, or (ii) use or attempt to use the Data, alone or in concert with other information, to compromise or otherwise infringe the confidentiality of information on an individual person who is the source of any Data or any clinical data or biological sample from which Data has been generated (a 'Data Subject' and their right to privacy, to identify or contact any individual Data Subject or group of Data Subjects, to extract or extrapolate any identifying information about a Data Subject, to establish a particular Data Subject's membership in a particular group of persons, or otherwise to cause harm or injury to any Data Subject.\u201d"
          },
          {
            "id": 5,
            "question": "Who created this dataset (e.g., which team, research group), and on behalf of which entity (e.g., company, institution, organization)?",
            "response": "This dataset was created by members of the AI-READI project, hereby referred to as the AI-READI Consortium. Details about each member and their institutions are available on the project website at https://aireadi.org."
          },
          {
            "id": 6,
            "question": "Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. If the funding institution differs from the research organization creating and managing the dataset, please state how.",
            "response": "The creation of the dataset was funded by the National Institutes of Health (NIH) through their Bridge2AI Program (https://commonfund.nih.gov/bridge2ai). The grant number is OT2ODO32644 and more information about the funding is available at https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481. Note that the funding institution is not creating or managing the dataset. The dataset is created and managed by the awardees of the grant (c.f. answer to the previous question)."
          },
          {
            "id": 7,
            "question": "What is the distribution of backgrounds and experience/expertise of the dataset curators/generators?",
            "response": "There is a wide range of experience within the project team, including senior, mid-career, and early career faculty members as well as clinical research coordinators, staff, and interns. They collectively cover many areas of expertise including clinical research, data collection, data management, data standards, bioinformatics, team science, and ethics, among others. Visit https://aireadi.org/team for more information."
          }
        ],
        "composition": [
          {
            "id": 1,
            "question": "What do the instances that comprise the dataset represent (e.g., documents, images, people, countries)? Are there multiple types of instances? Please provide a description.",
            "response": "Each instance represents an individual patient."
          },
          {
            "id": 2,
            "question": "How many instances are there in total (of each type, if appropriate) (breakdown based on schema, provide data stats)?",
            "response": "There are 1067 instances in this current version of the dataset (version 2, released fall 2024)."
          },
          {
            "id": 3,
            "question": "How many patients / subjects does this dataset represent? Answer this for both the preliminary dataset and the current version of the dataset.",
            "response": "This version of the dataset from 1067 participants. The first version of the dataset, composed of data from the pilot data collection phase, had 204 instances."
          },
          {
            "id": 4,
            "question": "Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). Answer this question for the preliminary version and the current version of the dataset in question.",
            "response": "The dataset contains all possible instances. More specifically, the dataset contains data from all participants who have been enrolled during the first year of data collection for AI-READI."
          },
          {
            "id": 5,
            "question": "What data modality does each patient data consist of? If the data is hierarchical, provide the modality details for all levels (e.g: text, image, physiological signal). Break down in all levels and specify the modalities and devices.",
            "response": "Multiple modalities of data are collected for each participant, including survey data, clinical data, retinal imaging data, environmental sensor data, continuous glucose monitor data, and wearable activity monitor data. These encompass tabular data, imaging data, and physiological signal/waveform data. There is no unstructured text data included in this dataset. The exact forms used for data collection in REDCap are available [here](https://docs.aireadi.org/files/REDCap%20surveys%20and%20forms.pdf). Furthermore, all modalities, file formats, and devices are detailed in the dataset documentation at https://docs.aireadi.org/."
          },
          {
            "id": 6,
            "question": "What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.",
            "response": "Each instance consists of all of the data available for an individual participating in the study. See answer to question 5 for the data types associated with each instance."
          },
          {
            "id": 7,
            "question": "Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).?",
            "response": "Yes, not all modalities are available for all participants. Some participants elected not to participate in some study elements. In a few cases, the data collection device did not have any stored results or was returned too late to retrieve the results (e.g. battery died, data was lost). In a few cases, there may have been a data collision at some point in the process and data has been lost."
          },
          {
            "id": 8,
            "question": "Are relationships between individual instances made explicit? (e.g., They are all part of the same clinical trial, or a patient has multiple hospital visits and each visit is one instance)? If so, please describe how these relationships are made explicit.",
            "response": "Yes - all instances are part of the same prospective data generation project (AI-READI). There is currently only one visit per participant."
          },
          {
            "id": 9,
            "question": "Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. (e.g., losing data due to battery failure, or in survey data subjects skip the question, radiological sources of noise).",
            "response": "In cases of survey data, skipped questions or incomplete responses are expected. In cases of using wearables, improper use, technical failure such as battery failure or system malfunction are expected. In cases of imaging data, patient uncooperation, noise that may obscure the images and technical failure such as system malfunction, and data transfer failures are expected."
          },
          {
            "id": 10,
            "question": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, other datasets)? If it links to or relies on external resources,\n\n a. are there guarantees that they will exist, and remain constant, over time;\n\n b. are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created);\n\n c. are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.",
            "response": "The dataset is self-contained but does rely on the dataset documentation for users requiring additional information about the provenance of the dataset. The documentation is available at https://docs.aireadi.org. The documentation is shared under the CC-BY 4.0 license, so there are no restrictions associated with its use."
          },
          {
            "id": 11,
            "question": "Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications that is confidential)? If so, please provide a description.",
            "response": "No, the dataset does not contain data that might be considered confidential. No personally identifiable information is included in the dataset."
          },
          {
            "id": 12,
            "question": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise pose any safety risk (such as psychological safety and anxiety)? If so, please describe why.",
            "response": "No."
          },
          {
            "id": 13,
            "question": "If the dataset has been de-identified, were any measures taken to avoid the re-identification of individuals? Examples of such measures: removing patients with rare pathologies or shifting time stamps.",
            "response": ""
          },
          {
            "id": 14,
            "question": "Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.",
            "response": "No, the public dataset will not contain data that is considered sensitive. However, the controlled access dataset will contain data regarding racial and ethnic origins, location (5-digit zip code), as well as motor vehicle accident reports."
          }
        ],
        "devices": [
          {
            "id": 1,
            "question": "For data that requires a device or equipment for collection or the context of the experiment, answer the following additional questions or provide relevant information based on the device or context that is used (for example)\n\n   a. If there was an MRI machine used, what is the MRI machine and model used?\n\n   b. If heart rate was measured what is the device for heart rate variation that is used?\n\n   c. If cortisol measurement is reported at multi site, provide details,\n\n   d. If smartphones were used to collect the data, provide the names of models.\n\n   e. And so on,..",
            "response": "The devices included in the study are as follows, and more details can be found at [https://docs.aireadi.org](https://docs.aireadi.org):\n\n   **Environmental sensor device**\n\n   Participants will be sent home with an environmental sensor (a custom-designed sensor unit called the LeeLab Anura), which they will use for 10 continuous days before returning the devices to the clinical research coordinators for data download.\n\n   **Continuous glucose monitor (Dexcom G6)**\n\n   The Dexcom G6 is a real-time, integrated continuous glucose monitoring system (iCGM) that directly monitors blood glucose levels without requiring finger sticks. It must be worn continuously in order to collect data.\n\n   **Wearable accelerometer (Physical activity monitor)**\n\n   The Garmin Vivosmart 5 Fitness Activity tracker will be used to measure data related to physical activity.\n\n   **Heart rate**\n\n   Heart rate can be read from EKG or blood pressure measurement devices.\n\n   **Blood pressure**\n\n   Blood pressure devices used for the study across the various data acquisition sites are: OMRON HEM 907XL Blood Pressure Monitor, Medline MDS4001 Automatic Digital Blood Pressure Monitor, and Welch Allyn 6000 series Vital signs monitor with Welch Allyn FlexiPort Reusable Blood Pressure Cuff.\n\n   **Visual acuity**\n\n   M&S Technologies EVA device to test visual acuity. The test is administered at a distance of 4 meters from a touch-screen monitor that is 12x20 inches. Participants will read letters from the screen. Photopic Conditions: No neutral density filters are used. A general occluder will be used for photopic testing. The participant wears their own prescription spectacles or trial frames. For Mesopic conditions, a neutral density (ND) filter will be used. The ND filter will either be a lens added to trial frames to reduce incoming light on the tested eye, OR a handheld occluder with a neutral density\n   filter (which we will designate as \u201cND-occluder) over the glasses will be used. The ND-occluder is different from a standard occluder and is used only for vision testing under mesopic conditions.\n\n   **Contrast sensitivity**\n\n   The MARS Letter Contrast Sensitivity test (Perceptrix) was conducted monocularly under both Photopic conditions (with a general occluder) and Mesopic conditions (using a Neutral Density occluder with a low luminance filter lens). The standardized order of MARS cards was as follows: Photopic OD, Photopic OS, Mesopic OD, and Mesopic OS. The background luminance of the charts fell within the range of 60 to 120 cd/m2, with an optimal level of 85 cd/m2. Illuminance was recommended to be between 189 to 377 lux, with an optimal level of 267 lux. While the designed viewing distance was 50 cm, it could vary between 40 to 59 cm. Patients were required to wear their appropriate near correction: reading glasses or trial frames with +2.00D lenses. All testing was carried out under undilated conditions. Patients were instructed to read the letter left to right across each line on the chart. Patients were encouraged to guess, even if they perceived the letters as too faint. Testing was terminated either when the patient made two consecutive errors or reached the end of the chart. The log contrast sensitivity (log CS) values were recorded by multiplying the number of errors prior to the final correct letter by 0.04 and subtracting the result from the log CS value at the final correct letter. If a patient reached the end of the chart without making two consecutive errors, the final correct letter was simply the last one correctly identified.\n\n   **Autorefraction**\n\n   KR 800 Auto Keratometer/Refractor.\n\n   **EKG**\n\n   Philips (manufacturer of Pagewriter TC30 Cardiograph)\n\n   **Lensometer**\n\n   Lensometer devices used at data acquisition sites across the study include: NIDEK LM-600P Auto Lensometer, Topcon-CL-200 computerized Lensometer, and Topcon-CL-300 computerized Lensometer\n\n   **Undilated fundus photography - Optomed Aurora**\n\n   The Optomed Aurora IQ is a handheld fundus camera that can take non-mydriatic images of the ocular fundus. It has a 50\u00b0 field of view, 5 Mpix sensor, and high-contrast optical design. The camera is non-mydriatic, meaning it doesn't require the pupil to be dilated, so it can be used for detailed viewing of the retina. Images taken during the AI-READI visit, are undilated images taken in a dark room while a patient is sitting on a comfortable chair, laying back. As it becomes challenging to get a good view because of the patients not being dilated and the handheld nature of this imaging modality, the quality of the images vary from patient to patient and within the same patient.\n\n   **Dilated fundus photography - Eidon**\n\n   The iCare EIDON is a widefield TrueColor confocal fundus imaging system that can capture images up to 200\u00b0. It comes with multiple imaging modalities, including TrueColor, blue, red, Red-Free, and infrared confocal images. The system offers widefield, ultra-high-resolution imaging and the capability to image through cataract and media opacities. It operates without dilation (minimum pupil 2.5 mm) and provides the flexibility of both fully automated and fully manual modes. Additionally, the iCare EIDON features an all-in-one compact design, eliminating the need for an additional PC. AI READI images using EIDON include two main modalities: 1. Single Field Central IR/FAF 2. Smart Horizontal Mosaic. Imaging is done in fully automated mode in a dark room with the machine moving and positioning according to the patient's head aiming at optimizing the view and minimizing operator's involvement/operator induced noise.\n\n   **Spectralis HRA (Heidelberg Engineering)**\n\n   The Heidelberg Spectralis HRA+OCT is an ophthalmic imaging system that combines optical coherence tomography (OCT) with retinal angiography. It is a modular, upgradable platform that allows clinicians to configure it for their specific diagnostic workflow. It has the confocal scanning laser ophthalmoscope (cSLO) technology that not only offers documentation of clinical findings but also often highlights critical diagnostic details that are not visible on traditional clinical ophthalmoscopy. Since cSLO imaging minimizes the effects of light scatter, it can be used effectively even in patients with cataracts. For AI READI subjects, imaging is done in a dark room using the following modalities: ONH-RC, PPole-H, and OCTA of the macula. As the machine is operated by the imaging team and is not fully automated, quality issues may arise, which may lead to skipping this modality and missing data.\n\n   **Triton DRI OCT (Topcon Healthcare)**\n\n   The DRI OCT Triton is a device from Topcon Healthcare that combines swept-source OCT technology with multimodal fundus imaging. The DRI OCT Triton uses swept-source technology to visualize the deepest layers of the eye, including through cataracts. It also enhances visualization of outer retinal structures and deep pathologies. The DRI OCT Triton has a 1,050 nm wavelength light source and a non-mydriatic color fundus camera. AI READI imaging is done in a dark room with minimal intervention from the imager as the machine positioning is done automatically. This leads to higher quality images with minimal operator induced error. Imaging is done in 12.0X12.0 mm and 6.0X6.0 mm OCTA, and 12.0 mm X9.0 mmX6.0 mm 3D Horizontal and Radial scan modes.\n\n   **Maestro2 3D OCT (Topcon Healthcare)**\n\n   The Maestro2 is a robotic OCT and color fundus camera system from Topcon Healthcare. It can capture a 12 mm x 9 mm wide-field OCT scan that includes the macula and optic disc. The Maestro2 can also capture high-resolution non-mydriatic, true color fundus photography, OCT, and OCTA with a single button press. Imaging is done in a dark room and automatically with minimal involvement of the operator. Protocols include 12.0 mm X9.0 mm widefield, 6.0 mm X 6.0 mm 3D macula scan and 6.0 mm X 6.0 mm OCTA (scan rate: 50 kHz).\n\n   **FLIO (Heidelberg Engineering)**\n\n   Fluorescence Lifetime Imaging Ophthalmoscopy (FLIO) is an advanced imaging technique used in ophthalmology. It is a non-invasive method that provides valuable information about the metabolic and functional status of the retina. FLIO is based on the measurement of fluorescence lifetimes, which is the duration a fluorophore remains in its excited state before emitting a photon and returning to the ground state. FLIO utilizes this fluorescence lifetime information to capture and analyze the metabolic processes occurring in the retina. Different retinal structures and molecules exhibit distinct fluorescence lifetimes, allowing for the visualization of metabolic changes, cellular activity, and the identification of specific biomolecules. The imaging is done by an operator in a dark room analogous to a straightforward heidelberg spectralis OCT. However, as it takes longer than a usual spectralis OCT and exposes patients to uncomfortable levels of light, it is kept to be performed as the last modality of an AI READI visit. Because of this patients may not be at their best possible compliance.\n\n   **Cirrus 5000 Angioplex (Carl Zeiss Meditec)**\n\n   The Zeiss Cirrus 5000 Angioplex is a high-definition optical coherence tomography (OCT) system that offers non-invasive imaging of retinal microvasculature. The imaging is done in a dark room by an operator and it is pretty straightforward and analogous to what is done in the ophthalmology clinics on a day to day basis. Imaging protocols include 512 X 512 and 200 X 200 macula and ONH scans and also OCTA of the macula. Zeiss Cirrus 5000 also provides a 60-degree OCTA widefield view. 8x8mm single scans and 14x14mm automated OCTA montage allow for rapid peripheral assessment of the retina as well.\n\n   **Monofilament testing for peripheral neuropathy**\n\n   Monofilament test is a standard clinical test to monitor peripheral neuropathy in diabetic patients. It is done using a standard 10g monofilament applying pressure to different points on the plantar surface of the feet. If patients sense the monofilament, they confirm by saying \u201cyes\u201d; if patients do not sense the monofilament after it bends, they are considered to be insensate. When the sequence is completed, the insensate area is retested for confirmation. This sequence is further repeated randomly at each of the testing sites on each foot until results are obtained.The results are recorded on an iPad, Laptop, or a paper questionnaire and are directly added to the project's RedCap by the clinical research staff.\n\n   **Montreal Cognitive Assessment (MoCA)**\n\n   The Montreal Cognitive Assessment (MoCA) is a simple, in-office screening tool that helps detect mild cognitive impairment and early onset of dementia. The MoCA evaluates cognitive domains such as: Memory, Executive functioning, Attention, Language, Visuospatial, Orientation, Visuoconstructional skills, Conceptual thinking, Calculations. The MoCA generates a total score and six domain-specific index scores. The maximum score is 30, and anything below 24 is a sign of cognitive impairment. A final total score of 26 and above is considered normal. Some disadvantages of the MoCA include: Professionals require training to score the test, A person's level of education may affect the test, Socioeconomic factors may affect the test, People living with depression or other mental health issues may score similarly to those with mild dementia. AI READI research staff perform this test on an iPad using a pre-installed software (MoCA Duo app downloaded from the app store) that captures all the patients responses in an interactive manner."
          }
        ],
        "challenge": [
          {
            "id": 1,
            "question": "Which factors in the data might limit the generalization of potentially derived models? Is this information available as auxiliary labels for challenge tests? For instance:\n\n   a. Number and diversity of devices included in the dataset.\n\n   b. Data recording specificities, e.g., the view for a chest x-ray image.\n\n   c. Number and diversity of recording sites included in the dataset.\n\n   d. Distribution shifts over time.",
            "response": "While the AI-READI's cross-sectional database ultimately aims to achieve balance across race/ethnicity, biological sex, and diabetes presence and severity, the pilot study is not balanced across these parameters.\n\n   Three recording sites were strategically selected to achieve diverse recruitment: the University of Alabama at Birmingham (UAB), the University of California San Diego (UCSD), and the University of Washington (UW). The sites were chosen for geographic diversity across the United States and to ensure diverse representation across various racial and ethnic groups. Individuals from all demographic backgrounds were recruited at all 3 sites.\n\n   Factors influencing the generalization of derived models include the predominantly urban and hospital-based recruitment, which may not fully capture diverse cultural and socioeconomic backgrounds. The study cohort may not provide a comprehensive representation of the population, as it does not include other races/ethnicities such as Pacific Islanders and Native Americans.\n\n   Information on device make and model, including specific modalities like macula scans or wide scans during OCT, were documented to ensure repeatability. Moreover, the study included multiple devices for one measure to enhance generalizability and represent the diverse range of equipment utilized in clinical settings."
          },
          {
            "id": 2,
            "question": "What confounding factors might be present in the data?\n\n   a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another?\n\n   b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another?",
            "response": "a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another? b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another? Uniform data collection protocols were implemented for all subjects, irrespective of their race/ethnicity, biological sex, or diabetes severity, across all study sites. The selection of study sites was intended to ensure equitable representation and minimize the potential for sampling bias."
          }
        ],
        "demographicInformation": [
          {
            "id": 1,
            "question": "Does the dataset identify any demographic sub-populations (e.g., by age, gender, sex, ethnicity)?",
            "response": "No"
          },
          {
            "id": 2,
            "question": "If no,\n\n   a. Is there any regulation that prevents demographic data collection in your study (for example, the country that the data is collected in)?\n\n   b. Are you employing methods to reduce the disparity of error rate between different demographic subgroups when demographic labels are unavailable? Please describe.",
            "response": "No. We are suggesting a split for training/validation/testing models that is aimed at reducing disparities in models developed using this dataset."
          }
        ],
        "preprocessing": [
          {
            "id": 1,
            "question": "Was there any pre-processing for the de-identification of the patients? Provide the answer for the preliminary and the current version of the dataset",
            "response": ""
          },
          {
            "id": 2,
            "question": "Was there any pre-processing for cleaning the data? Provide the answer for the preliminary and the current version of the dataset",
            "response": "There were several quality control measures used at the time of data entry/acquisition. For example, clinical data outside of expected min/max ranges were flagged in REDCap, which was visible in reports viewed by clinical research coordinators (CRCs) and Data Managers. Using these REDCap reports as guides, Data Managers and CRCs examined participant records and determined if an error was likely. Data were checked for the following and edited if errors were detected:\n\n   1. Credibility, based on range checks to determine if all responses fall within a prespecified reasonable range\n\n   2. Incorrect flow through prescribed skip patterns\n\n   3. Missing data that can be directly filed from other portions of an individual's record\n\n   4. The omission and/or duplication of records\n\n   Editing was only done under the guidance and approval of the site PI. If corrected data was available from elsewhere in the respondent's answers, the error was corrected. If there was no logical or appropriate way to correct the data, the Data site PI reviewed the values and made decisions about whether those values should be removed from the data.\n\n   Once data were sent from each of the study sites to the central project team, additional processing steps were conducted in preparation for dissemination. For example, all data were mapped to standardized terminologies when possible, such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, a common data model for observational health data, and the Digital Imaging and Communications in Medicine (DICOM), a commonly used standard for medical imaging data. Details about the data processing approaches for each data domain/modality are described in the dataset documentation at https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Was the \u201craw\u201d data (post de-identification) saved in addition to the preprocessed/cleaned data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data",
            "response": "The raw data is saved and expected to be preserved by the AI-READI project at least for the duration of the project but is not anticipated to be shared outside the project team right now, because it has not been mapped to standardized terminologies and because the raw data may accidentally include personal health information or personally identifiable information (e.g. in free text fields). There is a possibility that raw data may be included in future releases of the controlled access dataset."
          },
          {
            "id": 4,
            "question": "Were instances excluded from the dataset at the time of preprocessing? If so, why? For example, instances related to patients under 18 might be discarded.",
            "response": "No data were excluded from the dataset at the time of preprocessing. However, regarding to study recruitment (i.e. ability to participate in the study), the following eligibility criteria were used:\n\n   Inclusion Criteria:\n\n   - Able to provide consent\n   - \u2265 40 years old\n   - Persons with or without type 2 diabetes\n   - Must speak and read English\n\n   Exclusion Criteria:\n\n   - Must not be pregnant\n   - Must not have gestational diabetes\n   - Must not have Type 1 diabetes"
          },
          {
            "id": 5,
            "question": "If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Answer this question for both the preliminary dataset and the current version of the dataset",
            "response": "N/A"
          }
        ],
        "labeling": [
          {
            "id": 1,
            "question": "Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version.\n\n   a. If yes:\n\n   1. What are the labels provided?\n\n   2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?\n\n  N/A - no labels are provided. \n\n b. What labeling strategy was used?\n\n   1. Gold standard label available in the data (e.g. cancers validated by biopsies)\n\n   2. Proxy label computed from available data:\n\n      1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions)\n\n      2. Which tables and features were considered to compute the label?\n\n   3. Which proportion of the data has gold standard labels?\n\n  N/A - no labels are provided \n\n c. Human-labeled data\n\n   1. How many labellers were considered?\n\n   2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, \u2026)\n\n   3. What guidelines did they follow?\n\n   4. How many labellers provide a label per instance?\n\n      If multiple labellers per instance:\n\n      1. What is the rater agreement? How was disagreement handled?\n      2. Are all labels provided, or summaries (e.g. maximum vote)?\n\n   5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes)\n\n   6. On average, how much time was required to annotate each instance?\n\n   7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?",
            "response": "N/A - no labels are provided. \n\n No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications."
          },
          {
            "id": 2,
            "question": "What are the human level performances in the applications that the dataset is supposed to address?",
            "response": "N/A"
          },
          {
            "id": 3,
            "question": "Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. ",
            "response": "N/A - no labeling was performed"
          },
          {
            "id": 4,
            "question": "Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?",
            "response": "No, we do not have formal guidelines in place."
          },
          {
            "id": 5,
            "question": "Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.",
            "response": "The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced as well as possible for sex, race/ethnicity and diabetes status (no diabetes, prediabetes/lifestyle controlled, oral medication controlled, and insulin controlled)."
          }
        ],
        "collection": [
          {
            "id": 1,
            "question": "Were any REB/IRB approval (e.g., by an institutional review board or research ethics board) received? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "The initial IRB approval at the University of Washington was received on December 20, 2022. The initial approval letter can be found [here](https://docs.aireadi.org/files/Approval_STUDY00016228_Lee_initial.pdf). An annual renewal application to the IRB about the status and progress of the study is required and due within 90 days of expiration."
          },
          {
            "id": 2,
            "question": "How was the data associated with each instance acquired? Was the data directly observable (e.g., medical images, labs or vitals), reported by subjects (e.g., survey responses, pain levels, itching/burning sensations), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.",
            "response": "The acquisition of data varied based on the domain; some data were directly observable (such as labs, vitals, and retinal imaging), whereas other data were reported by subjects (e.g. survey responses). Verification of data entry was performed when possible (e.g. cross-referencing entered medications with medications that were physically brought in or photographed by each study participant). Details for each data domain are available in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Provide the answer for all modalities and collected data. Has this information been changed through the process? If so, explain why.",
            "response": "The procedures for data collection and processing is available at https://docs.aireadi.org."
          },
          {
            "id": 4,
            "question": "Who was involved in the data collection process (e.g., patients, clinicians, doctors, ML researchers, hospital staff, vendors, etc.) and how were they compensated (e.g., how much were contributors paid)?",
            "response": "Details about the AI-READI team members involved in the data collection process are available at https://aireadi.org/team. Their effort was supported by the National Institutes of Health award OT2OD032644 based on the percentage of effort contributed, and salaries which aligned with the funding guidelines at each site. Study subjects received a compensation of $200 for the study visit also through the grant funding."
          },
          {
            "id": 5,
            "question": "Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.",
            "response": "The timeline for the overall project spans four years, encompassing one year dedicated to protocol development and training, and years 2-4 allocated for subject recruitment and data collection. Approximately 4% of participants are expected to undergo a follow-up examination in Year 4. The data collection process is specifically tailored to enable downstream pseudotime manifold analysis\u2014an approach used to predict disease trajectories. This involves gathering and learning from complex, multimodal data from participants exhibiting varying disease severity, ranging from normal to insulin-dependent Type 2 Diabetes Mellitus (T2DM). The timeframe also allows for the collection of the highest number of subjects possible to ensure a balanced representation of racial and ethnic groups and mitigate biases in computer vision algorithms.\n\nFor this version of the dataset, the timeframe for data collection was July 19, 2023 to July 31, 2024."
          },
          {
            "id": 6,
            "question": "Does the dataset relate to people? If not, you may skip the remaining questions in this section.",
            "response": "Yes"
          },
          {
            "id": 7,
            "question": "Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., hospitals, app company)?",
            "response": "The data was collected directly from participants across the three recruiting sites. Recruitment pools were identified by screening Electronic Health Records (EHR) for diabetes and prediabetes ICD-10 codes for all patients who have had an encounter with the sites' health systems within the past 2 years.\n\n"
          },
          {
            "id": 8,
            "question": "Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.",
            "response": "Yes, each individual was aware of the data collection, as this was not passive data collection or secondary use of existing data, but rather active data collection directly from participants."
          },
          {
            "id": 9,
            "question": "Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.",
            "response": "Informed consent to participate was required before participation in any part of the protocol (including questionnaires). Potential participants were given the option to read all consent documentation electronically (e-consent) before their visit and give their consent with an electronic signature without verbal communication with a clinical research coordinator. Participants may access e-consent documentation in REDCap and decide at that point they do not want to participate or would like additional information. The approved consent form for the principal project site University of Washington is available [here](https://docs.aireadi.org/v2/AI-READI%20Consent_Form_Standard_Mod17May2023_useforpilot.pdf). The other clinical sites had IRB reliance and used the same consent form, with minor institution-specific language incorporated depending on individual institutional requirements."
          },
          {
            "id": 10,
            "question": "If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).",
            "response": "Participants were permitted to withdraw consent at any time and cease study participation. However, any data that had been shared or used up to that point would stay in the dataset. This is clearly communicated in the consent document."
          },
          {
            "id": 11,
            "question": "In which countries was the data collected?",
            "response": "USA"
          },
          {
            "id": 12,
            "question": "Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "No, a data protection impact analysis has not been conducted."
          }
        ],
        "inclusion": [
          {
            "id": 1,
            "question": "Is there any language-based communication with patients (e.g: English, French)? If yes, describe the choices of language(s) for communication. (for example, if there is an app used for communication, what are the language options?)",
            "response": "English language was used for communication with study participants."
          },
          {
            "id": 2,
            "question": "What are the accessibility measurements and what aspects were considered when the study was designed and implemented?",
            "response": "Accessibility measurements were not specifically assessed. However, transportation assistance (rideshare services) was offered to study participants who endorsed barriers to transporting themselves to study visits."
          },
          {
            "id": 3,
            "question": "If data is part of a clinical study, what are the inclusion criteria?",
            "response": "The eligibility criteria for the study were as follows:\n\n Inclusion Criteria:\n\n - Able to provide consent\n - \u2265 40 years old\n - Persons with or without type 2 diabetes\n - Must speak and read English\n\n Exclusion Criteria:\n\n - Must not be pregnant\n - Must not have gestational diabetes\n - Must not have Type 1 diabetes"
          }
        ],
        "uses": [
          {
            "id": 1,
            "question": "Has the dataset been used for any tasks already? If so, please provide a description. ",
            "response": "No"
          },
          {
            "id": 2,
            "question": "Does using the dataset require the citation of the paper or any other forms of acknowledgement? If yes, is it easily accessible through google scholar or other repositories",
            "response": "Yes, use of the dataset requires citation to the resources specified in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. (besides Google scholar)",
            "response": "No"
          },
          {
            "id": 4,
            "question": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?",
            "response": "No, to the extent of our knowledge, we do not currently anticipate any uses of the dataset that could result in unfair treatment or harm. However, there is a theoretical risk of future re-identification."
          },
          {
            "id": 5,
            "question": "Are there tasks for which the dataset should not be used? If so, please provide a description. (for example, dataset creators could recommend against using the dataset for considering immigration cases, as part of insurance policies)",
            "response": "This is answered in a prior question (see details regarding license terms)."
          }
        ],
        "distribution": [
          {
            "id": 1,
            "question": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.",
            "response": "The dataset will be distributed and be available for public use."
          },
          {
            "id": 2,
            "question": "How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?",
            "response": "The dataset will be available through the FAIRhub platform (http://fairhub.io/). The dataset' DOI is https://doi.org/10.60775/fairhub.2"
          },
          {
            "id": 3,
            "question": "When was/will the dataset be distributed?",
            "response": "The first version of the dataset was distributed in May 2024, and the second version of the dataset was distributed in November 2024."
          },
          {
            "id": 4,
            "question": "Assuming the dataset is available, will it be/is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.",
            "response": "We provide here the license file containing the terms for reusing the AI-READI dataset (https://doi.org/10.5281/zenodo.10642459). These license terms were specifically tailored to enable reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purpose while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications."
          },
          {
            "id": 5,
            "question": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.10642459)"
          },
          {
            "id": 6,
            "question": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.10642459)"
          }
        ],
        "maintenance": [
          {
            "id": 1,
            "question": "Who is supporting/hosting/maintaining the dataset?",
            "response": "The AI-READI team will be supporting and maintaining the dataset. The dataset is hosted on FAIRhub through Microsoft Azure."
          },
          {
            "id": 2,
            "question": "How can the owner/curator/manager of the dataset be contacted (e.g. email address)?",
            "response": "We refer to the README file included with the dataset for contact information."
          },
          {
            "id": 3,
            "question": "Is there an erratum? If so, please provide a link or other access point.",
            "response": ""
          },
          {
            "id": 4,
            "question": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?",
            "response": "The dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants complete the study visit."
          },
          {
            "id": 5,
            "question": "If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.",
            "response": "There are no limits on the retention of the data associated with the instances."
          },
          {
            "id": 6,
            "question": "Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how and for how long. If not, please describe how its obsolescence will be communicated to users.",
            "response": "N/A - as mentioned in the response to question 4, the dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants are enrolled."
          },
          {
            "id": 7,
            "question": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?",
            "response": "No, currently there is no mechanism for others to extend or augment the AI-READI dataset outside of those who are involved in the project."
          }
        ]
      }
    },
    "files": [
      {
        "children": [
          {
            "label": "ecg_12lead",
            "children": [
              {
                "label": "philips_tc30",
                "children": []
              }
            ]
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "cardiac_ecg"
      },
      {
        "children": [
          {
            "label": "dqd_omop.json",
            "children": []
          }
        ],
        "label": "clinical_data"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "leelab_anura"
              }
            ],
            "label": "environmental_sensor"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "environment"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_flio"
              }
            ],
            "label": "flio"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_flio"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "structural_oct"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_oct"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "enface"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "flow_cube"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "segmentation"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_octa"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "optomed_aurora"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              }
            ],
            "label": "cfp"
          },
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              }
            ],
            "label": "faf"
          },
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "ir"
          }
        ],
        "label": "retinal_photography"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "heart_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "oxygen_saturation"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "physical_activity"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "physical_activity_calorie"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "respiratory_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "sleep"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "stress"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "wearable_activity_monitor"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "dexcom_g6"
              }
            ],
            "label": "continuous_glucose_monitoring"
          }
        ],
        "label": "wearable_blood_glucose"
      },
      {
        "label": "CHANGELOG.md"
      },
      {
        "label": "LICENSE.txt"
      },
      {
        "label": "README.md"
      },
      {
        "label": "dataset_description.json"
      },
      {
        "label": "dataset_structure_description.json"
      },
      {
        "label": "healthsheet.md"
      },
      {
        "label": "participants.json"
      },
      {
        "label": "participants.tsv"
      },
      {
        "label": "study_description.json"
      }
    ],
    "data": {
      "size": 2010899452756,
      "fileCount": 165051,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://fairhub.io/datasets/2",
    "created": "1731052800"
  },
  {
    "id": 15,
    "canonicalId": "agudjafpa77j71jntu5gldy8",
    "datasetId": "lxuj9dtekvbr6rs7gwe620fy",
    "doi": "10.60775/fairhub.3",
    "title": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project",
    "description": "The AI-READI project seeks to create and share a flagship ethically-sourced dataset of type 2 diabetes.",
    "versionTitle": "3.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/study_description.json",
        "identificationModule": {
          "officialTitle": "AI Ready and Exploratory Atlas for Diabetes Insights",
          "acronym": "AI-READI",
          "orgStudyIdInfo": {
            "orgStudyId": "OT2OD032644",
            "orgStudyIdType": "U.S. National Institutes of Health (NIH) Grant/Contract Award Number",
            "orgStudyIdLink": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
          },
          "secondaryIdInfoList": [
            {
              "secondaryId": "NCT06002048",
              "secondaryIdType": "ClinicalTrials.gov",
              "secondaryIdLink": "https://classic.clinicaltrials.gov/ct2/show/NCT06002048"
            }
          ]
        },
        "statusModule": {
          "overallStatus": "Enrolling by invitation",
          "startDateStruct": {
            "startDate": "2023-07-19",
            "startDateType": "Actual"
          },
          "completionDateStruct": {
            "completionDate": "2027-01-01",
            "completionDateType": "Anticipated"
          }
        },
        "sponsorCollaboratorsModule": {
          "leadSponsor": {
            "leadSponsorName": "Washington University in St. Louis",
            "leadSponsorIdentifier": {
              "leadSponsorIdentifierValue": "https://ror.org/01yc7t268",
              "leadSponsorIdentifierScheme": "ROR",
              "schemeURI": "https://ror.org/"
            }
          },
          "responsibleParty": {
            "responsiblePartyType": "Principal Investigator",
            "responsiblePartyInvestigatorFirstName": "Aaron",
            "responsiblePartyInvestigatorLastName": "Lee",
            "responsiblePartyInvestigatorTitle": "Associate Professor",
            "responsiblePartyInvestigatorIdentifier": [
              {
                "responsiblePartyInvestigatorIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                "responsiblePartyInvestigatorIdentifierScheme": "ORCID",
                "schemeURI": "https://orcid.org/"
              }
            ],
            "responsiblePartyInvestigatorAffiliation": {
              "responsiblePartyInvestigatorAffiliationName": "Washington University in St. Louis",
              "responsiblePartyInvestigatorAffiliationIdentifier": {
                "responsiblePartyInvestigatorAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                "responsiblePartyInvestigatorAffiliationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          },
          "collaboratorList": [
            {
              "collaboratorName": "National Institutes of Health",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/01cwqze88",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "California Medical Innovations Institute",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0156zyn36",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Johns Hopkins University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00za53h95",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Oregon Health & Science University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/009avj582",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "Stanford University",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/00f54p054",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of Alabama at Birmingham",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/008s83205",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "collaboratorName": "University of California, San Diego",
              "collaboratorNameIdentifier": {
                "collaboratorNameIdentifierValue": "https://ror.org/0168r3w48",
                "collaboratorNameIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        },
        "oversightModule": {
          "isFDARegulatedDrug": "No",
          "isFDARegulatedDevice": "No",
          "humanSubjectReviewStatus": "Submitted, approved",
          "oversightHasDMC": "No"
        },
        "descriptionModule": {
          "briefSummary": "The study will collect a cross-sectional dataset of 4000 people across the US from diverse racial/ethnic groups who are either 1) healthy, or 2) belong in one of the three stages of diabetes severity (pre-diabetes/lifestyle controlled, oral medication and/or non-insulin-injectable medication controlled, or insulin dependent), forming a total of four groups of patients. Clinical data (social determinants of health surveys, continuous glucose monitoring data, biomarkers, genetic data, retinal imaging, cognitive testing, etc.) will be collected. The purpose of this project is data generation to allow future creation of artificial intelligence/machine learning (AI/ML) algorithms aimed at defining disease trajectories and underlying genetic links in different racial/ethnic cohorts. A smaller subgroup of participants will be invited to come for a follow-up visit in year 4 of the project (longitudinal arm of the study). Data will be placed in an open-source repository and samples will be sent to the study sample repository and used for future research.",
          "detailedDescription": "The Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available."
        },
        "conditionsModule": {
          "conditionList": [
            {
              "conditionName": "Type 2 Diabetes",
              "conditionIdentifier": {
                "conditionClassificationCode": "D003924",
                "conditionScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "conditionURI": "https://meshb.nlm.nih.gov/record/ui?ui=D003924"
              }
            }
          ],
          "keywordList": [
            {
              "keywordValue": "Retinal Imaging"
            },
            {
              "keywordValue": "Data Sharing",
              "keywordIdentifier": {
                "keywordClassificationCode": "D033181",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D033181"
              }
            },
            {
              "keywordValue": "Exploratory Data Collection"
            },
            {
              "keywordValue": "Machine Learning",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000069550",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
              }
            },
            {
              "keywordValue": "Artificial Intelligence",
              "keywordIdentifier": {
                "keywordClassificationCode": "D001185",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
              }
            },
            {
              "keywordValue": "Electrocardiography",
              "keywordIdentifier": {
                "keywordClassificationCode": "D004562",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
              }
            },
            {
              "keywordValue": "Continuous Glucose Monitoring",
              "keywordIdentifier": {
                "keywordClassificationCode": "D000095583",
                "keywordScheme": "Medical Subject Headings (MeSH)",
                "schemeURI": "https://meshb.nlm.nih.gov/",
                "keywordURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
              }
            },
            {
              "keywordValue": "Retinal imaging"
            },
            {
              "keywordValue": "Eye exam"
            }
          ]
        },
        "designModule": {
          "studyType": "Observational",
          "isPatientRegistry": "No",
          "designInfo": {
            "designObservationalModelList": [
              "Cohort"
            ],
            "designTimePerspectiveList": [
              "Cross-sectional"
            ]
          },
          "bioSpec": {
            "bioSpecRetention": "Samples With DNA",
            "bioSpecDescription": "Participants who consent are asked to provide both blood and urine for research use.  There are three distinct elements that follow from these collections; first, a small amount of the collected blood is sent to the local collection site hospital lab for a complete blood cell count on the day of the encounter. Second, an additional portion of the blood and the urine sample are processed and stored at the collection site for batch shipment to the University of Washington Nutrition and Obesity Research Center for specialized lab tests.  Third, the remaining majority of the collected blood is processed into a variety of derivatives that are being used to establish a large repository of biospecimens at the University of Alabama at Birmingham to be used in research to further our understanding of diabetes, diabetic associated eye disease and other applications related to health and related diseases. These derivatives include isolated plasma, serum, DNA, buffy coats (white blood cells), blood stabilized for future isolation of RNA and peripheral blood mononuclear cells (PBMC). These derivatives are separated into small aliquots to facilitate future approved research test and are stored at either -80oC or at cryogenic temperatures (PBMC)."
          },
          "enrollmentInfo": {
            "enrollmentCount": "4000",
            "enrollmentType": "Anticipated"
          }
        },
        "armsInterventionsModule": {
          "armGroupList": [
            {
              "armGroupLabel": "Healthy",
              "armGroupDescription": "Participants who do not have Type 1 or Type 2 Diabetes",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Pre-diabetes/Lifestyle Controlled",
              "armGroupDescription": "Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifesyle adjustments",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Oral Medication and/or Non-insulin-injectable Medication Controlled",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            },
            {
              "armGroupLabel": "Insulin Dependent",
              "armGroupDescription": "Participants with Type 2 Diabetes whose blood sugar is controlled by insulin",
              "armGroupInterventionList": [
                "AI-READI protocol"
              ]
            }
          ],
          "interventionList": [
            {
              "interventionType": "Other",
              "interventionName": "AI-READI protocol",
              "interventionDescription": "Custom protocol followed for the AI-READI study. Details are available at in the documentation of v2.0.0 of the dataset at https://docs.aireadi.org/"
            }
          ]
        },
        "eligibilityModule": {
          "sex": "All",
          "genderBased": "No",
          "minimumAge": "40 Years",
          "maximumAge": "85 Years",
          "healthyVolunteers": "No",
          "eligibilityCriteria": {
            "eligibilityCriteriaInclusion": [
              "Adults (\u2265 40 years old)",
              "Patients with and without type 2 diabetes",
              "Able to provide consent",
              "Must be able to read and speak English"
            ],
            "eligibilityCriteriaExclusion": [
              "Adults older than 85 years of age",
              "Pregnancy",
              "Gestational diabetes",
              "Type 1 diabetes"
            ]
          },
          "studyPopulation": "Adult patients will be recruited into one of four groups: 1) healthy/no diabetes, 2) pre-diabetes/borderline diabetes/lifestyle-controlled diabetes, 3) oral medication and/or non-insulin injectable medication controlled type 2 diabetes, or 4) insulin dependent type 2 diabetes. The investigators aim to recruit approximately 1000 patients into each of the four groups. Patients will be recruited from University of Washington (UW), University of California at San Diego (UCSD), and University of Alabama at Birmingham (UAB). The study aims to recruit 1,000 subjects from each of the following racial and ethnic groups: White, Asian, Hispanic, and Black. Subjects will be age- and sex-matched within and between groups.",
          "samplingMethod": "Non-Probability Sample"
        },
        "contactsLocationsModule": {
          "centralContactList": [
            {
              "centralContactFirstName": "Aaron",
              "centralContactLastName": "Lee",
              "centralContactDegree": "MD",
              "centralContactIdentifier": [
                {
                  "centralContactIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "centralContactIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "centralContactAffiliation": {
                "centralContactAffiliationName": "Washington University in St. Louis",
                "centralContactAffiliationIdentifier": {
                  "centralContactAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "centralContactAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "centralContactEMail": "contact@aireadi.org"
            }
          ],
          "overallOfficialList": [
            {
              "overallOfficialFirstName": "Aaron",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7452-1648",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Washington University in St. Louis",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cecilia",
              "overallOfficialLastName": "Lee",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-1994-7213",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Washington University in St. Louis",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/01yc7t268",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Amir",
              "overallOfficialLastName": "Bahmani",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-4533-9334",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sally L.",
              "overallOfficialLastName": "Baxter",
              "overallOfficialDegree": "MD, MSc",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-5271-7690",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Christopher G.",
              "overallOfficialLastName": "Chute",
              "overallOfficialDegree": "MD, DrPH",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-5437-2545",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Jorge",
              "overallOfficialLastName": "Contreras",
              "overallOfficialDegree": "J.D.",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-7899-3060",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Utah",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/03r0ha626",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Nicholas",
              "overallOfficialLastName": "Evans",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-3330-0224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Massachusetts Lowell",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/03hamhx47",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Samantha",
              "overallOfficialLastName": "Hurst",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9843-2845",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "T. Y. Alvin",
              "overallOfficialLastName": "Liu",
              "overallOfficialDegree": "MD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-2957-0755",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Johns Hopkins University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00za53h95",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Gerald",
              "overallOfficialLastName": "McGwin",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-9592-1133",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Shannon",
              "overallOfficialLastName": "McWeeney",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0001-8333-6607",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Oregon Health & Science University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/009avj582",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Cynthia",
              "overallOfficialLastName": "Owsley",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-3424-011X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of Alabama at Birmingham",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/008s83205",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Bhavesh",
              "overallOfficialLastName": "Patel",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-0307-262X",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "California Medical Innovations Institute",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0156zyn36",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Michael",
              "overallOfficialLastName": "Snyder",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0003-0784-7987",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Sara J.",
              "overallOfficialLastName": "Singer",
              "overallOfficialDegree": "MBA, PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "http://orcid.org/0000-0002-3374-1177",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "Stanford University",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/00f54p054",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            },
            {
              "overallOfficialFirstName": "Linda M.",
              "overallOfficialLastName": "Zangwill",
              "overallOfficialDegree": "PhD",
              "overallOfficialIdentifier": [
                {
                  "overallOfficialIdentifierValue": "https://orcid.org/0000-0002-1143-5224",
                  "overallOfficialIdentifierScheme": "ORCID",
                  "schemeURI": "https://orcid.org"
                }
              ],
              "overallOfficialAffiliation": {
                "overallOfficialAffiliationName": "University of California, San Diego",
                "overallOfficialAffiliationIdentifier": {
                  "overallOfficialAffiliationIdentifierValue": "https://ror.org/0168r3w48",
                  "overallOfficialAffiliationIdentifierScheme": "ROR",
                  "schemeURI": "https://ror.org"
                }
              },
              "overallOfficialRole": "Study Principal Investigator"
            }
          ],
          "locationList": [
            {
              "locationFacility": "University of Alabama at Birmingham",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Birmingham",
              "locationZip": "35233",
              "locationState": "Alabama",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/008s83205",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of California, San Diego",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "San Diego",
              "locationZip": "92093",
              "locationState": "California",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/0168r3w48",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            },
            {
              "locationFacility": "University of Washington",
              "locationStatus": "Enrolling by invitation",
              "locationCity": "Seattle",
              "locationZip": "98109",
              "locationState": "Washington",
              "locationCountry": "United States",
              "locationIdentifier": {
                "locationIdentifierValue": "https://ror.org/00cvxb145",
                "locationIdentifierScheme": "ROR",
                "schemeURI": "https://ror.org/"
              }
            }
          ]
        }
      },
      "readme": "## Overview of the study\n\nThe Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) project seeks to create a flagship ethically-sourced dataset to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. The ability to understand and affect the course of complex, multi-organ diseases such as T2DM has been limited by a lack of well-designed, high quality, large, and inclusive multimodal datasets. The AI-READI team of investigators will aim to collect a cross-sectional dataset of 4,000 people and longitudinal data from 10% of the study cohort across the US. The study cohort will be balanced for self-reported race/ethnicity, gender, and diabetes disease stage. Data collection will be specifically designed to permit downstream pseudo-time manifold analysis, an approach used to predict disease trajectories by collecting and learning from complex, multimodal data from participants with differing disease severity (normal to insulin-dependent T2DM). The long-term objective for this project is to develop a foundational dataset in T2DM, agnostic to existing classification criteria or biases, which can be used to reconstruct a temporal atlas of T2DM development and reversal towards health (i.e., salutogenesis). Data will be optimized for downstream AI/ML research and made publicly available.\n\n## Description of the dataset\n\nThis dataset contains data from 2280 participants that was collected between July 19, 2023 and May 01, 2025. Data from multiple modalities are included. A full list is provided in the `Data Standards` section below. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed.\n\nThe dataset contains 356,343 files and is around 3.81 TB in size.\n\nA detailed description of the dataset is available in the AI-READI documentation for v3.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Protocol\n\nThe protocol followed for collecting the data can be found in the AI-READI documentation for v3.0.0 of the dataset at [docs.aireadi.org](https://docs.aireadi.org/).\n\n## Dataset access/restrictions\n\nAccessing the dataset requires several steps, including:\n\n- Login in through a verified ID system\n- Agreeing to use the data only for type 2 diabetes related research.\n- Agreeing to the license terms which set certain restrictions and obligations for data usage (see `License` section below).\n\n## Data standards followed\n\nThis dataset is organized following the [Clinical Dataset Structure (CDS) v0.1.1](https://cds-specification.readthedocs.io/en/v0.1.1/). We refer to the CDS documentation for more details. Briefly, data is organized at the root level into one directory per datatype (c.f. Table below). Within each datatype folder, there is one folder per modality. Within each modality folder, there is one folder per device used to collect that modality. Within each device folder, there is one folder per participant. Each datatype, modality, and device folder is named using a name that best defines it. Each participant folder is named after the participant's ID number used in the study. For each datatype, the data files follow the standards listed in the Table below. More details are available in the dataset_structure_description.json metadata file included in this dataset.\n\n| Datatype directory name   | Description                                                                                                                                                                                      | File format standard followed                                                                                                                                     |\n| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cardiac_ecg               | This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.      | [WaveForm DataBase (WFDB)](https://wfdb.readthedocs.io/en/latest/wfdb.html)                                                                                       |\n| clinical_data             | This directory contains clinical data collected through REDCap. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.                                                  | [Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi)                                               |\n| environment               | This directory contains data collected through an environmental sensor device custom built for the AI-READI project.                                                                             | [Earth Science Data Systems (ESDS) format](https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data) |\n| retinal_flio              | This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores. | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_oct               | This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.                                   | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_octa              | This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.                     | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| retinal_photography       | This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.                                                                          | [Digital Imaging and Communications in Medicine (DICOM)](http://medical.nema.org/)                                                                                |\n| wearable_activity_monitor | This directory contains data collected through a wearable fitness tracker.                                                                                                                       | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n| wearable_blood_glucose    | This directory contains data collected through a continuous glucose monitoring (CGM) device.                                                                                                     | [Open mHealth](https://www.openmhealth.org/documentation/#/schema-docs/schema-library)                                                                            |\n\n## Resources\n\nAll of our data files are in formats that are accessible with free software commonly used for such data types so no specific software is required. Some useful resources related to this dataset are listed below:\n\n- Documentation of the dataset: [docs.aireadi.org](https://docs.aireadi.org/) (see 'Dataset v3.0.0' for this version of the dataset)\n- AI-READI project website: [aireadi.org](https://aireadi.org/)\n- Zenodo community of the AI-READI project: [zenodo.org/communities/aireadi](https://zenodo.org/communities/aireadi)\n- GitHub organization of the AI-READI project: [github.com/AI-READI](https://github.com/AI-READI)\n\n### Suggested split\n\nThe suggested split for training, validating, and testing AI/ML models is included in the participants.tsv file that will be included in the dataset. A summary is provided in the table below.\n\n|                         | Train       |           |       |         | Val         |           |       |         | Test        |           |       |         | Total       |           |       |         |\n| ----------------------- |-------------|-----------|-------|---------|-------------|-----------|-------|---------|-------------|-----------|-------|---------|-------------|-----------|-------|---------|\n|                         | Hispanic    | Asian     | Black | White   | Hispanic    | Asian     | Black | White   | Hispanic    | Asian     | Black | White   | Hispanic    | Asian     | Black | White   |\n| Race/ethnicity (count)  | 204         | 369       | 343   | 660     | 88          | 88        | 88    | 88      | 88          | 88        | 88    | 88      | 380         | 545       | 519   | 836     |\n|                         | Male        | Female    |       |         | Male        | Female    |       |         | Male        | Female    |       |         | Male        | Female    |       |         |\n| Sex (count)             | 599         | 977       |       |         | 176         | 176       |       |         | 176         | 176       |       |         | 951         | 1329      |       |         |\n|                         | No DM       | Lifestyle | Oral  | Insulin | No DM       | Lifestyle | Oral  | Insulin | No DM       | Lifestyle | Oral  | Insulin | No DM       | Lifestyle | Oral  | Insulin |\n| Diabetes status (count) | 600         | 384       | 487   | 105     | 88          | 88        | 109   | 67      | 88          | 88        | 90    | 86      | 776         | 560       | 686   | 258     |\n| Mean age (years \u00b1 sd)   | 61.1 \u00b1 11.3 |           |       |         | 60.3 \u00b1 10.8 |           |       |         | 60.5 \u00b1 11.4 |           |       |         | 60.8 \u00b1 11.3 |           |       |         |\n| Total                   | 1576        |           |       |         | 352         |           |       |         | 352         |           |       |         | 2280        |           |       |         |\n\n- No DM: Participants who do not have Type 1 or Type 2 Diabetes\n- Lifestyle: Participants with pre-Type 2 Diabetes and those with Type 2 Diabetes whose blood sugar is controlled by lifestyle adjustments\n- Oral: Participants with Type 2 Diabetes whose blood sugar is controlled by oral or injectable medications other than insulin\n- Insulin: Participants with Type 2 Diabetes whose blood sugar is controlled by insulin\n\n### Changes between versions of the dataset\n\nChanges between the current version of the dataset and the previous one are provided in details in the CHANGELOG file included in the dataset (also visible at docs.aireadi.org). A summary of the major changes is provided in the table below.\n\n| Dataset      | v1.0.0 pilot    | year 2 data        | year 3 data       | v3.0.0 main study  |\n| ------------ | --------------- | ------------------ |-------------------|--------------------|\n| Participants | 204             | 863                | 1213              | 2280               |\n| Data types   | 15+ data types  | +1 image device    | +3 image device   | 15+ data types     |\n| Processing   | custom / ad hoc | automated + custom |automated + custom | automated + custom |\n| Release date | 5/3/2024        | included in v2.0.0 |included in v3.0.0 | 11/17/2025         |\n\n## License\n\nThis work is licensed under a custom license specifically tailored to enable the reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purposes while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications. More details are available in the License file included in the dataset and also available at https://doi.org/10.5281/zenodo.17555036.\n\n## How to cite\n\nIf you use this dataset for any purpose, please cite the resources specified in the AI-READI documentation for version 3.0.0 of the dataset at https://docs.aireadi.org.\n\n## Contact\n\nFor any questions, suggestions, or feedback related to this dataset, please go to https://aireadi.org/contact. We refer to the study_description.json and dataset_description.json metadata files included in this dataset for additional information about the contact person/entity, authors, and contributors of the dataset.\n\n## Acknowledgement\n\nThe AI-READI project is supported by NIH grant [1OT2OD032644](https://reporter.nih.gov/search/1ADgncihCk6fdMRJdCnBjg/project-details/10471118) through the NIH Bridge2AI Common Fund program.",
      "datasetDescription": {
        "schema": "https://schema.aireadi.org/v0.1.0/dataset_description.json",
        "identifier": {
          "identifierValue": "10.60775/fairhub.3",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Flagship Dataset of Type 2 Diabetes from the AI-READI Project"
          },
          {
            "titleValue": "AI-READI dataset",
            "titleType": "AlternativeTitle"
          }
        ],
        "version": "3.0.0",
        "creator": [
          {
            "creatorName": "AI-READI Consortium",
            "nameType": "Organizational"
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-11-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on FAIRhub"
          },
          {
            "dateValue": "2023-07-19/2025-05-01",
            "dateType": "Collected",
            "dateInformation": "Period when the data was collected"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Type 2 Diabetes",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": true,
          "deIdentHIPAA": true,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No identifiers were collected so no active de-identification was necessary but we checked that no identifiable data per US HIPAA were present in the data."
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": "The public version of the dataset can only be used for type 2 diabetes related research. A private version will allow for more generic use."
        },
        "description": [
          {
            "descriptionValue": "This dataset contains data from 2280 participants that was collected between from July 19, 2023 and May 01, 2025. Data from multiple modalities are included. The data in this dataset contain no protected health information (PHI). Information related to the sex and race/ethnicity of the participants as well as medication used has also been removed. A detailed description of the dataset is available in the AI-READI documentation for v3.0.0 of the dataset at https://docs.aireadi.org",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [
          {
            "relatedIdentifierValue": "https://docs.aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          },
          {
            "relatedIdentifierValue": "https://aireadi.org/",
            "relatedIdentifierType": "URL",
            "relationType": "IsDocumentedBy",
            "resourceTypeGeneral": "Other"
          }
        ],
        "subject": [
          {
            "subjectValue": "Diabetes mellitus",
            "subjectIdentifier": {
              "classificationCode": "45636-8",
              "subjectScheme": "Logical Observation Identifier Names and Codes (LOINC)",
              "schemeURI": "https://loinc.org/",
              "valueURI": "https://loinc.org/45636-8"
            }
          },
          {
            "subjectValue": "Machine Learning",
            "subjectIdentifier": {
              "classificationCode": "D000069550",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000069550"
            }
          },
          {
            "subjectValue": "Artificial Intelligence",
            "subjectIdentifier": {
              "classificationCode": "D001185",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D001185"
            }
          },
          {
            "subjectValue": "Electrocardiography",
            "subjectIdentifier": {
              "classificationCode": "D004562",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
            }
          },
          {
            "subjectValue": "Continuous Glucose Monitoring",
            "subjectIdentifier": {
              "classificationCode": "D000095583",
              "subjectScheme": "Medical Subject Headings (MeSH)",
              "schemeURI": "https://meshb.nlm.nih.gov/",
              "valueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D000095583"
            }
          },
          {
            "subjectValue": "Retinal imaging"
          },
          {
            "subjectValue": "Eye exam"
          }
        ],
        "managingOrganization": {
          "name": "Washington University in St. Louis",
          "managingOrganizationIdentifier": {
            "managingOrganizationIdentifierValue": "https://ror.org/01yc7t268",
            "managingOrganizationScheme": "ROR",
            "schemeURI": "https://ror.org"
          }
        },
        "accessType": "PublicDownloadSelfAttestationRequired",
        "accessDetails": {
          "description": "Accessing the dataset requires several steps, including: Login in through a verified ID system, Agreeing to use the data only for type 2 diabetes related research, Agreeing to the license terms which set certain restrictions and obligations for data usage (see 'rights' property)"
        },
        "rights": [
          {
            "rightsName": "AI-READI custom license v2.0",
            "rightsURI": "https://doi.org/10.5281/zenodo.17555036"
          }
        ],
        "publisher": {
          "publisherName": "FAIRhub"
        },
        "size": [
          "3.81 TB",
          "356343 files"
        ],
        "fundingReference": [
          {
            "funderName": "National Institutes of Health",
            "funderIdentifier": {
              "funderIdentifierValue": "https://ror.org/01cwqze88",
              "funderIdentifierType": "ROR",
              "schemeURI": "https://ror.org"
            },
            "awardNumber": {
              "awardNumberValue": "OT2OD032644",
              "awardURI": "https://reporter.nih.gov/search/yatARMM-qUyKAhnQgsCTAQ/project-details/10885481"
            },
            "awardTitle": "Bridge2AI: Salutogenesis Data Generation Project"
          }
        ],
        "format": [
          "application/dicom",
          "text/markdown",
          "text/csv",
          "application/json"
        ]
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [
          {
            "directoryName": "cardiac_ecg",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains electrocardiogram data collected by a 12 lead protocol (the current standard), Holter monitor, or smartwatch. The terms ECG and EKG are often used interchangeably.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Electrocardiogram",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C168186",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C168186"
                  }
                ]
              },
              {
                "relatedTermValue": "Electrocardiography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D004562",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D004562"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "WaveForm DataBase (WFDB)",
                "standardDescription": "Set of file standards designed for reading and storing physiologic signal data, and associated annotations.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://wfdb.readthedocs.io/en/latest/wfdb.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "ecg_12lead",
                "directoryType": "modality",
                "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Electrocardiography",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other",
                    "relatedIdentifierDescription": "This page describes 3 types of ECG protocol including the 12 lead (standard) protocol."
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "philips_tc30",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains ECG data collected using the 12 lead protocol using the Philips PageWriter TC30 device.",
                    "relatedIdentifier": [
                      {
                        "relatedIdentifierValue": "https://www.documents.philips.com/doclib/enc/fetch/2000/4504/577242/577243/577246/581601/711562/DXL_ECG_Algorithm_Physician_s_Guide_(ENG)_Ed.2.pdf",
                        "relatedIdentifierType": "URL",
                        "relationType": "IsDocumentedBy",
                        "resourceTypeGeneral": "Other",
                        "relatedIdentifierDescription": "The 'Philips DXL ECG Algorithm Physician\u2019s Guide' contains information on the fields that are printed on an ECG report."
                      }
                    ]
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS) v0.1.1",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 302931703,
            "numberOfFiles": 4515
          },
          {
            "directoryName": "clinical_data",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains clinical data collected through REDCap, including blood/urine lab values and survey data. Each CSV file in this directory is a one-to-one mapping to the OMOP CDM tables.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Clinical Data",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C15783",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C15783"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory is named following the specification of this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "The Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)",
                "standardDescription": "Standard designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence.",
                "standardUse": "All the data files within this directory follow this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.qk984b",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "dqd_omop.json",
                "metadataFileDescription": "The dqd_omop.json file supports OMOP CDM data quality analysis using the OMOP CDM Data Quality Dashboard (DQD). The OMOP CDM DQD tool (https://ohdsi.github.io/DataQualityDashboard/) runs a set of > 3500 data quality checks against an OMOP CDM instance",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ohdsi.github.io/DataQualityDashboard/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 176182781,
            "numberOfFiles": 7
          },
          {
            "directoryName": "environment",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through an environmental sensor device custom built for the AI-READI project.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Environmental sensor data"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "ASCII File Format Guidelines for Earth Science Data",
                "standardDescription": "NASA recommended practices for formatting and describing ASCII encoded data files",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.earthdata.nasa.gov/esdis/esco/standards-and-practices/ascii-file-format-guidelines-for-earth-science-data",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "environmental_sensor",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "leelab_anura",
                    "directoryDescription": "You can learn more about the data in this directory by consulting relevant documentation. Search 'AS7431' with Type set as 'Datasheet' at https://ams-osram.com/support/download-center. Search 'DS3231 Precision RTC' at https://www.adafruit.com, look for product ID 5188, select this link and scroll down to Technical Details to find a link for the Datasheet (as of this writing, you may find the datasheet at https://www.analog.com/media/en/technical-documentation/data-sheets/DS3231.pdf). Search 'Datasheet SEN5x' in the search box at https://sensirion.com/ to get the document titled 'Datasheet SEN5x' (the name of the downloaded file may be 'Sensirion_Datasheet_Environmental_Node_SEN5x.pdf'). Search 'NOx Index' in the search box at https://sensirion.com to get the document titled 'What is Sensirion's NOx Index?' (the name of the downloaded file may be 'Info_Note NOx_Index.pdf').",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 55625676514,
            "numberOfFiles": 2232
          },
          {
            "directoryName": "retinal_flio",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through fluorescence lifetime imaging ophthalmoscopy (FLIO), an imaging modality for in vivo measurement of lifetimes of endogenous retinal fluorophores.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Fuorescence Lifetime Imaging Ophthalmoscopy"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "flio",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_flio",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1069466876718,
            "numberOfFiles": 7969
          },
          {
            "directoryName": "retinal_oct",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography (OCT), an imaging method using lasers that is used for mapping subsurface structure.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C20828",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C20828"
                  }
                ]
              },
              {
                "relatedTermValue": "Tomography, Optical Coherence",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "D041623",
                    "relatedTermScheme": "Medical Subject Headings (MeSH)",
                    "relatedTermSchemeURI": "https://meshb.nlm.nih.gov/",
                    "relatedTermValueURI": "https://meshb.nlm.nih.gov/record/ui?ui=D041623"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "structural_oct",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCT data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1317625293027,
            "numberOfFiles": 56478
          },
          {
            "directoryName": "retinal_octa",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected using optical coherence tomography angiography (OCTA), a non-invasive imaging technique that generates volumetric angiography images.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography"
              },
              {
                "relatedTermValue": "Optical Coherence Tomography Angiography Images"
              },
              {
                "relatedTermValue": "OCTA Images"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "enface",
                "directoryType": "modality",
                "directoryDescription": "This directory contains en face data collected using OCTA. En face images, derived from 3D volume scans, are also referred to as C-scan OCT. These images provide a 2D view of the retina layers and have an orientation siimilar to fundus photographs.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains OCTA en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "flow_cube",
                "directoryType": "modality",
                "directoryDescription": "This directory contains flow cube data collected using OCTA. Flow cube provides information on blood flow in a 3D view.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Maestro2 device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube data collected from the Triton device, manufacture by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains flow cube en face data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              },
              {
                "directoryName": "segmentation",
                "directoryType": "modality",
                "directoryDescription": "This directory contains segmentation data collected using OCTA. The segmentation information is presented in the form of heightmaps and includes information about the associated layers.",
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Maestro device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Triton device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains segmentation data collected from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 1155908809724,
            "numberOfFiles": 173721
          },
          {
            "directoryName": "retinal_photography",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains retinal photography data, which are 2D images. They are also referred to as fundus photography.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              },
              {
                "relatedIdentifierValue": "https://en.wikipedia.org/wiki/Fundus_photography",
                "relatedIdentifierType": "URL",
                "relationType": "IsDescribedBy",
                "resourceTypeGeneral": "Other"
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Eye Fundus Photography",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C147467",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C147467"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Digital Imaging and Communications in Medicine (DICOM)",
                "standardDescription": "Standard for the digital storage and transmission of medical images and related information.",
                "standardUse": "All the data files within this directory follow the format specified in this standard.",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.b7z8by",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "http://medical.nema.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "cfp",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://ophthalmology.med.ubc.ca/patient-care/ophthalmic-photography/color-fundus-photography/#:~:text=Color%20Fundus%20Retinal%20Photography%20uses,monitor%20their%20change%20over%20time",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "optomed_aurora",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Aurora device, manufactured by Optomed."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "topcon_triton",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains retinal photography data, specifically color fundus photographs from the Triton device, manufactured by Topcon."
                  }
                ]
              },
              {
                "directoryName": "faf",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data, specificallly fundus autofluorescence photographs that uses the fluorescent characteristics of lipofuscin in an non-invasive way.",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://eyewiki.aao.org/Fundus_Autofluorescence",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains fundus autofluorescence photographs from the Eidon device, manufactured by iCare."
                  }
                ]
              },
              {
                "directoryName": "ir",
                "directoryType": "modality",
                "directoryDescription": "This directory contains retinal photography data using near-infrared reflectance (IR).",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8349282/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ],
                "directoryList": [
                  {
                    "directoryName": "heidelberg_spectralis",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Spectralis device, manufactured by Heidelberg."
                  },
                  {
                    "directoryName": "icare_eidon",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Eidon device, manufactured by iCare."
                  },
                  {
                    "directoryName": "topcon_maestro2",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Maestro2 device, manufactured by Topcon."
                  },
                  {
                    "directoryName": "zeiss_cirrus",
                    "directoryType": "device",
                    "directoryDescription": "This directory contains IR images, specifically from the Cirrus device, manufactured by Zeiss."
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 174381046406,
            "numberOfFiles": 93921
          },
          {
            "directoryName": "wearable_activity_monitor",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a wearable fitness tracker.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "smartwatch"
              },
              {
                "relatedTermValue": "activity monitoring"
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "heart_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "oxygen_saturation",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "physical_activity_calorie",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "respiratory_rate",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "sleep",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              },
              {
                "directoryName": "stress",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "garmin_vivosmart5",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 38313536220,
            "numberOfFiles": 15245
          },
          {
            "directoryName": "wearable_blood_glucose",
            "directoryType": "dataType",
            "directoryDescription": "This directory contains data collected through a continuous glucose monitoring (CGM) device.",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://docs.aireadi.org",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other",
                "relatedIdentifierDescription": "This is the documentation of the AI-READI dataset that contains additional information about the data contained in this directory."
              }
            ],
            "relatedTerm": [
              {
                "relatedTermValue": "Continuous Glucose Monitoring System",
                "relatedTermIdentifier": [
                  {
                    "relatedTermClassificationCode": "C159776",
                    "relatedTermScheme": "NCI Thesaurus (NCIT)",
                    "relatedTermSchemeURI": "https://ncim.nci.nih.gov/",
                    "relatedTermValueURI": "https://ncit.nci.nih.gov/ncitbrowser/pages/concept_details.jsf?dictionary=NCI%20Thesaurus&code=C159776"
                  }
                ]
              }
            ],
            "relatedStandard": [
              {
                "standardName": "Clinical Data Structure (CDS) v0.1.1",
                "standardDescription": "Standard for consistently structuring and describing clinical research datasets",
                "standardUse": "This directory and its sub-directories are named and organized following the specification from this standard.",
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              },
              {
                "standardName": "Open mHealth",
                "standardDescription": "Open Standard for Mobile Health Data (Open mHealth) is the leading mobile health data interoperability standard.",
                "standardUse": "All the data files within this directory follow the format specified in this standard",
                "standardIdentifier": [
                  {
                    "identifierValue": "https://doi.org/10.25504/FAIRsharing.mrpMBj",
                    "identifierType": "DOI"
                  }
                ],
                "standardRelatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://www.openmhealth.org/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDescribedBy"
                  }
                ]
              }
            ],
            "directoryList": [
              {
                "directoryName": "continuous_glucose_monitoring",
                "directoryType": "modality",
                "directoryList": [
                  {
                    "directoryName": "dexcom_g6",
                    "directoryType": "device"
                  }
                ]
              }
            ],
            "metadataFileList": [
              {
                "metadataFileName": "manifest.tsv",
                "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
                "relatedIdentifier": [
                  {
                    "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                    "relatedIdentifierType": "URL",
                    "relationType": "IsDocumentedBy",
                    "resourceTypeGeneral": "Other"
                  }
                ]
              }
            ],
            "size": 4169006971,
            "numberOfFiles": 2246
          }
        ],
        "metadataFileList": [
          {
            "metadataFileName": "CHANGELOG.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "dataset_structure_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "healthsheet.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "LICENSE.txt",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "participants.tsv",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "README.md",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          },
          {
            "metadataFileName": "study_description.json",
            "metadataFileDescription": "This is a metadata file based on the Clinical Dataset Structure (CDS)",
            "relatedIdentifier": [
              {
                "relatedIdentifierValue": "https://cds-specification.readthedocs.io/en/v0.1.1/",
                "relatedIdentifierType": "URL",
                "relationType": "IsDocumentedBy",
                "resourceTypeGeneral": "Other"
              }
            ]
          }
        ]
      },
      "healthsheet": {
        "generalInformation": [
          {
            "id": 1,
            "question": "Provide a 2 sentence summary of this dataset.",
            "response": "The Artificial Intelligence Ready and Exploratory Atlas for Diabetes Insights (AI-READI) is a dataset consisting of data collected from individuals with and without \u201cType 2 Diabetes Mellitus (T2DM)\u201d and harmonized across 3 data collection sites. The composition of the dataset was designed with future studies using AI/Machine Learning in mind. This included recruitment sampling procedures aimed at achieving approximately equal distribution of participants across sex, race, and diabetes severity, as well as the design of a data acquisition protocol across multiple domains (survey data, physical measurements, clinical data, imaging data, wearable device data, etc.) to enable downstream AI/ML analyses that may not be feasible with existing data sources such as claims or electronic health records data. The goal is to better understand salutogenesis (the pathway from disease to health) in T2DM. Some data that are not considered to be sensitive personal health data will be available to the public for download upon agreement with a license that defines how the data can be used. The full dataset will be accessible by entering into a data use agreement. The public dataset will include survey data, blood and urine lab results, fitness activity levels, clinical measurements (e.g. monofilament and cognitive function testing), retinal images, ECG, blood sugar levels, and home air quality. The data held under controlled access include 5-digit zip code, sex, race, ethnicity, genetic sequencing data, past health records, and traffic and accident reports. Of note, the overall enrollment goal is to have balanced distribution between different racial groups. As enrollment is ongoing, the periodic updates to data releases may not have achieved balanced distribution across groups."
          },
          {
            "id": 2,
            "question": "Has the dataset been audited before? If yes, by whom and what are the results?",
            "response": "The dataset has not undergone any formal external audits. However, the dataset has been reviewed internally by AI-READI team members for quality checks and to ensure that no personally identifiable information was accidentally included."
          }
        ],
        "versioning": [
          {
            "id": 1,
            "question": "Does the dataset get released as static versions or is it dynamically updated?\n\n   a. If static, how many versions of the dataset exist?\n\n   b. If dynamic, how frequently is the dataset updated?",
            "response": "The dataset gets released as static versions. This is the third version of the dataset and consists of data collected up through the end of the second year of the study, i.e. between July 19, 2023 and May 1st, 2025. There are plans to release new versions of the dataset approximately once a year with additional data from participants who have been enrolled since the last dataset version release."
          },
          {
            "id": 2,
            "question": "Is this datasheet created for the original version of the dataset? If not, which version of the dataset is this datasheet for?",
            "response": "This datasheet is created for the third version of the dataset."
          },
          {
            "id": 3,
            "question": "Are there any datasheets created for any versions of this dataset?",
            "response": "There was a previous datasheet created for the first version of the dataset, which consisted of data collected during the pilot data collection phase. It is available here: https://docs.aireadi.org/docs/1/dataset/healthsheet. \n\nThere was also a previous datasheet created for the second version of the dataset, which consisted of data collected up to the end of the first full year of the study. It is available here: https://docs.aireadi.org/docs/2/dataset/healthsheet"
          },
          {
            "id": 4,
            "question": "Does the current version/subversion of the dataset come with predefined task(s), labels, and recommended data splits (e.g., for training, development/validation, testing)? If yes, please provide a high-level description of the introduced tasks, data splits, and labeling, and explain the rationale behind them. Please provide the related links and references. If not, is there any resource (website, portal, etc.) to keep track of all defined tasks and/or associated label definitions? (please note that more detailed questions w.r.t labeling is provided in further sections)",
            "response": "See response to question #6 under \u201cLabeling and subjectivity of labeling\u201d."
          },
          {
            "id": 5,
            "question": "If the dataset has multiple versions, and this datasheet represents one of them, answer the following questions:\n\n   a. What are the characteristics that have been changed between different versions of the dataset? ",
            "response": "This version of the dataset includes more patients than the first version of the dataset."
          },
          {
            "id": 6,
            "question": "b. Explain the motivation/rationale for creating the current version of the dataset.",
            "response": "The current version of the dataset includes data from the second year of the study, rather than only the data collected from the pilot data collection phase (which comprised the first version of the dataset) and the first year of the study (which comprised the second version of the dataset)."
          },
          {
            "id": 7,
            "question": "c. Does this version have more subjects/patients represented in the data, or fewer?",
            "response": "This version has more subjects/patients represented in the data (the first version of the dataset contains data from 204 participants, the second version contains additional data from 863 participants for a total of 1067 participants, and this version contains a total of 2280 participants)."
          },
          {
            "id": 8,
            "question": "d. Does this version of the dataset have extended data or new data from the same patients as the older versions? Were any patients, data fields, or data points removed? If so, why?",
            "response": "The data fields/types are largely the same as the prior version of the dataset. However, the Snellen visual acuity variables were dropped in this version of the dataset, such that now only logMAR visual acuity measurements are available."
          },
          {
            "id": 9,
            "question": "e. Do we expect more versions of the dataset to be released?",
            "response": "Yes, enrollment is ongoing, and future versions of the dataset will be released that will include larger numbers of subjects/patients as enrollment increases."
          },
          {
            "id": 10,
            "question": "f. Is this datasheet for a version of the dataset? If yes, does this sub-version of the dataset introduce a new task, labeling, and/or recommended data splits? If the answer to any of these questions is yes, explain the rationale behind it.",
            "response": "This datasheet does not include new tasks or labeling. It does include a new recommended data split for the 2280 participants balancing training, validation, and testing sets for age, sex, races/ethnicities, and study group."
          },
          {
            "id": 11,
            "question": "g. Are you aware of any widespread version(s)/subversion(s) of the dataset? If yes, what is the addressed task, or application that is addressed?",
            "response": "No"
          }
        ],
        "motivation": [
          {
            "id": 1,
            "question": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.",
            "response": "The purpose for creating the dataset was to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. T2DM is a growing public health threat. Yet, the current understanding of T2DM, especially in the context of salutogenesis, is limited. Given the complexity of T2DM, AI-based approaches may help with improving our understanding but a key issue is the lack of data ready for training AI models. The AI-READI dataset is intended to fill this gap."
          },
          {
            "id": 2,
            "question": "What are the applications that the dataset is meant to address? (e.g., administrative applications, software applications, research)",
            "response": "The multi-modal dataset being collected is being gathered to facilitate downstream pseudotime manifolds and various applications in artificial intelligence."
          },
          {
            "id": 3,
            "question": "Are there any types of usage or applications that are discouraged from using this dataset? If so, why?",
            "response": "The AI READI dataset License imposes certain restrictions on the usage of the data. The restrictions are described in the License files available at https://doi.org/10.5281/zenodo.17555036."
          },
          {
            "id": 4,
            "question": "Who created this dataset (e.g., which team, research group), and on behalf of which entity (e.g., company, institution, organization)?",
            "response": "This dataset was created by members of the AI-READI project, hereby referred to as the AI-READI Consortium. Details about each member and their institutions are available on the project website at https://aireadi.org."
          },
          {
            "id": 5,
            "question": "Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. If the funding institution differs from the research organization creating and managing the dataset, please state how.",
            "response": "The creation of the dataset was funded by the National Institutes of Health (NIH) through their Bridge2AI Program (https://commonfund.nih.gov/bridge2ai). The grant number is OT2ODO32644 and more information about the funding is available at https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481. Note that the funding institution is not creating or managing the dataset. The dataset is created and managed by the awardees of the grant (c.f. answer to the previous question)."
          },
          {
            "id": 6,
            "question": "What is the distribution of backgrounds and experience/expertise of the dataset curators/generators?",
            "response": "There is a wide range of experience within the project team, including senior, mid-career, and early career faculty members as well as clinical research coordinators, staff, and interns. They collectively cover many areas of expertise including clinical research, data collection, data management, data standards, bioinformatics, team science, and ethics, among others. Visit https://aireadi.org/team for more information."
          }
        ],
        "composition": [
          {
            "id": 1,
            "question": "What do the instances that comprise the dataset represent (e.g., documents, images, people, countries)? Are there multiple types of instances? Please provide a description.",
            "response": "Each instance represents an individual patient."
          },
          {
            "id": 2,
            "question": "How many instances are there in total (of each type, if appropriate) (breakdown based on schema, provide data stats)?",
            "response": "There are 2280 instances in this current version of the dataset (version 3, released fall 2025)."
          },
          {
            "id": 3,
            "question": "How many patients / subjects does this dataset represent? Answer this for both the preliminary dataset and the current version of the dataset.",
            "response": "This version of the dataset has data from 2280 participants. The first version of the dataset, composed of data from the pilot data collection phase, had 204 instances. The second version of the dataset, composed of data from the first year of the study, had 1067 instances."
          },
          {
            "id": 4,
            "question": "Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). Answer this question for the preliminary version and the current version of the dataset in question.",
            "response": "The dataset contains all possible instances. More specifically, the dataset contains data from all participants who have been enrolled during the first year of data collection for AI-READI."
          },
          {
            "id": 5,
            "question": "What data modality does each patient data consist of? If the data is hierarchical, provide the modality details for all levels (e.g: text, image, physiological signal). Break down in all levels and specify the modalities and devices.",
            "response": "Multiple modalities of data are collected for each participant, including survey data, clinical data, retinal imaging data, environmental sensor data, continuous glucose monitor data, and wearable activity monitor data. These encompass tabular data, imaging data, and physiological signal/waveform data. There is no unstructured text data included in this dataset. The exact forms used for data collection in REDCap are available [here](https://docs.aireadi.org/v3/REDCap%20surveys%20and%20forms.pdf). Furthermore, all modalities, file formats, and devices are detailed in the dataset documentation at https://docs.aireadi.org/."
          },
          {
            "id": 6,
            "question": "What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.",
            "response": "Each instance consists of all of the data available for an individual participating in the study. See answer to question 5 for the data types associated with each instance."
          },
          {
            "id": 7,
            "question": "Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).?",
            "response": "Yes, not all modalities are available for all participants. Some participants elected not to participate in some study elements. In a few cases, the data collection device did not have any stored results or was returned too late to retrieve the results (e.g. battery died, data was lost). In a few cases, there may have been a data collision at some point in the process and data has been lost."
          },
          {
            "id": 8,
            "question": "Are relationships between individual instances made explicit? (e.g., They are all part of the same clinical trial, or a patient has multiple hospital visits and each visit is one instance)? If so, please describe how these relationships are made explicit.",
            "response": "Yes - all instances are part of the same prospective data generation project (AI-READI). There is currently only one visit per participant."
          },
          {
            "id": 9,
            "question": "Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. (e.g., losing data due to battery failure, or in survey data subjects skip the question, radiological sources of noise).",
            "response": "In cases of survey data, skipped questions or incomplete responses are expected. In cases of using wearables, improper use, technical failure such as battery failure or system malfunction are expected. In cases of imaging data, patient uncooperation, noise that may obscure the images and technical failure such as system malfunction, and data transfer failures are expected."
          },
          {
            "id": 10,
            "question": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, other datasets)? If it links to or relies on external resources,\n\n a. are there guarantees that they will exist, and remain constant, over time;\n\n b. are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created);\n\n c. are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.",
            "response": "The dataset is self-contained but does rely on the dataset documentation for users requiring additional information about the provenance of the dataset. The documentation is available at https://docs.aireadi.org. The documentation is shared under the CC-BY 4.0 license, so there are no restrictions associated with its use."
          },
          {
            "id": 11,
            "question": "Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications that is confidential)? If so, please provide a description.",
            "response": "No, the dataset does not contain data that might be considered confidential. No personally identifiable information is included in the dataset."
          },
          {
            "id": 12,
            "question": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise pose any safety risk (such as psychological safety and anxiety)? If so, please describe why.",
            "response": "No."
          },
          {
            "id": 13,
            "question": "If the dataset has been de-identified, were any measures taken to avoid the re-identification of individuals? Examples of such measures: removing patients with rare pathologies or shifting time stamps.",
            "response": ""
          },
          {
            "id": 14,
            "question": "Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.",
            "response": "No, the public dataset will not contain data that is considered sensitive. However, the controlled access dataset will contain data regarding racial and ethnic origins, location (5-digit zip code), as well as motor vehicle accident reports."
          }
        ],
        "devices": [
          {
            "id": 1,
            "question": "For data that requires a device or equipment for collection or the context of the experiment, answer the following additional questions or provide relevant information based on the device or context that is used (for example)\n\n   a. If there was an MRI machine used, what is the MRI machine and model used?\n\n   b. If heart rate was measured what is the device for heart rate variation that is used?\n\n   c. If cortisol measurement is reported at multi site, provide details,\n\n   d. If smartphones were used to collect the data, provide the names of models.\n\n   e. And so on,..",
            "response": "The devices included in the study are as follows, and more details can be found at [https://docs.aireadi.org](https://docs.aireadi.org):\n\n   **Environmental sensor device**\n\n   Participants will be sent home with an environmental sensor (a custom-designed sensor unit called the LeeLab Anura), which they will use for 10 continuous days before returning the devices to the clinical research coordinators for data download.\n\n   **Continuous glucose monitor (Dexcom G6)**\n\n   The Dexcom G6 is a real-time, integrated continuous glucose monitoring system (iCGM) that directly monitors blood glucose levels without requiring finger sticks. It must be worn continuously in order to collect data.\n\n   **Wearable accelerometer (Physical activity monitor)**\n\n   The Garmin Vivosmart 5 Fitness Activity tracker will be used to measure data related to physical activity.\n\n   **Heart rate**\n\n   Heart rate can be read from EKG or blood pressure measurement devices.\n\n   **Blood pressure**\n\n   Blood pressure devices used for the study across the various data acquisition sites are: OMRON HEM 907XL Blood Pressure Monitor, Medline MDS4001 Automatic Digital Blood Pressure Monitor, and Welch Allyn 6000 series Vital signs monitor with Welch Allyn FlexiPort Reusable Blood Pressure Cuff.\n\n   **Visual acuity**\n\n   M&S Technologies EVA device to test visual acuity. The test is administered at a distance of 4 meters from a touch-screen monitor that is 12x20 inches. Participants will read letters from the screen. Photopic Conditions: No neutral density filters are used. A general occluder will be used for photopic testing. The participant wears their own prescription spectacles or trial frames. For Mesopic conditions, a neutral density (ND) filter will be used. The ND filter will either be a lens added to trial frames to reduce incoming light on the tested eye, OR a handheld occluder with a neutral density\n   filter (which we will designate as \u201cND-occluder) over the glasses will be used. The ND-occluder is different from a standard occluder and is used only for vision testing under mesopic conditions.\n\n   **Contrast sensitivity**\n\n   The MARS Letter Contrast Sensitivity test (Perceptrix) was conducted monocularly under both Photopic conditions (with a general occluder) and Mesopic conditions (using a Neutral Density occluder with a low luminance filter lens). The standardized order of MARS cards was as follows: Photopic OD, Photopic OS, Mesopic OD, and Mesopic OS. The background luminance of the charts fell within the range of 60 to 120 cd/m2, with an optimal level of 85 cd/m2. Illuminance was recommended to be between 189 to 377 lux, with an optimal level of 267 lux. While the designed viewing distance was 50 cm, it could vary between 40 to 59 cm. Patients were required to wear their appropriate near correction: reading glasses or trial frames with +2.00D lenses. All testing was carried out under undilated conditions. Patients were instructed to read the letter left to right across each line on the chart. Patients were encouraged to guess, even if they perceived the letters as too faint. Testing was terminated either when the patient made two consecutive errors or reached the end of the chart. The log contrast sensitivity (log CS) values were recorded by multiplying the number of errors prior to the final correct letter by 0.04 and subtracting the result from the log CS value at the final correct letter. If a patient reached the end of the chart without making two consecutive errors, the final correct letter was simply the last one correctly identified.\n\n   **Autorefraction**\n\n   KR 800 Auto Keratometer/Refractor.\n\n   **EKG**\n\n   Philips (manufacturer of Pagewriter TC30 Cardiograph)\n\n   **Lensometer**\n\n   Lensometer devices used at data acquisition sites across the study include: NIDEK LM-600P Auto Lensometer, Topcon-CL-200 computerized Lensometer, and Topcon-CL-300 computerized Lensometer\n\n   **Undilated fundus photography - Optomed Aurora**\n\n   The Optomed Aurora IQ is a handheld fundus camera that can take non-mydriatic images of the ocular fundus. It has a 50\u00b0 field of view, 5 Mpix sensor, and high-contrast optical design. The camera is non-mydriatic, meaning it doesn't require the pupil to be dilated, so it can be used for detailed viewing of the retina. Images taken during the AI-READI visit, are undilated images taken in a dark room while a patient is sitting on a comfortable chair, laying back. As it becomes challenging to get a good view because of the patients not being dilated and the handheld nature of this imaging modality, the quality of the images vary from patient to patient and within the same patient.\n\n   **Dilated fundus photography - Eidon**\n\n   The iCare EIDON is a widefield TrueColor confocal fundus imaging system that can capture images up to 200\u00b0. It comes with multiple imaging modalities, including TrueColor, blue, red, Red-Free, and infrared confocal images. The system offers widefield, ultra-high-resolution imaging and the capability to image through cataract and media opacities. It operates without dilation (minimum pupil 2.5 mm) and provides the flexibility of both fully automated and fully manual modes. Additionally, the iCare EIDON features an all-in-one compact design, eliminating the need for an additional PC. AI READI images using EIDON include two main modalities: 1. Single Field Central IR/FAF 2. Smart Horizontal Mosaic. Imaging is done in fully automated mode in a dark room with the machine moving and positioning according to the patient's head aiming at optimizing the view and minimizing operator's involvement/operator induced noise.\n\n   **Spectralis HRA (Heidelberg Engineering)**\n\n   The Heidelberg Spectralis HRA+OCT is an ophthalmic imaging system that combines optical coherence tomography (OCT) with retinal angiography. It is a modular, upgradable platform that allows clinicians to configure it for their specific diagnostic workflow. It has the confocal scanning laser ophthalmoscope (cSLO) technology that not only offers documentation of clinical findings but also often highlights critical diagnostic details that are not visible on traditional clinical ophthalmoscopy. Since cSLO imaging minimizes the effects of light scatter, it can be used effectively even in patients with cataracts. For AI READI subjects, imaging is done in a dark room using the following modalities: ONH-RC, PPole-H, and OCTA of the macula. As the machine is operated by the imaging team and is not fully automated, quality issues may arise, which may lead to skipping this modality and missing data.\n\n   **Triton DRI OCT (Topcon Healthcare)**\n\n   The DRI OCT Triton is a device from Topcon Healthcare that combines swept-source OCT technology with multimodal fundus imaging. The DRI OCT Triton uses swept-source technology to visualize the deepest layers of the eye, including through cataracts. It also enhances visualization of outer retinal structures and deep pathologies. The DRI OCT Triton has a 1,050 nm wavelength light source and a non-mydriatic color fundus camera. AI READI imaging is done in a dark room with minimal intervention from the imager as the machine positioning is done automatically. This leads to higher quality images with minimal operator induced error. Imaging is done in 12.0X12.0 mm and 6.0X6.0 mm OCTA, and 12.0 mm X9.0 mmX6.0 mm 3D Horizontal and Radial scan modes.\n\n   **Maestro2 3D OCT (Topcon Healthcare)**\n\n   The Maestro2 is a robotic OCT and color fundus camera system from Topcon Healthcare. It can capture a 12 mm x 9 mm wide-field OCT scan that includes the macula and optic disc. The Maestro2 can also capture high-resolution non-mydriatic, true color fundus photography, OCT, and OCTA with a single button press. Imaging is done in a dark room and automatically with minimal involvement of the operator. Protocols include 12.0 mm X9.0 mm widefield, 6.0 mm X 6.0 mm 3D macula scan and 6.0 mm X 6.0 mm OCTA (scan rate: 50 kHz).\n\n   **FLIO (Heidelberg Engineering)**\n\n   Fluorescence Lifetime Imaging Ophthalmoscopy (FLIO) is an advanced imaging technique used in ophthalmology. It is a non-invasive method that provides valuable information about the metabolic and functional status of the retina. FLIO is based on the measurement of fluorescence lifetimes, which is the duration a fluorophore remains in its excited state before emitting a photon and returning to the ground state. FLIO utilizes this fluorescence lifetime information to capture and analyze the metabolic processes occurring in the retina. Different retinal structures and molecules exhibit distinct fluorescence lifetimes, allowing for the visualization of metabolic changes, cellular activity, and the identification of specific biomolecules. The imaging is done by an operator in a dark room analogous to a straightforward heidelberg spectralis OCT. However, as it takes longer than a usual spectralis OCT and exposes patients to uncomfortable levels of light, it is kept to be performed as the last modality of an AI READI visit. Because of this patients may not be at their best possible compliance.\n\n   **Cirrus 5000 Angioplex (Carl Zeiss Meditec)**\n\n   The Zeiss Cirrus 5000 Angioplex is a high-definition optical coherence tomography (OCT) system that offers non-invasive imaging of retinal microvasculature. The imaging is done in a dark room by an operator and it is pretty straightforward and analogous to what is done in the ophthalmology clinics on a day to day basis. Imaging protocols include 512 X 512 and 200 X 200 macula and ONH scans and also OCTA of the macula. Zeiss Cirrus 5000 also provides a 60-degree OCTA widefield view. 8x8mm single scans and 14x14mm automated OCTA montage allow for rapid peripheral assessment of the retina as well.\n\n   **Monofilament testing for peripheral neuropathy**\n\n   Monofilament test is a standard clinical test to monitor peripheral neuropathy in diabetic patients. It is done using a standard 10g monofilament applying pressure to different points on the plantar surface of the feet. If patients sense the monofilament, they confirm by saying \u201cyes\u201d; if patients do not sense the monofilament after it bends, they are considered to be insensate. When the sequence is completed, the insensate area is retested for confirmation. This sequence is further repeated randomly at each of the testing sites on each foot until results are obtained.The results are recorded on an iPad, Laptop, or a paper questionnaire and are directly added to the project's RedCap by the clinical research staff.\n\n   **Montreal Cognitive Assessment (MoCA)**\n\n   The Montreal Cognitive Assessment (MoCA) is a simple, in-office screening tool that helps detect mild cognitive impairment and early onset of dementia. The MoCA evaluates cognitive domains such as: Memory, Executive functioning, Attention, Language, Visuospatial, Orientation, Visuoconstructional skills, Conceptual thinking, Calculations. The MoCA generates a total score and six domain-specific index scores. The maximum score is 30, and anything below 24 is a sign of cognitive impairment. A final total score of 26 and above is considered normal. Some disadvantages of the MoCA include: Professionals require training to score the test, A person's level of education may affect the test, Socioeconomic factors may affect the test, People living with depression or other mental health issues may score similarly to those with mild dementia. AI READI research staff perform this test on an iPad using a pre-installed software (MoCA Duo app downloaded from the app store) that captures all the patients responses in an interactive manner."
          }
        ],
        "challenge": [
          {
            "id": 1,
            "question": "Which factors in the data might limit the generalization of potentially derived models? Is this information available as auxiliary labels for challenge tests? For instance:\n\n   a. Number and diversity of devices included in the dataset.\n\n   b. Data recording specificities, e.g., the view for a chest x-ray image.\n\n   c. Number and diversity of recording sites included in the dataset.\n\n   d. Distribution shifts over time.",
            "response": "While the AI-READI's cross-sectional database ultimately aims to achieve balance across race/ethnicity, biological sex, and diabetes presence and severity, the pilot study is not balanced across these parameters.\n\n   Three recording sites were strategically selected to achieve diverse recruitment: the University of Alabama at Birmingham (UAB), the University of California San Diego (UCSD), and the University of Washington (UW). The sites were chosen for geographic diversity across the United States and to ensure diverse representation across various racial and ethnic groups. Individuals from all demographic backgrounds were recruited at all 3 sites.\n\n   Factors influencing the generalization of derived models include the predominantly urban and hospital-based recruitment, which may not fully capture diverse cultural and socioeconomic backgrounds. The study cohort may not provide a comprehensive representation of the population, as it does not include other races/ethnicities such as Pacific Islanders and Native Americans.\n\n   Information on device make and model, including specific modalities like macula scans or wide scans during OCT, were documented to ensure repeatability. Moreover, the study included multiple devices for one measure to enhance generalizability and represent the diverse range of equipment utilized in clinical settings."
          },
          {
            "id": 2,
            "question": "What confounding factors might be present in the data?\n\n   a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another?\n\n   b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another?",
            "response": "Uniform data collection protocols were implemented for all subjects, irrespective of their race/ethnicity, biological sex, or diabetes severity, across all study sites. The selection of study sites was intended to ensure equitable representation and minimize the potential for sampling bias."
          }
        ],
        "demographicInformation": [
          {
            "id": 1,
            "question": "Does the dataset identify any demographic sub-populations (e.g., by age, gender, sex, ethnicity)?",
            "response": "No"
          },
          {
            "id": 2,
            "question": "If no,\n\n   a. Is there any regulation that prevents demographic data collection in your study (for example, the country that the data is collected in)?",
            "response": "No."
          },
          {
            "id": 3,
            "question": "b. Are you employing methods to reduce the disparity of error rate between different demographic subgroups when demographic labels are unavailable? Please describe.",
            "response": "We are suggesting a split for training/validation/testing models that is aimed at reducing disparities in models developed using this dataset."
          }
        ],
        "preprocessing": [
          {
            "id": 1,
            "question": "Was there any pre-processing for the de-identification of the patients? Provide the answer for the preliminary and the current version of the dataset",
            "response": ""
          },
          {
            "id": 2,
            "question": "Was there any pre-processing for cleaning the data? Provide the answer for the preliminary and the current version of the dataset",
            "response": "There were several quality control measures used at the time of data entry/acquisition. For example, clinical data outside of expected min/max ranges were flagged in REDCap, which was visible in reports viewed by clinical research coordinators (CRCs) and Data Managers. Using these REDCap reports as guides, Data Managers and CRCs examined participant records and determined if an error was likely. Data were checked for the following and edited if errors were detected:\n\n   1. Credibility, based on range checks to determine if all responses fall within a prespecified reasonable range\n\n   2. Incorrect flow through prescribed skip patterns\n\n   3. Missing data that can be directly filed from other portions of an individual's record\n\n   4. The omission and/or duplication of records\n\n   Editing was only done under the guidance and approval of the site PI. If corrected data was available from elsewhere in the respondent's answers, the error was corrected. If there was no logical or appropriate way to correct the data, the Data site PI reviewed the values and made decisions about whether those values should be removed from the data.\n\n   Once data were sent from each of the study sites to the central project team, additional processing steps were conducted in preparation for dissemination. For example, all data were mapped to standardized terminologies when possible, such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, a common data model for observational health data, and the Digital Imaging and Communications in Medicine (DICOM), a commonly used standard for medical imaging data. Details about the data processing approaches for each data domain/modality are described in the dataset documentation at https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Was the \u201craw\u201d data (post de-identification) saved in addition to the preprocessed/cleaned data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data",
            "response": "The raw data is saved and expected to be preserved by the AI-READI project at least for the duration of the project but is not anticipated to be shared outside the project team right now, because it has not been mapped to standardized terminologies and because the raw data may accidentally include personal health information or personally identifiable information (e.g. in free text fields). There is a possibility that raw data may be included in future releases of the controlled access dataset."
          },
          {
            "id": 4,
            "question": "Were instances excluded from the dataset at the time of preprocessing? If so, why? For example, instances related to patients under 18 might be discarded.",
            "response": "No data were excluded from the dataset at the time of preprocessing. However, regarding to study recruitment (i.e. ability to participate in the study), the following eligibility criteria were used:\n\n   Inclusion Criteria:\n\n   - Able to provide consent\n   - \u2265 40 years old\n   - Persons with or without type 2 diabetes\n   - Must speak and read English\n\n   Exclusion Criteria:\n\n   - Must not be pregnant\n   - Must not have gestational diabetes\n   - Must not have Type 1 diabetes"
          },
          {
            "id": 5,
            "question": "If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Answer this question for both the preliminary dataset and the current version of the dataset",
            "response": "N/A"
          }
        ],
        "labeling": [
          {
            "id": 1,
            "question": "Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version.\n\n   a. If yes:\n\n   1. What are the labels provided?\n\n   2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?",
            "response": "N/A - no labels are provided"
          },
          {
            "id": 2,
            "question": "b. What labeling strategy was used?\n\n   1. Gold standard label available in the data (e.g. cancers validated by biopsies)\n\n   2. Proxy label computed from available data:\n\n      1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions)\n\n      2. Which tables and features were considered to compute the label?\n\n   3. Which proportion of the data has gold standard labels?",
            "response": "N/A - no labels are provided"
          },
          {
            "id": 3,
            "question": "c. Human-labeled data\n\n   1. How many labellers were considered?\n\n   2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, \u2026)\n\n   3. What guidelines did they follow?\n\n   4. How many labellers provide a label per instance?\n\n      If multiple labellers per instance:\n\n      1. What is the rater agreement? How was disagreement handled?\n      2. Are all labels provided, or summaries (e.g. maximum vote)?\n\n   5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes)\n\n   6. On average, how much time was required to annotate each instance?\n\n   7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?",
            "response": "N/A - no labels are provided. No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications."
          },
          {
            "id": 4,
            "question": "What are the human level performances in the applications that the dataset is supposed to address?",
            "response": "N/A"
          },
          {
            "id": 5,
            "question": "Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.",
            "response": "N/A - no labeling was performed"
          },
          {
            "id": 6,
            "question": "Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?",
            "response": "No, we do not have formal guidelines in place."
          },
          {
            "id": 7,
            "question": "Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.",
            "response": "The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced as well as possible for sex, race/ethnicity and diabetes status (no diabetes, prediabetes/lifestyle controlled, oral medication controlled, and insulin controlled)."
          }
        ],
        "collection": [
          {
            "id": 1,
            "question": "Were any REB/IRB approval (e.g., by an institutional review board or research ethics board) received? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "The initial IRB approval at the University of Washington was received on December 20, 2022. The initial approval letter can be found [here](https://docs.aireadi.org/v3/Approval_STUDY00016228_Lee_initial.pdf). An annual renewal application to the IRB about the status and progress of the study is required and due within 90 days of expiration."
          },
          {
            "id": 2,
            "question": "How was the data associated with each instance acquired? Was the data directly observable (e.g., medical images, labs or vitals), reported by subjects (e.g., survey responses, pain levels, itching/burning sensations), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.",
            "response": "The acquisition of data varied based on the domain; some data were directly observable (such as labs, vitals, and retinal imaging), whereas other data were reported by subjects (e.g. survey responses). Verification of data entry was performed when possible (e.g. cross-referencing entered medications with medications that were physically brought in or photographed by each study participant). Details for each data domain are available in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Provide the answer for all modalities and collected data. Has this information been changed through the process? If so, explain why.",
            "response": "The procedures for data collection and processing is available at https://docs.aireadi.org."
          },
          {
            "id": 4,
            "question": "Who was involved in the data collection process (e.g., patients, clinicians, doctors, ML researchers, hospital staff, vendors, etc.) and how were they compensated (e.g., how much were contributors paid)?",
            "response": "Details about the AI-READI team members involved in the data collection process are available at https://aireadi.org/team. Their effort was supported by the National Institutes of Health award OT2OD032644 based on the percentage of effort contributed, and salaries which aligned with the funding guidelines at each site. Study subjects received a compensation of $200 for the study visit also through the grant funding."
          },
          {
            "id": 5,
            "question": "Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.",
            "response": "The timeline for the overall project spans four years, encompassing one year dedicated to protocol development and training, and years 2-4 allocated for subject recruitment and data collection. Approximately 4% of participants are expected to undergo a follow-up examination in Year 4. The data collection process is specifically tailored to enable downstream pseudotime manifold analysis\u2014an approach used to predict disease trajectories. This involves gathering and learning from complex, multimodal data from participants exhibiting varying disease severity, ranging from normal to insulin-dependent Type 2 Diabetes Mellitus (T2DM). The timeframe also allows for the collection of the highest number of subjects possible to ensure a balanced representation of racial and ethnic groups and mitigate biases in computer vision algorithms.\n\n For this version of the dataset, the timeframe for data collection was July 19, 2023 to May 01, 2025."
          },
          {
            "id": 6,
            "question": "Does the dataset relate to people? If not, you may skip the remaining questions in this section.",
            "response": "Yes"
          },
          {
            "id": 7,
            "question": "Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., hospitals, app company)?",
            "response": "The data was collected directly from participants across the three recruiting sites. Recruitment pools were identified by screening Electronic Health Records (EHR) for diabetes and prediabetes ICD-10 codes for all patients who have had an encounter with the sites' health systems within the past 2 years."
          },
          {
            "id": 8,
            "question": "Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.",
            "response": "Yes, each individual was aware of the data collection, as this was not passive data collection or secondary use of existing data, but rather active data collection directly from participants."
          },
          {
            "id": 9,
            "question": "Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.",
            "response": "Informed consent to participate was required before participation in any part of the protocol (including questionnaires). Potential participants were given the option to read all consent documentation electronically (e-consent) before their visit and give their consent with an electronic signature without verbal communication with a clinical research coordinator. Participants may access e-consent documentation in REDCap and decide at that point they do not want to participate or would like additional information. The approved consent form for the principal project site University of Washington is available [here](https://docs.aireadi.org/v3/AI-READI%20Consent_Form_Standard_Mod17May2023_useforpilot.pdf). The other clinical sites had IRB reliance and used the same consent form, with minor institution-specific language incorporated depending on individual institutional requirements."
          },
          {
            "id": 10,
            "question": "If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).",
            "response": "Participants were permitted to withdraw consent at any time and cease study participation. However, any data that had been shared or used up to that point would stay in the dataset. This is clearly communicated in the consent document."
          },
          {
            "id": 11,
            "question": "In which countries was the data collected?",
            "response": "USA"
          },
          {
            "id": 12,
            "question": "Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.",
            "response": "No, a data protection impact analysis has not been conducted."
          }
        ],
        "inclusion": [
          {
            "id": 1,
            "question": "Is there any language-based communication with patients (e.g: English, French)? If yes, describe the choices of language(s) for communication. (for example, if there is an app used for communication, what are the language options?)",
            "response": "English language was used for communication with study participants."
          },
          {
            "id": 2,
            "question": "What are the accessibility measurements and what aspects were considered when the study was designed and implemented?",
            "response": "Accessibility measurements were not specifically assessed. However, transportation assistance (rideshare services) was offered to study participants who endorsed barriers to transporting themselves to study visits."
          },
          {
            "id": 3,
            "question": "If data is part of a clinical study, what are the inclusion criteria?",
            "response": "The eligibility criteria for the study were as follows:\n\n Inclusion Criteria:\n\n - Able to provide consent\n - \u2265 40 years old\n - Persons with or without type 2 diabetes\n - Must speak and read English\n\n Exclusion Criteria:\n\n - Must not be pregnant\n - Must not have gestational diabetes\n - Must not have Type 1 diabetes"
          }
        ],
        "uses": [
          {
            "id": 1,
            "question": "Has the dataset been used for any tasks already? If so, please provide a description. ",
            "response": "No"
          },
          {
            "id": 2,
            "question": "Does using the dataset require the citation of the paper or any other forms of acknowledgement? If yes, is it easily accessible through google scholar or other repositories",
            "response": "Yes, use of the dataset requires citation to the resources specified in https://docs.aireadi.org."
          },
          {
            "id": 3,
            "question": "Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. (besides Google scholar)",
            "response": "No"
          },
          {
            "id": 4,
            "question": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?",
            "response": "No, to the extent of our knowledge, we do not currently anticipate any uses of the dataset that could result in unfair treatment or harm. However, there is a theoretical risk of future re-identification."
          },
          {
            "id": 5,
            "question": "Are there tasks for which the dataset should not be used? If so, please provide a description. (for example, dataset creators could recommend against using the dataset for considering immigration cases, as part of insurance policies)",
            "response": "This is answered in a prior question (see details regarding license terms)."
          }
        ],
        "distribution": [
          {
            "id": 1,
            "question": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.",
            "response": "The dataset will be distributed and be available for public use."
          },
          {
            "id": 2,
            "question": "How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?",
            "response": "The dataset will be available through the FAIRhub platform (http://fairhub.io/). The dataset' DOI is https://doi.org/10.60775/fairhub.3"
          },
          {
            "id": 3,
            "question": "When was/will the dataset be distributed?",
            "response": "The first version of the dataset was distributed in May 2024, the second version of the dataset was distributed in November 2024, and the third version of the dataset was distributed in November 2025."
          },
          {
            "id": 4,
            "question": "Assuming the dataset is available, will it be/is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.",
            "response": "We provide here the license file containing the terms for reusing the AI-READI dataset (https://doi.org/10.5281/zenodo.17555036). These license terms were specifically tailored to enable reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purpose while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications."
          },
          {
            "id": 5,
            "question": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.17555036)"
          },
          {
            "id": 6,
            "question": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.",
            "response": "Refer to license (https://doi.org/10.5281/zenodo.17555036)"
          }
        ],
        "maintenance": [
          {
            "id": 1,
            "question": "Who is supporting/hosting/maintaining the dataset?",
            "response": "The AI-READI team will be supporting and maintaining the dataset. The dataset is hosted on FAIRhub through Microsoft Azure."
          },
          {
            "id": 2,
            "question": "How can the owner/curator/manager of the dataset be contacted (e.g. email address)?",
            "response": "We refer to the README file included with the dataset for contact information."
          },
          {
            "id": 3,
            "question": "Is there an erratum? If so, please provide a link or other access point.",
            "response": ""
          },
          {
            "id": 4,
            "question": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?",
            "response": "The dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants complete the study visit."
          },
          {
            "id": 5,
            "question": "If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.",
            "response": "There are no limits on the retention of the data associated with the instances."
          },
          {
            "id": 6,
            "question": "Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how and for how long. If not, please describe how its obsolescence will be communicated to users.",
            "response": "N/A - as mentioned in the response to question 4, the dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants are enrolled."
          },
          {
            "id": 7,
            "question": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?",
            "response": "No, currently there is no mechanism for others to extend or augment the AI-READI dataset outside of those who are involved in the project."
          }
        ]
      }
    },
    "files": [
      {
        "children": [
          {
            "label": "ecg_12lead",
            "children": [
              {
                "label": "philips_tc30",
                "children": []
              }
            ]
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "cardiac_ecg"
      },
      {
        "children": [
          {
            "label": "dqd_omop.json",
            "children": []
          }
        ],
        "label": "clinical_data"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "leelab_anura"
              }
            ],
            "label": "environmental_sensor"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "environment"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_flio"
              }
            ],
            "label": "flio"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_flio"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "structural_oct"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_oct"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "enface"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "flow_cube"
          },
          {
            "children": [
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              }
            ],
            "label": "segmentation"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "retinal_octa"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "optomed_aurora"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              }
            ],
            "label": "cfp"
          },
          {
            "children": [
              {
                "children": [],
                "label": "icare_eidon"
              }
            ],
            "label": "faf"
          },
          {
            "children": [
              {
                "children": [],
                "label": "heidelberg_spectralis"
              },
              {
                "children": [],
                "label": "icare_eidon"
              },
              {
                "children": [],
                "label": "topcon_maestro2"
              },
              {
                "children": [],
                "label": "topcon_triton"
              },
              {
                "children": [],
                "label": "zeiss_cirrus"
              },
              {
                "label": "manifest.tsv"
              }
            ],
            "label": "ir"
          }
        ],
        "label": "retinal_photography"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "heart_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "oxygen_saturation"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "physical_activity"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "physical_activity_calorie"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "respiratory_rate"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "sleep"
          },
          {
            "children": [
              {
                "children": [],
                "label": "garmin_vivosmart5"
              }
            ],
            "label": "stress"
          },
          {
            "label": "manifest.tsv"
          }
        ],
        "label": "wearable_activity_monitor"
      },
      {
        "children": [
          {
            "children": [
              {
                "children": [],
                "label": "dexcom_g6"
              }
            ],
            "label": "continuous_glucose_monitoring"
          }
        ],
        "label": "wearable_blood_glucose"
      },
      {
        "label": "CHANGELOG.md"
      },
      {
        "label": "LICENSE.txt"
      },
      {
        "label": "README.md"
      },
      {
        "label": "dataset_description.json"
      },
      {
        "label": "dataset_structure_description.json"
      },
      {
        "label": "healthsheet.md"
      },
      {
        "label": "participants.json"
      },
      {
        "label": "participants.tsv"
      },
      {
        "label": "study_description.json"
      }
    ],
    "data": {
      "size": 3815969779678,
      "fileCount": 356343,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://fairhub.io/datasets/3",
    "created": "1763366401"
  },
  {
    "id": 16,
    "canonicalId": "w5n9yl8kfvw7tlalu9a5z65s",
    "datasetId": "w5n9yl8kfvw7tlalu9a5z65s",
    "doi": "10.5281/zenodo.8254022",
    "title": "Dataset for PT-OCT ANN Project",
    "description": "<p>This dataset contains files for training and validation of the ANN in PT-OCT</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This dataset contains files for training and validation of the ANN in PT-OCT</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.8254022",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset for PT-OCT ANN Project"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Hossein",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "York University"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-08-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This dataset contains files for training and validation of the ANN in PT-OCT</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "PT-OCT, ANN"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "302.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 316774809,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8254022",
    "created": "1692244314"
  },
  {
    "id": 17,
    "canonicalId": "lnwcy72y202s1gx3bnw2fjxx",
    "datasetId": "lnwcy72y202s1gx3bnw2fjxx",
    "doi": "10.5281/zenodo.10537424",
    "title": "DeepEye: A Deep learning approach for Detection of Automated Retinal Eye Disease Diagnosis",
    "description": "No description available",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "No description available",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.10537424",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "DeepEye: A Deep learning approach for Detection of Automated Retinal Eye Disease Diagnosis"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Samiksha Pachade1, Prasanna Porwal1, Dhanashree Thulkar2, Manesh Kokare1, Girish Deshmukh6, Vivek Sahasrabuddhe7, Luca Giancardo5, Gwenol\u00e9 Quellec4, Fabrice M\u00e9riaudeau3",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, Maharashtra, India"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-01-20",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "No description available",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "330.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 346554368,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/10537424",
    "created": "1705729785"
  },
  {
    "id": 18,
    "canonicalId": "t0aw3zf9387d6b54d8cpipx6",
    "datasetId": "t0aw3zf9387d6b54d8cpipx6",
    "doi": "10.5281/zenodo.15054026",
    "title": "OCT images of sodium iodate induced retinal degeneration and corresponding ML models for image segmentation",
    "description": "No description available",
    "versionTitle": "1.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "No description available",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.15054026",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCT images of sodium iodate induced retinal degeneration and corresponding ML models for image segmentation"
          }
        ],
        "version": "1.0.0",
        "creator": [
          {
            "creatorName": "Zeng, Yong",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          },
          {
            "creatorName": "Qian, Haohua",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          },
          {
            "creatorName": "Zhou, Jiaming",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-03-20",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "No description available",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "668.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 700973056,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/15054026",
    "created": "1749660967"
  },
  {
    "id": 19,
    "canonicalId": "rd7bj3g04kjhvoeyd8esxodz",
    "datasetId": "rd7bj3g04kjhvoeyd8esxodz",
    "doi": "10.5061/dryad.tht76hf0c",
    "title": "Dataset for RSOS-211108",
    "description": "<p><span>The tomography of participants' right eyes measuring by optical OCT and rotating Scheimpflug imaging (OCULUS <em>Pentacam</em>). </span></p>\n\n<p><span>A custom-built MATLAB code for automatic segmentation of corneal OCT images.</span></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><span>The tomography of participants' right eyes measuring by optical OCT and rotating Scheimpflug imaging (OCULUS <em>Pentacam</em>). </span></p>\n\n<p><span>A custom-built MATLAB code for automatic segmentation of corneal OCT images.</span></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.tht76hf0c",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset for RSOS-211108"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ran, Ziying",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Liverpool"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-12-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><span>The tomography of participants' right eyes measuring by optical OCT and rotating Scheimpflug imaging (OCULUS <em>Pentacam</em>). </span></p>\n\n<p><span>A custom-built MATLAB code for automatic segmentation of corneal OCT images.</span></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "366.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 383883673,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5790134",
    "created": "1639776924"
  },
  {
    "id": 20,
    "canonicalId": "zdsp0yh4sl6kr3t895hsral7",
    "datasetId": "zdsp0yh4sl6kr3t895hsral7",
    "doi": "10.5281/zenodo.17408396",
    "title": "The human fovea is relatively horizontally elongated in infantile nystagmus",
    "description": "<p>Numerical data associated with the manuscript, MATLAB code to identify the deepest foveal scan from a stack of foveal OCT images, and OCT images of the deepest scans from study participants.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Numerical data associated with the manuscript, MATLAB code to identify the deepest foveal scan from a stack of foveal OCT images, and OCT images of the deepest scans from study participants.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17408396",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "The human fovea is relatively horizontally elongated in infantile nystagmus"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Dunn, Matt James",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Cardiff University"
              }
            ]
          },
          {
            "creatorName": "Thomas, Nikita",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Cardiff University"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-10-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Numerical data associated with the manuscript, MATLAB code to identify the deepest foveal scan from a stack of foveal OCT images, and OCT images of the deepest scans from study participants.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "48.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 50331648,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17408396",
    "created": "1761062874"
  },
  {
    "id": 21,
    "canonicalId": "n79dkh9xbkd18grszakg00da",
    "datasetId": "n79dkh9xbkd18grszakg00da",
    "doi": "10.5281/zenodo.4462320",
    "title": "Classification of pseudocalcium visual responses from mouse retinal ganglion cells",
    "description": "<p>The dataset includes spike data of 9 different recordings from mouse&nbsp;retina ganglion cells and the&nbsp;<em>Pseudocalcium </em>traces after convolving spikes with OGB kernel.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The dataset includes spike data of 9 different recordings from mouse&nbsp;retina ganglion cells and the&nbsp;<em>Pseudocalcium </em>traces after convolving spikes with OGB kernel.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.4462320",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Classification of pseudocalcium visual responses from mouse retinal ganglion cells"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Hamed Shabani",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, Centre for Ophthalmology, Eberhard Karls University, T\u00fcbingen, Germany"
              }
            ]
          },
          {
            "creatorName": "Eberhart Zrenner",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, Centre for Ophthalmology, Eberhard Karls University, T\u00fcbingen, Germany"
              }
            ]
          },
          {
            "creatorName": "Daniel Rathbun",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Detroit Institute of Ophthalmology, Henry Ford Health System, Detroit, MI, United States of America"
              }
            ]
          },
          {
            "creatorName": "zohreh hosseinzadeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Molecular and Cellular Mechanisms of Neurodegeneration, Paul Flechsig Institute for Brain Research, University of Leipzig, Leipzig, Germany"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-01-30",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The dataset includes spike data of 9 different recordings from mouse&nbsp;retina ganglion cells and the&nbsp;<em>Pseudocalcium </em>traces after convolving spikes with OGB kernel.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Ganglion cell, cell type, clustering"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "296.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 310902784,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4462320",
    "created": "1612004411"
  },
  {
    "id": 22,
    "canonicalId": "j3bkvejzbt1vl9ji493gqd0h",
    "datasetId": "j3bkvejzbt1vl9ji493gqd0h",
    "doi": "10.5281/zenodo.6582341",
    "title": "Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors",
    "description": "<p>Dataset for Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors<br>\n<br>\nAuthors left anonymous for double-blind review</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset for Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors<br>\n<br>\nAuthors left anonymous for double-blind review</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6582341",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Anonymous",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Anonymous"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-05-25",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset for Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors<br>\n<br>\nAuthors left anonymous for double-blind review</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1383.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1450495180,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6582341",
    "created": "1653511910"
  },
  {
    "id": 23,
    "canonicalId": "jz7287b2trn5s0tunoydbskm",
    "datasetId": "jz7287b2trn5s0tunoydbskm",
    "doi": "10.5281/zenodo.5798072",
    "title": "Multiscale Entropy Analysis of Retinal Signals Reveals Reduced Complexity in a Mouse Model of Alzheimer's Disease",
    "description": "<p>MEA recordings from&nbsp;wild-type and 5xFAD mice&nbsp;retinas used for the analyses in the manuscript &quot;Multiscale Entropy Analysis of Retinal Signals Reveals Reduced Complexity in a Mouse Model of Alzheimer&#39;s Disease&quot;.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>MEA recordings from&nbsp;wild-type and 5xFAD mice&nbsp;retinas used for the analyses in the manuscript &quot;Multiscale Entropy Analysis of Retinal Signals Reveals Reduced Complexity in a Mouse Model of Alzheimer&#39;s Disease&quot;.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.5798072",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Multiscale Entropy Analysis of Retinal Signals Reveals Reduced Complexity in a Mouse Model of Alzheimer's Disease"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Araya-Arriagada, Joaqu\u00edn",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Garay, Sebasti\u00e1n",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Rojas, Crist\u00f3bal",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Duran-Aniotz, Claudia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Palacios, Adrian G.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Chac\u00f3n, Max",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Medina, Leonel E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universidad de Santiago de Chile"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-12-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>MEA recordings from&nbsp;wild-type and 5xFAD mice&nbsp;retinas used for the analyses in the manuscript &quot;Multiscale Entropy Analysis of Retinal Signals Reveals Reduced Complexity in a Mouse Model of Alzheimer&#39;s Disease&quot;.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1255.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1316277452,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5798072",
    "created": "1666036454"
  },
  {
    "id": 24,
    "canonicalId": "v8ngl1po3cbuauezzdhiidnv",
    "datasetId": "v8ngl1po3cbuauezzdhiidnv",
    "doi": "10.5281/zenodo.16983454",
    "title": "Dataset: Differential topographic organization and retinal inheritance of direction and orientation selectivity in the visual thalamus",
    "description": "<p>Dataset and code to generate the figures of Yaakov*<em>, </em>Heukamp* et al.,2025 (Nature Communications): \"Differential topographic organization and retinal inheritance of direction and orientation selectivity in the visual thalamus\"</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset and code to generate the figures of Yaakov*<em>, </em>Heukamp* et al.,2025 (Nature Communications): \"Differential topographic organization and retinal inheritance of direction and orientation selectivity in the visual thalamus\"</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.16983454",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset: Differential topographic organization and retinal inheritance of direction and orientation selectivity in the visual thalamus"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Rivlin-Etzion, Michal",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Weizmann Institute of Science"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-08-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset and code to generate the figures of Yaakov*<em>, </em>Heukamp* et al.,2025 (Nature Communications): \"Differential topographic organization and retinal inheritance of direction and orientation selectivity in the visual thalamus\"</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "309.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 324009984,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/16983454",
    "created": "1756383800"
  },
  {
    "id": 25,
    "canonicalId": "kp30tmhfn2lxj136fhtckvxq",
    "datasetId": "kp30tmhfn2lxj136fhtckvxq",
    "doi": "10.5281/zenodo.13759203",
    "title": "Nonlinear spatial integration allows the retina to detect the sign of defocus in natural scenes",
    "description": "<p>The dataset comprises three types of data. First, multi-electrode array recordings of mouse retinal ganglion cells performed by Awen Louboutin and Tom Quetu. Second, point spread functions from mouse and human optical eye models, and associated convolved natural images. Third, results from a convolutional neural network model trainings.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The dataset comprises three types of data. First, multi-electrode array recordings of mouse retinal ganglion cells performed by Awen Louboutin and Tom Quetu. Second, point spread functions from mouse and human optical eye models, and associated convolved natural images. Third, results from a convolutional neural network model trainings.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.13759203",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Nonlinear spatial integration allows the retina to detect the sign of defocus in natural scenes"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Goethals, Sarah",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "EssilorLuxottica (France)"
              }
            ]
          },
          {
            "creatorName": "Louboutin, Awen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9"
              }
            ]
          },
          {
            "creatorName": "Hamlaoui, Samy",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "EssilorLuxottica (France)"
              }
            ]
          },
          {
            "creatorName": "Quetu, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9"
              }
            ]
          },
          {
            "creatorName": "Virgili, Samuele",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-09-13",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The dataset comprises three types of data. First, multi-electrode array recordings of mouse retinal ganglion cells performed by Awen Louboutin and Tom Quetu. Second, point spread functions from mouse and human optical eye models, and associated convolved natural images. Third, results from a convolutional neural network model trainings.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "18785.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 19698339020,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/13759203",
    "created": "1726230582"
  },
  {
    "id": 26,
    "canonicalId": "nixe1byswpf6rsem9mjqrh61",
    "datasetId": "nixe1byswpf6rsem9mjqrh61",
    "doi": "10.5281/zenodo.56515",
    "title": "Retinal status analysis method based on feature extraction and quantitative grading in OCT images",
    "description": "<p>The raw database includes 200 retinal OCT&nbsp;images judged as normal by ophthalmologists and 100 images with various abnormalities. The software includes the main steps for retinal status analysis. The software was carried out in Matlab.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The raw database includes 200 retinal OCT&nbsp;images judged as normal by ophthalmologists and 100 images with various abnormalities. The software includes the main steps for retinal status analysis. The software was carried out in Matlab.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.56515",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Retinal status analysis method based on feature extraction and quantitative grading in OCT images"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Fu Dongmei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Science and Technology Beijing"
              }
            ]
          },
          {
            "creatorName": "Tong Hejun",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Science and Technology Beijing"
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-06-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The raw database includes 200 retinal OCT&nbsp;images judged as normal by ophthalmologists and 100 images with various abnormalities. The software includes the main steps for retinal status analysis. The software was carried out in Matlab.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Retinal OCT images; Image processing; Morphological characterization; Feature quantification; Grade evaluation"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "172.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 180774502,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/56515",
    "created": "1467198876"
  },
  {
    "id": 27,
    "canonicalId": "qdwaf1k9v9mgdkv2brrzbtkw",
    "datasetId": "qdwaf1k9v9mgdkv2brrzbtkw",
    "doi": "10.5281/zenodo.1316912",
    "title": "Matejcic et al. 2018 data release",
    "description": "<p>Dataset and scripts from study Matej\u010di\u0107&nbsp;et al. 2018. PLoS Biology, on isotropic growth of the embryonic zebrafish retinal tissue.</p>",
    "versionTitle": "v1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset and scripts from study Matej\u010di\u0107&nbsp;et al. 2018. PLoS Biology, on isotropic growth of the embryonic zebrafish retinal tissue.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.1316912",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Matejcic et al. 2018 data release"
          }
        ],
        "version": "v1",
        "creator": [
          {
            "creatorName": "Marija Matejcic",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG)"
              }
            ]
          },
          {
            "creatorName": "Guillaume Salbrex",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The Francis Crick Institute"
              }
            ]
          },
          {
            "creatorName": "Caren Norden",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG)"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-07-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset and scripts from study Matej\u010di\u0107&nbsp;et al. 2018. PLoS Biology, on isotropic growth of the embryonic zebrafish retinal tissue.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "isotropic"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "pseudostratified epithelium"
          },
          {
            "subjectValue": "zebrafish"
          },
          {
            "subjectValue": "tissue growth"
          },
          {
            "subjectValue": "tissue shape"
          },
          {
            "subjectValue": "cell shape"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1258291,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/1316912",
    "created": "1532009323"
  },
  {
    "id": 28,
    "canonicalId": "yyly9q1jq7z1o0l64dhfp40a",
    "datasetId": "yyly9q1jq7z1o0l64dhfp40a",
    "doi": "10.5281/zenodo.12646034",
    "title": "Comparing the Clinical Viability of Automated Fundus Image Segmentation Methods - Subjective Evaluation Results",
    "description": "<p>A dataset contains answers collected through subjective evaluation presented in the paper <a href=\"https://www.mdpi.com/1424-8220/22/23/9101\">Comparing the Clinical Viability of Automated Fundus Image Segmentation Methods</a>.</p>\n<p>Results are part of the doctoral thesis named: <em>\"A Methodology for Robustness Evaluation of Deep Learning Methods in Retinal Vessel Segmentation\"</em></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>A dataset contains answers collected through subjective evaluation presented in the paper <a href=\"https://www.mdpi.com/1424-8220/22/23/9101\">Comparing the Clinical Viability of Automated Fundus Image Segmentation Methods</a>.</p>\n<p>Results are part of the doctoral thesis named: <em>\"A Methodology for Robustness Evaluation of Deep Learning Methods in Retinal Vessel Segmentation\"</em></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.12646034",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Comparing the Clinical Viability of Automated Fundus Image Segmentation Methods - Subjective Evaluation Results"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Gojic, Gorana",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-07-04",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>A dataset contains answers collected through subjective evaluation presented in the paper <a href=\"https://www.mdpi.com/1424-8220/22/23/9101\">Comparing the Clinical Viability of Automated Fundus Image Segmentation Methods</a>.</p>\n<p>Results are part of the doctoral thesis named: <em>\"A Methodology for Robustness Evaluation of Deep Learning Methods in Retinal Vessel Segmentation\"</em></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 314572,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/12646034",
    "created": "1720081719"
  },
  {
    "id": 29,
    "canonicalId": "sxdi1jv8pdct2v17cccuzp77",
    "datasetId": "sxdi1jv8pdct2v17cccuzp77",
    "doi": "10.5281/zenodo.17298706",
    "title": "Data for 'Multidimensional Data Analysis and Classification using SMIAL'",
    "description": "<p>This deposit contains the raw image files used in 'Multidimensional Data Analysis and Classification using SMIAL'. This includes hyperspectral autofluorescence images of melanoma COLO679 and A375 cells, and fibroblast 142BR. Autofluorescence images of mitochondrial networks in retinal h1RPE7 cells is also stored here.&nbsp;</p>",
    "versionTitle": "1.00",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This deposit contains the raw image files used in 'Multidimensional Data Analysis and Classification using SMIAL'. This includes hyperspectral autofluorescence images of melanoma COLO679 and A375 cells, and fibroblast 142BR. Autofluorescence images of mitochondrial networks in retinal h1RPE7 cells is also stored here.&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17298706",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data for 'Multidimensional Data Analysis and Classification using SMIAL'"
          }
        ],
        "version": "1.00",
        "creator": [
          {
            "creatorName": "Goldys, Ewa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UNSW Sydney"
              }
            ]
          },
          {
            "creatorName": "Handley, Shannon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Knab, Aline",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Bhargava, Akanksha",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UNSW Canberra"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-10-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This deposit contains the raw image files used in 'Multidimensional Data Analysis and Classification using SMIAL'. This includes hyperspectral autofluorescence images of melanoma COLO679 and A375 cells, and fibroblast 142BR. Autofluorescence images of mitochondrial networks in retinal h1RPE7 cells is also stored here.&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "10435.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 10942729420,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17298706",
    "created": "1760078066"
  },
  {
    "id": 30,
    "canonicalId": "w7pdfcmhpr3kud4ht7wtvr28",
    "datasetId": "w7pdfcmhpr3kud4ht7wtvr28",
    "doi": "10.5281/zenodo.11217687",
    "title": "GWAS Summary Statistics For Eye Imaging Traits",
    "description": "<p>This repository hosts GWAS summary statistics for eye imaging traits (OCT and fundus image traits) on the UK Biobank cohort.</p>\n<p>We have a website (<a href=\"https://www.eyekp.org/\">eyekp.org</a>) showing more details with respect to this repository.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This repository hosts GWAS summary statistics for eye imaging traits (OCT and fundus image traits) on the UK Biobank cohort.</p>\n<p>We have a website (<a href=\"https://www.eyekp.org/\">eyekp.org</a>) showing more details with respect to this repository.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.11217687",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "GWAS Summary Statistics For Eye Imaging Traits"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Zhao, Bingxin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-05-20",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This repository hosts GWAS summary statistics for eye imaging traits (OCT and fundus image traits) on the UK Biobank cohort.</p>\n<p>We have a website (<a href=\"https://www.eyekp.org/\">eyekp.org</a>) showing more details with respect to this repository.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "31017.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 32524206080,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/11217687",
    "created": "1716168106"
  },
  {
    "id": 31,
    "canonicalId": "ifeqhgvcqfmu72s2xcu96i90",
    "datasetId": "ifeqhgvcqfmu72s2xcu96i90",
    "doi": "10.5281/zenodo.7505822",
    "title": "Retinal Fundus Multi-Disease Image Dataset (RFMiD) 2.0",
    "description": "<p>Retinal Fundus Multi-disease Image Dataset (RFMiD 2.0) is an auxiliary dataset to our previously published dataset. RFMiD 2.0 is a more challenging dataset to research society to develop the computer-based disease diagnosis system. Diabetic Retinopathy, cataracts, and refractive error in the eye are leading diseases that may&nbsp;lead to&nbsp;permanent vision loss more frequently. Therefore, developing an AI-based model to classify these diseases is useful for ophthalmologists. This dataset consists of 860 images of frequently and rarely observed 51 diseases. However, some images are labeled with multiple diseases. This dataset is useful for the research and development of AI-based medical healthcare systems in ophthalmology.&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Retinal Fundus Multi-disease Image Dataset (RFMiD 2.0) is an auxiliary dataset to our previously published dataset. RFMiD 2.0 is a more challenging dataset to research society to develop the computer-based disease diagnosis system. Diabetic Retinopathy, cataracts, and refractive error in the eye are leading diseases that may&nbsp;lead to&nbsp;permanent vision loss more frequently. Therefore, developing an AI-based model to classify these diseases is useful for ophthalmologists. This dataset consists of 860 images of frequently and rarely observed 51 diseases. However, some images are labeled with multiple diseases. This dataset is useful for the research and development of AI-based medical healthcare systems in ophthalmology.&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7505822",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Retinal Fundus Multi-Disease Image Dataset (RFMiD) 2.0"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Sachin Panchal",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Excellence in Signal and Image Processing, Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, Maharashtra 431606, India"
              }
            ]
          },
          {
            "creatorName": "Ankita Naik",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Excellence in Signal and Image Processing, Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, Maharashtra 431606, India"
              }
            ]
          },
          {
            "creatorName": "Manesh Kokare",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Excellence in Signal and Image Processing, Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, Maharashtra 431606, India"
              }
            ]
          },
          {
            "creatorName": "Samiksha Pachade",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Rushikesh Naigaonkar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Shri Ganapati Netralaya State of Art Eye Care Hospital, Jalna, Maharashtra, India"
              }
            ]
          },
          {
            "creatorName": "Prerana Phadnis",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Lions Eye Hospital, Nanded, Maharashtra, India"
              }
            ]
          },
          {
            "creatorName": "Archana Bhange",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Keya Eye Clinic, Pune, Maharashtra, India"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-01-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Retinal Fundus Multi-disease Image Dataset (RFMiD 2.0) is an auxiliary dataset to our previously published dataset. RFMiD 2.0 is a more challenging dataset to research society to develop the computer-based disease diagnosis system. Diabetic Retinopathy, cataracts, and refractive error in the eye are leading diseases that may&nbsp;lead to&nbsp;permanent vision loss more frequently. Therefore, developing an AI-based model to classify these diseases is useful for ophthalmologists. This dataset consists of 860 images of frequently and rarely observed 51 diseases. However, some images are labeled with multiple diseases. This dataset is useful for the research and development of AI-based medical healthcare systems in ophthalmology.&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Data Analysis; Ocular Diseases; Retinal Fundus Image Dataset; Data Annotation; Multilabel Classification."
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "60.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 63648563,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7505822",
    "created": "1672919846"
  },
  {
    "id": 32,
    "canonicalId": "ocs4qw0lhidy5km96liwqrbp",
    "datasetId": "ocs4qw0lhidy5km96liwqrbp",
    "doi": "10.5281/zenodo.7476775",
    "title": "Immunostaining data for reviewing manuscript",
    "description": "<p>The reviewing manuscript entitled &quot;CRISPR/Cas9 Mediated Specific Ablation of Vegfa in Retinal Pigment Epithelium Efficiently Regresses Choroidal Neovascularization&quot; show immunostaining in the figure plates. The entire immunostaining images series are provided here as .tif image sequences. The file names refer to the figure numbers and position in the figure plates.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The reviewing manuscript entitled &quot;CRISPR/Cas9 Mediated Specific Ablation of Vegfa in Retinal Pigment Epithelium Efficiently Regresses Choroidal Neovascularization&quot; show immunostaining in the figure plates. The entire immunostaining images series are provided here as .tif image sequences. The file names refer to the figure numbers and position in the figure plates.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7476775",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Immunostaining data for reviewing manuscript"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Jinkyu Park",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Gang Cui",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Hyundong Lee",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Han Jeong",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Jay Jiyong Kwak",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Junwon Lee",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Institute of Human Barrier Research, Gangnam Severance Hospital, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          },
          {
            "creatorName": "Suk Ho Byeon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Eye Hospital, Severance Hospital, Institue of Vision Research, Yonsei University College of Medicine, Seoul, South Korea"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-12-23",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The reviewing manuscript entitled &quot;CRISPR/Cas9 Mediated Specific Ablation of Vegfa in Retinal Pigment Epithelium Efficiently Regresses Choroidal Neovascularization&quot; show immunostaining in the figure plates. The entire immunostaining images series are provided here as .tif image sequences. The file names refer to the figure numbers and position in the figure plates.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Immunostaining data"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "9.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 9751756,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7476775",
    "created": "1671806139"
  },
  {
    "id": 33,
    "canonicalId": "qmk3u0gnukiike0wspanr1bc",
    "datasetId": "qmk3u0gnukiike0wspanr1bc",
    "doi": "10.5281/zenodo.4304781",
    "title": "High Resolution Fundus Image Database for Monomodal Single-Channel Image Registration of Thin Features",
    "description": "<p>A high resolution image database of 42 image pairs (related by elastic deformations) created from original images from the High Resolution Fundus Image Database.<br>\n<br>\nConsists of thin linear structures that lack sufficient overlap to pose a challenge for classic similarity measures based on overlapping pixels commonly used in image registration.</p>\n\n<p>The dataset contains the intensity grayscale images, as well as binary masks of the retinal area, and labels of the vessels, segmented by expert annotators.<br>\n&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>A high resolution image database of 42 image pairs (related by elastic deformations) created from original images from the High Resolution Fundus Image Database.<br>\n<br>\nConsists of thin linear structures that lack sufficient overlap to pose a challenge for classic similarity measures based on overlapping pixels commonly used in image registration.</p>\n\n<p>The dataset contains the intensity grayscale images, as well as binary masks of the retinal area, and labels of the vessels, segmented by expert annotators.<br>\n&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.4304781",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "High Resolution Fundus Image Database for Monomodal Single-Channel Image Registration of Thin Features"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Johan \u00d6fverstedt",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Information Technology, Uppsala University"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-12-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>A high resolution image database of 42 image pairs (related by elastic deformations) created from original images from the High Resolution Fundus Image Database.<br>\n<br>\nConsists of thin linear structures that lack sufficient overlap to pose a challenge for classic similarity measures based on overlapping pixels commonly used in image registration.</p>\n\n<p>The dataset contains the intensity grayscale images, as well as binary masks of the retinal area, and labels of the vessels, segmented by expert annotators.<br>\n&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "314.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 330196582,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4304781",
    "created": "1607023267"
  },
  {
    "id": 34,
    "canonicalId": "uc22mz2k9tn4rcp0kue4v4fi",
    "datasetId": "uc22mz2k9tn4rcp0kue4v4fi",
    "doi": "10.5281/zenodo.17151869",
    "title": "SYN-OCT",
    "description": "<p>The SYN-OCT dataset consists of 200,000 synthetic cross-sectional circumpapillary OCT images, including 100,000 images generated to represent glaucoma eyes and 100,000 images generated to represent healthy eyes. These synthetic images were produced using generative models trained on real OCT scans collected from participants at the Singapore Eye Research Institute.&nbsp;This resource is designed to support the development and validation of deep learning models for glaucoma detection and analysis. It also serves as a landmark dataset for advancing research on synthetic medical image generation and its potential applications in clinical and AI settings.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The SYN-OCT dataset consists of 200,000 synthetic cross-sectional circumpapillary OCT images, including 100,000 images generated to represent glaucoma eyes and 100,000 images generated to represent healthy eyes. These synthetic images were produced using generative models trained on real OCT scans collected from participants at the Singapore Eye Research Institute.&nbsp;This resource is designed to support the development and validation of deep learning models for glaucoma detection and analysis. It also serves as a landmark dataset for advancing research on synthetic medical image generation and its potential applications in clinical and AI settings.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17151869",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "SYN-OCT"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Wong, Damon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Kumar, Ashish Jith Sreejith",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Chong, Rachel S.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore National Eye Center"
              }
            ]
          },
          {
            "creatorName": "Nongpiur, Monisha E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Hussain, Rahat",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Wong, Tina",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Pereira, Shamira",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Aung, Tin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Tan, Bingyao",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Cheng, Ching-Yu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Vithana N, Eranga",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Chua, Jacqueline",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Schmetterer, Leopold",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The SYN-OCT dataset consists of 200,000 synthetic cross-sectional circumpapillary OCT images, including 100,000 images generated to represent glaucoma eyes and 100,000 images generated to represent healthy eyes. These synthetic images were produced using generative models trained on real OCT scans collected from participants at the Singapore Eye Research Institute.&nbsp;This resource is designed to support the development and validation of deep learning models for glaucoma detection and analysis. It also serves as a landmark dataset for advancing research on synthetic medical image generation and its potential applications in clinical and AI settings.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "3104.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 3254989619,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17151869",
    "created": "1769354049"
  },
  {
    "id": 35,
    "canonicalId": "bi4tpb7d2ri3ygab4e4r6ys9",
    "datasetId": "bi4tpb7d2ri3ygab4e4r6ys9",
    "doi": "10.5281/zenodo.17708044",
    "title": "Sonogenetic Vision Restoration Dataset (EMC and MscL-G22S)",
    "description": "<p>This dataset contains raw electrophysiology recordings, MEA firing-rate matrices, ultrasound calibration files, confocal images, and analysis scripts used in the study \"Sonogenetic Activation of Retinal Neurons for Vision Restoration\". The data support all main findings and figures in the manuscript.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This dataset contains raw electrophysiology recordings, MEA firing-rate matrices, ultrasound calibration files, confocal images, and analysis scripts used in the study \"Sonogenetic Activation of Retinal Neurons for Vision Restoration\". The data support all main findings and figures in the manuscript.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17708044",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Sonogenetic Vision Restoration Dataset (EMC and MscL-G22S)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "zhou, qifa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Southern California"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-11-25",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This dataset contains raw electrophysiology recordings, MEA firing-rate matrices, ultrasound calibration files, confocal images, and analysis scripts used in the study \"Sonogenetic Activation of Retinal Neurons for Vision Restoration\". The data support all main findings and figures in the manuscript.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Sonogenetics"
          },
          {
            "subjectValue": "Ultrasound Stimulation"
          },
          {
            "subjectValue": "Vision Restoration"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "124.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 130967142,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17708044",
    "created": "1764058787"
  },
  {
    "id": 36,
    "canonicalId": "opyxdq4b5dxpizuue0vijj6t",
    "datasetId": "opyxdq4b5dxpizuue0vijj6t",
    "doi": "10.5281/zenodo.12659652",
    "title": "Evaluation benchmark for natural robustness evaluation of retinal vessel segmentation models",
    "description": "<p>A dataset contains benchmark images for natural robustness evaluation of deep learning models for retinal vessel segmentation. The dataset consists of three mainstream retinal vessel segmentation datasets: DRIVE, STARE, and CHASE_DB1.</p>\n<p>For each dataset are provided:</p>\n<ul>\n<li><em>images </em>- directory containing fundus images augmented using <a href=\"https://github.com/goranagojic/AugOOD\">AugOOD</a> tool for fast image augmentation for OOD robustness evaluation.</li>\n<li><em>labels</em> - directory with labels that correspond to the images.</li>\n<li><em>masks</em> - directory with FoV masks that correspond to the images.</li>\n</ul>\n<p>The benchmark is used in the paper <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6809\">Robustness of deep learning methods for ocular fundus segmentation: Evaluation of blur sensitivity</a> to evaluate natural robustness of a portfolio of deep learning models for retinal vessel segmentation from fundus images.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>A dataset contains benchmark images for natural robustness evaluation of deep learning models for retinal vessel segmentation. The dataset consists of three mainstream retinal vessel segmentation datasets: DRIVE, STARE, and CHASE_DB1.</p>\n<p>For each dataset are provided:</p>\n<ul>\n<li><em>images </em>- directory containing fundus images augmented using <a href=\"https://github.com/goranagojic/AugOOD\">AugOOD</a> tool for fast image augmentation for OOD robustness evaluation.</li>\n<li><em>labels</em> - directory with labels that correspond to the images.</li>\n<li><em>masks</em> - directory with FoV masks that correspond to the images.</li>\n</ul>\n<p>The benchmark is used in the paper <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6809\">Robustness of deep learning methods for ocular fundus segmentation: Evaluation of blur sensitivity</a> to evaluate natural robustness of a portfolio of deep learning models for retinal vessel segmentation from fundus images.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.12659652",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Evaluation benchmark for natural robustness evaluation of retinal vessel segmentation models"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Goji\u0107, Gorana",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-07-04",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>A dataset contains benchmark images for natural robustness evaluation of deep learning models for retinal vessel segmentation. The dataset consists of three mainstream retinal vessel segmentation datasets: DRIVE, STARE, and CHASE_DB1.</p>\n<p>For each dataset are provided:</p>\n<ul>\n<li><em>images </em>- directory containing fundus images augmented using <a href=\"https://github.com/goranagojic/AugOOD\">AugOOD</a> tool for fast image augmentation for OOD robustness evaluation.</li>\n<li><em>labels</em> - directory with labels that correspond to the images.</li>\n<li><em>masks</em> - directory with FoV masks that correspond to the images.</li>\n</ul>\n<p>The benchmark is used in the paper <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6809\">Robustness of deep learning methods for ocular fundus segmentation: Evaluation of blur sensitivity</a> to evaluate natural robustness of a portfolio of deep learning models for retinal vessel segmentation from fundus images.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "7774.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 8152468684,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/12659652",
    "created": "1720133458"
  },
  {
    "id": 37,
    "canonicalId": "sexfq9nzauwo3nwbmfhe4e8s",
    "datasetId": "sexfq9nzauwo3nwbmfhe4e8s",
    "doi": "10.5281/zenodo.4005629",
    "title": "Axial currents recordings in mouse retinal ganglion cells",
    "description": "<p>Patch-clamp recordings in mouse retinal ganglion cells performed by Martijn Sierksma. It comprises axial currents recorded in voltage clamp, spontaneous activity and the response to current pulses. The cells were filled with biocytin and subsequently labelled for ankyrin G. The dataset comprises confocal images of the full neurons morphology and their axon initial segments performed by Sarah Goethals.&nbsp;</p>\n\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Patch-clamp recordings in mouse retinal ganglion cells performed by Martijn Sierksma. It comprises axial currents recorded in voltage clamp, spontaneous activity and the response to current pulses. The cells were filled with biocytin and subsequently labelled for ankyrin G. The dataset comprises confocal images of the full neurons morphology and their axon initial segments performed by Sarah Goethals.&nbsp;</p>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.4005629",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Axial currents recordings in mouse retinal ganglion cells"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Sierksma, Martijn C.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Neuroscience, Erasmus MC, University Medical Center Rotterdam, 3000 CA, Rotterdam, The Netherlands"
              }
            ]
          },
          {
            "creatorName": "Goethals, Sarah",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9, INSERM, CNRS, Institut de la Vision, 17 rue Moreau, F-75012 Paris, France"
              }
            ]
          },
          {
            "creatorName": "Nicol, Xavier",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9, INSERM, CNRS, Institut de la Vision, 17 rue Moreau, F-75012 Paris, France"
              }
            ]
          },
          {
            "creatorName": "R\u00e9aux-Le Goazigo, Annabelle",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9, INSERM, CNRS, Institut de la Vision, 17 rue Moreau, F-75012 Paris, France"
              }
            ]
          },
          {
            "creatorName": "Brette, Romain",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sorbonne Universit\u00e9, INSERM, CNRS, Institut de la Vision, 17 rue Moreau, F-75012 Paris, France"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-08-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Patch-clamp recordings in mouse retinal ganglion cells performed by Martijn Sierksma. It comprises axial currents recorded in voltage clamp, spontaneous activity and the response to current pulses. The cells were filled with biocytin and subsequently labelled for ankyrin G. The dataset comprises confocal images of the full neurons morphology and their axon initial segments performed by Sarah Goethals.&nbsp;</p>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "7609.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 7978929356,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4005629",
    "created": "1598868170"
  },
  {
    "id": 38,
    "canonicalId": "zgkfrx6fvi94bkgggmmfw7ax",
    "datasetId": "zgkfrx6fvi94bkgggmmfw7ax",
    "doi": "10.5281/zenodo.18463827",
    "title": "Dynamic neurovascular adaptation of the retina during high-altitude hypoxia: integrated analysis of ERG and OCTA changes in healthy subjects",
    "description": "<p>This dataset contains Supplementary Table S1 (Directional analysis data of retinal vessel density) and Supplementary Figure S1 (Windmill plots) supporting the findings of the manuscript titled:Dynamic neurovascular adaptation of the retina during high-altitude hypoxia: integrated analysis of ERG and OCTA changes in healthy subjects.</p>\n<p>Submitted to: Journal of Applied Physiology</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This dataset contains Supplementary Table S1 (Directional analysis data of retinal vessel density) and Supplementary Figure S1 (Windmill plots) supporting the findings of the manuscript titled:Dynamic neurovascular adaptation of the retina during high-altitude hypoxia: integrated analysis of ERG and OCTA changes in healthy subjects.</p>\n<p>Submitted to: Journal of Applied Physiology</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.18463827",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dynamic neurovascular adaptation of the retina during high-altitude hypoxia: integrated analysis of ERG and OCTA changes in healthy subjects"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Yu, Xinli",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2026",
        "date": [
          {
            "dateValue": "2026-02-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This dataset contains Supplementary Table S1 (Directional analysis data of retinal vessel density) and Supplementary Figure S1 (Windmill plots) supporting the findings of the manuscript titled:Dynamic neurovascular adaptation of the retina during high-altitude hypoxia: integrated analysis of ERG and OCTA changes in healthy subjects.</p>\n<p>Submitted to: Journal of Applied Physiology</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1153433,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/18463827",
    "created": "1770088184"
  },
  {
    "id": 39,
    "canonicalId": "r286fv2p3wsx9x3i8fwry8pt",
    "datasetId": "r286fv2p3wsx9x3i8fwry8pt",
    "doi": "10.5281/zenodo.7432971",
    "title": "Data set accompanying the paper \"Effective cell membrane tension is independent of polyacrylamide substrate stiffness\"",
    "description": "<p>This data set contains the data presented in the publication &quot;Effective cell membrane tension is independent of polyacrylamide substrate stiffness&quot;. It consists of optical tweezers data and traction force microscopy data of 3T3 fibroblasts and Xenopus retinal ganglion cells on several different substrates.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This data set contains the data presented in the publication &quot;Effective cell membrane tension is independent of polyacrylamide substrate stiffness&quot;. It consists of optical tweezers data and traction force microscopy data of 3T3 fibroblasts and Xenopus retinal ganglion cells on several different substrates.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7432971",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data set accompanying the paper \"Effective cell membrane tension is independent of polyacrylamide substrate stiffness\""
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kreysing, Eva",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge"
              }
            ]
          },
          {
            "creatorName": "Mc Hugh, Jeffrey",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Neuroglial Interactions in Cerebral Physiopathology, Coll\u00e8ge de France, Biological and Soft Systems Maxwell Centre, Cavendish Laboratory  Cambridge"
              }
            ]
          },
          {
            "creatorName": "Foster, Sarah K.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge"
              }
            ]
          },
          {
            "creatorName": "Andresen, Kurt",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physics,  Gettysburg College, USA"
              }
            ]
          },
          {
            "creatorName": "Greenhalgh, Ryan D.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge"
              }
            ]
          },
          {
            "creatorName": "Pillai, Eva K.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge"
              }
            ]
          },
          {
            "creatorName": "Dimitracopoulos, Andrea",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge"
              }
            ]
          },
          {
            "creatorName": "Keyser, Ulrich F.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Biological and Soft Systems Maxwell Centre, Cavendish Laboratory  Cambridge"
              }
            ]
          },
          {
            "creatorName": "Franze, Kristian",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Physiology, Development and Neuroscience University of Cambridge, Friedrich-Alexander University Erlangen-Nuremberg Institute of Medical Physics"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-12-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This data set contains the data presented in the publication &quot;Effective cell membrane tension is independent of polyacrylamide substrate stiffness&quot;. It consists of optical tweezers data and traction force microscopy data of 3T3 fibroblasts and Xenopus retinal ganglion cells on several different substrates.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Optical Tweezers"
          },
          {
            "subjectValue": "Traction Force Microscopy"
          },
          {
            "subjectValue": "retinal ganglion cells"
          },
          {
            "subjectValue": "3T3 fibroblasts"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "4713.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 4942882406,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7432971",
    "created": "1670937404"
  },
  {
    "id": 40,
    "canonicalId": "v9d9gh3m68gqzbtjq3u8gwwa",
    "datasetId": "v9d9gh3m68gqzbtjq3u8gwwa",
    "doi": "10.5281/zenodo.7618624",
    "title": "Automatic Choroid Vascularity Index Calculation in Optical Coherence Tomography Images with Low Contrast Sclerocho-roidal Junction Using Deep Learning",
    "description": "<p>This project aims to calculate Choroid Vascularity Index (CVI) in optical coherenece tomography (OCT) images, using loss modified U-Net. The method is detailed in &quot;Automatic Choroid Vascularity Index Calculation in Optical Coherence Tomography Images low contrast sclerochoroidal junction Using Deep Learning&quot;. The dataset consists of&nbsp;Enhanced-depth imaging optical coherence tomography images from two patient groups.</p>\n\n<p>&bull; First dataset is including Raster OCT B-scans from patients with diabetic retinopathy.</p>\n\n<p>&bull; Second dataset is including EDI-HD OCT B-scans from patients with pachychoroid spectrum.</p>",
    "versionTitle": "1.1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This project aims to calculate Choroid Vascularity Index (CVI) in optical coherenece tomography (OCT) images, using loss modified U-Net. The method is detailed in &quot;Automatic Choroid Vascularity Index Calculation in Optical Coherence Tomography Images low contrast sclerochoroidal junction Using Deep Learning&quot;. The dataset consists of&nbsp;Enhanced-depth imaging optical coherence tomography images from two patient groups.</p>\n\n<p>&bull; First dataset is including Raster OCT B-scans from patients with diabetic retinopathy.</p>\n\n<p>&bull; Second dataset is including EDI-HD OCT B-scans from patients with pachychoroid spectrum.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7618624",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Automatic Choroid Vascularity Index Calculation in Optical Coherence Tomography Images with Low Contrast Sclerocho-roidal Junction Using Deep Learning"
          }
        ],
        "version": "1.1",
        "creator": [
          {
            "creatorName": "Raheleh Kafieh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Durham University"
              }
            ]
          },
          {
            "creatorName": "Elias Khalili Pour",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Retina Ward, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran, Iran"
              }
            ]
          },
          {
            "creatorName": "Hamid Riazi-Esfahani",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Retina Ward, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran, Iran"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-02-07",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This project aims to calculate Choroid Vascularity Index (CVI) in optical coherenece tomography (OCT) images, using loss modified U-Net. The method is detailed in &quot;Automatic Choroid Vascularity Index Calculation in Optical Coherence Tomography Images low contrast sclerochoroidal junction Using Deep Learning&quot;. The dataset consists of&nbsp;Enhanced-depth imaging optical coherence tomography images from two patient groups.</p>\n\n<p>&bull; First dataset is including Raster OCT B-scans from patients with diabetic retinopathy.</p>\n\n<p>&bull; Second dataset is including EDI-HD OCT B-scans from patients with pachychoroid spectrum.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "OCT B-scans"
          },
          {
            "subjectValue": "diabetic retinopathy"
          },
          {
            "subjectValue": "pachychoroid spectrum"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "108.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 114189926,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7618624",
    "created": "1675803895"
  },
  {
    "id": 41,
    "canonicalId": "y3ddsxn8w633nf0wakkwjgu1",
    "datasetId": "y3ddsxn8w633nf0wakkwjgu1",
    "doi": "10.5281/zenodo.3813684",
    "title": "Data to \"Point-wise correlations between 10-2 Humphrey visual field and OCT data in open angle glaucoma\"",
    "description": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p>Cirafici, P., Maiello, G., Ancona, C., Masala, A., Traverso, C.E., &amp; Iester M. (in press) Point-wise correlations between Humphrey visual field and OCT data in open angle glaucoma. Eye</p>",
    "versionTitle": "V1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p>Cirafici, P., Maiello, G., Ancona, C., Masala, A., Traverso, C.E., &amp; Iester M. (in press) Point-wise correlations between Humphrey visual field and OCT data in open angle glaucoma. Eye</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.3813684",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data to \"Point-wise correlations between 10-2 Humphrey visual field and OCT data in open angle glaucoma\""
          }
        ],
        "version": "V1.0",
        "creator": [
          {
            "creatorName": "Maiello, Guido",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Justus Liebig University Giessen"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-05-07",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p>Cirafici, P., Maiello, G., Ancona, C., Masala, A., Traverso, C.E., &amp; Iester M. (in press) Point-wise correlations between Humphrey visual field and OCT data in open angle glaucoma. Eye</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "OCT"
          },
          {
            "subjectValue": "Visual Fields"
          },
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "ganglion cell complex"
          },
          {
            "subjectValue": "peripapillary retinal nerve fiber layer"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "2.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 2097152,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3813684",
    "created": "1588867123"
  },
  {
    "id": 42,
    "canonicalId": "e1d8rs01dwlw6v04oty5uqp5",
    "datasetId": "e1d8rs01dwlw6v04oty5uqp5",
    "doi": "10.5281/zenodo.1410499",
    "title": "Data to \"Retinal Blur from Natural Scenes and Eye Shape\"",
    "description": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p><strong>Maiello, G</strong>., Harrison, W. J., Vera-Diaz, F. A., &amp; Bex, P. J. (in preparation). Retinal Blur from Natural Scenes and Eye Shape.</p>",
    "versionTitle": "v1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p><strong>Maiello, G</strong>., Harrison, W. J., Vera-Diaz, F. A., &amp; Bex, P. J. (in preparation). Retinal Blur from Natural Scenes and Eye Shape.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.1410499",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data to \"Retinal Blur from Natural Scenes and Eye Shape\""
          }
        ],
        "version": "v1.0",
        "creator": [
          {
            "creatorName": "Maiello Guido",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Justus-Liebig University Gie\u00dfen"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-09-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This record contains experimental and analysis scripts (written in Matlab)&nbsp;as well as raw and processed data to reproduce the results shown in:</p>\n\n<p><strong>Maiello, G</strong>., Harrison, W. J., Vera-Diaz, F. A., &amp; Bex, P. J. (in preparation). Retinal Blur from Natural Scenes and Eye Shape.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Myopia; Refractive error; Blur; Natural-scene statistics; Eye shape"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1799.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1887122227,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/1410499",
    "created": "1536251250"
  },
  {
    "id": 43,
    "canonicalId": "f7vqp9h9ixkjr2ylv9kv9zyx",
    "datasetId": "f7vqp9h9ixkjr2ylv9kv9zyx",
    "doi": "10.5061/dryad.4xgxd2593",
    "title": "The cell adhesion molecule Sdk1 shapes assembly of a retinal circuit that detects localized edges",
    "description": "<p>Nearly 50 different mouse retinal ganglion cell (RGC) types sample the visual scene for distinct features. RGC feature selectivity arises from its synapses with a specific subset of amacrine (AC) and bipolar cell (BC) types, but how RGC dendrites arborize and collect input from these specific subsets remains poorly understood. Here we examine the hypothesis that RGCs employ molecular recognition systems to meet this challenge. By combining calcium imaging and type-specific histological stains we define a family of circuits that express the recognition molecule Sidekick 1 (Sdk1) which include a novel RGC type (S1-RGC) that responds to local edges. Genetic and physiological studies revealed that Sdk1 loss selectively disrupts S1-RGC visual responses which result from a loss of excitatory and inhibitory inputs and selective dendritic deficits on this neuron. We conclude that Sdk1 shapes dendrite growth and wiring to help S1-RGCs become feature selective.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Nearly 50 different mouse retinal ganglion cell (RGC) types sample the visual scene for distinct features. RGC feature selectivity arises from its synapses with a specific subset of amacrine (AC) and bipolar cell (BC) types, but how RGC dendrites arborize and collect input from these specific subsets remains poorly understood. Here we examine the hypothesis that RGCs employ molecular recognition systems to meet this challenge. By combining calcium imaging and type-specific histological stains we define a family of circuits that express the recognition molecule Sidekick 1 (Sdk1) which include a novel RGC type (S1-RGC) that responds to local edges. Genetic and physiological studies revealed that Sdk1 loss selectively disrupts S1-RGC visual responses which result from a loss of excitatory and inhibitory inputs and selective dendritic deficits on this neuron. We conclude that Sdk1 shapes dendrite growth and wiring to help S1-RGCs become feature selective.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.4xgxd2593",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "The cell adhesion molecule Sdk1 shapes assembly of a retinal circuit that detects localized edges"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Rochon, Pierre-Luc",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "McGill University"
              }
            ]
          },
          {
            "creatorName": "Theriault, Catherine",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "McGill University"
              }
            ]
          },
          {
            "creatorName": "Rangel Olguin, Aline Giselle",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "McGill University"
              }
            ]
          },
          {
            "creatorName": "Krishnaswamy, Arjun",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "McGill University"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-09-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Nearly 50 different mouse retinal ganglion cell (RGC) types sample the visual scene for distinct features. RGC feature selectivity arises from its synapses with a specific subset of amacrine (AC) and bipolar cell (BC) types, but how RGC dendrites arborize and collect input from these specific subsets remains poorly understood. Here we examine the hypothesis that RGCs employ molecular recognition systems to meet this challenge. By combining calcium imaging and type-specific histological stains we define a family of circuits that express the recognition molecule Sidekick 1 (Sdk1) which include a novel RGC type (S1-RGC) that responds to local edges. Genetic and physiological studies revealed that Sdk1 loss selectively disrupts S1-RGC visual responses which result from a loss of excitatory and inhibitory inputs and selective dendritic deficits on this neuron. We conclude that Sdk1 shapes dendrite growth and wiring to help S1-RGCs become feature selective.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "3293.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 3453065625,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4976257",
    "created": "1623957558"
  },
  {
    "id": 44,
    "canonicalId": "x10we39ewsolvxywxde4jdn2",
    "datasetId": "x10we39ewsolvxywxde4jdn2",
    "doi": "10.5061/dryad.dncjsxkvk",
    "title": "Inference of nonlinear receptive field subunits with spike-triggered clustering",
    "description": "<p>Responses of sensory neurons are often modeled using a weighted combination of rectified linear subunits. Since these subunits often cannot be measured directly, a flexible method is needed to infer their properties from the responses of downstream neurons. We present a method for maximum likelihood estimation of subunits by soft-clustering spike-triggered stimuli, and demonstrate its effectiveness in visual neurons. Subunits estimated from parasol retinal ganglion cells (RGCs) in macaque retina partitioned the receptive field into compact regions, likely representing aggregated bipolar cell inputs. Joint clustering revealed shared subunits in neighboring RGCs, producing a parsimonious population model. Closed-loop validation, using stimuli lying in the null space of the linear receptive field, revealed stronger nonlinearities in OFF cells than ON cells. Responses to natural images, jittered to emulate fixational eye movements, were accurately predicted by the subunit model. Finally, the generality of the approach was demonstrated in macaque V1 neurons.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Responses of sensory neurons are often modeled using a weighted combination of rectified linear subunits. Since these subunits often cannot be measured directly, a flexible method is needed to infer their properties from the responses of downstream neurons. We present a method for maximum likelihood estimation of subunits by soft-clustering spike-triggered stimuli, and demonstrate its effectiveness in visual neurons. Subunits estimated from parasol retinal ganglion cells (RGCs) in macaque retina partitioned the receptive field into compact regions, likely representing aggregated bipolar cell inputs. Joint clustering revealed shared subunits in neighboring RGCs, producing a parsimonious population model. Closed-loop validation, using stimuli lying in the null space of the linear receptive field, revealed stronger nonlinearities in OFF cells than ON cells. Responses to natural images, jittered to emulate fixational eye movements, were accurately predicted by the subunit model. Finally, the generality of the approach was demonstrated in macaque V1 neurons.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.dncjsxkvk",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Inference of nonlinear receptive field subunits with spike-triggered clustering"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Shah, Nishal",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          },
          {
            "creatorName": "Brackbill, Nora",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          },
          {
            "creatorName": "Rhoades, Colleen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          },
          {
            "creatorName": "Kling, Alexandra",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          },
          {
            "creatorName": "Goetz, Georges",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          },
          {
            "creatorName": "Litke, Alan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Santa Cruz"
              }
            ]
          },
          {
            "creatorName": "Sher, Alexander",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Santa Cruz"
              }
            ]
          },
          {
            "creatorName": "Simoncelli, Eero",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Howard Hughes Medical Institute"
              }
            ]
          },
          {
            "creatorName": "Chichilnisky, E.J.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Stanford University"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-01-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Responses of sensory neurons are often modeled using a weighted combination of rectified linear subunits. Since these subunits often cannot be measured directly, a flexible method is needed to infer their properties from the responses of downstream neurons. We present a method for maximum likelihood estimation of subunits by soft-clustering spike-triggered stimuli, and demonstrate its effectiveness in visual neurons. Subunits estimated from parasol retinal ganglion cells (RGCs) in macaque retina partitioned the receptive field into compact regions, likely representing aggregated bipolar cell inputs. Joint clustering revealed shared subunits in neighboring RGCs, producing a parsimonious population model. Closed-loop validation, using stimuli lying in the null space of the linear receptive field, revealed stronger nonlinearities in OFF cells than ON cells. Responses to natural images, jittered to emulate fixational eye movements, were accurately predicted by the subunit model. Finally, the generality of the approach was demonstrated in macaque V1 neurons.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "6452.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 6765726924,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4992182",
    "created": "1624096158"
  },
  {
    "id": 45,
    "canonicalId": "bez3gdrhgqlgsn7s9mh402mv",
    "datasetId": "bez3gdrhgqlgsn7s9mh402mv",
    "doi": "10.5061/dryad.nh0fp1b",
    "title": "Data from: Human foveal cone photoreceptor topography and its dependence on eye length",
    "description": "We provide the first measures of foveal cone density as a function of axial length in living eyes and discuss the physical and visual implications of our findings. We used a new generation Adaptive Optics Scanning Laser Ophthalmoscope to image cones at and near the fovea in 28 eyes of 16 subjects. Cone density and other metrics were computed in units of visual angle and linear retinal units. The foveal cone mosaic in longer eyes is expanded at the fovea, but not in proportion to eye length. Despite retinal stretching (decrease in cones/mm2), myopes generally have a higher angular sampling density (increase in cones/deg2) in and around the fovea compared to emmetropes, offering the potential for better visual acuity. Reports of deficits in best-corrected foveal vision in myopes compared to emmetropes cannot be explained by increased spacing between photoreceptors caused by retinal stretching during myopic progression.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "We provide the first measures of foveal cone density as a function of axial length in living eyes and discuss the physical and visual implications of our findings. We used a new generation Adaptive Optics Scanning Laser Ophthalmoscope to image cones at and near the fovea in 28 eyes of 16 subjects. Cone density and other metrics were computed in units of visual angle and linear retinal units. The foveal cone mosaic in longer eyes is expanded at the fovea, but not in proportion to eye length. Despite retinal stretching (decrease in cones/mm2), myopes generally have a higher angular sampling density (increase in cones/deg2) in and around the fovea compared to emmetropes, offering the potential for better visual acuity. Reports of deficits in best-corrected foveal vision in myopes compared to emmetropes cannot be explained by increased spacing between photoreceptors caused by retinal stretching during myopic progression.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.nh0fp1b",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Human foveal cone photoreceptor topography and its dependence on eye length"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Wang, Yiyi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Bensaid, Nicolas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Tiruveedhula, Pavan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Ma, Jianqiang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Ravikumar, Sowmya",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Roorda, Austin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2019",
        "date": [
          {
            "dateValue": "2019-08-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "We provide the first measures of foveal cone density as a function of axial length in living eyes and discuss the physical and visual implications of our findings. We used a new generation Adaptive Optics Scanning Laser Ophthalmoscope to image cones at and near the fovea in 28 eyes of 16 subjects. Cone density and other metrics were computed in units of visual angle and linear retinal units. The foveal cone mosaic in longer eyes is expanded at the fovea, but not in proportion to eye length. Despite retinal stretching (decrease in cones/mm2), myopes generally have a higher angular sampling density (increase in cones/deg2) in and around the fovea compared to emmetropes, offering the potential for better visual acuity. Reports of deficits in best-corrected foveal vision in myopes compared to emmetropes cannot be explained by increased spacing between photoreceptors caused by retinal stretching during myopic progression.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "31.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 33239859,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4994030",
    "created": "1624121423"
  },
  {
    "id": 46,
    "canonicalId": "tpynvbamu4eljvkv1v7ijpej",
    "datasetId": "tpynvbamu4eljvkv1v7ijpej",
    "doi": "10.7272/Q6RV0KZ3",
    "title": "Asymmetric retinal direction tuning predicts optokinetic eye movements across stimulus conditions",
    "description": "<p>Across species, the optokinetic reflex (OKR) stabilizes vision during self-motion. OKR occurs when ON direction-selective retinal ganglion cells (oDSGCs) detect slow, global image motion on the retina. How oDSGC activity is integrated centrally to generate behavior remains unknown. Here, we discover mechanisms that contribute to motion encoding in vertically-tuned oDSGCs and leverage these findings to empirically define signal transformation between retinal output and vertical OKR behavior. We demonstrate that motion encoding in vertically-tuned oDSGCs is contrast-sensitive and asymmetric for oDSGC types that prefer opposite directions. These phenomena arise from the interplay between spike threshold nonlinearities and differences in synaptic input weights, including shifts in the balance of excitation and inhibition. In behaving mice, these neurophysiological observations, along with a central subtraction of oDSGC outputs, accurately predict the trajectories of vertical OKR across stimulus conditions. Thus, asymmetric tuning across competing sensory channels can critically shape behavior. Available here are the data associated with these findings.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Across species, the optokinetic reflex (OKR) stabilizes vision during self-motion. OKR occurs when ON direction-selective retinal ganglion cells (oDSGCs) detect slow, global image motion on the retina. How oDSGC activity is integrated centrally to generate behavior remains unknown. Here, we discover mechanisms that contribute to motion encoding in vertically-tuned oDSGCs and leverage these findings to empirically define signal transformation between retinal output and vertical OKR behavior. We demonstrate that motion encoding in vertically-tuned oDSGCs is contrast-sensitive and asymmetric for oDSGC types that prefer opposite directions. These phenomena arise from the interplay between spike threshold nonlinearities and differences in synaptic input weights, including shifts in the balance of excitation and inhibition. In behaving mice, these neurophysiological observations, along with a central subtraction of oDSGC outputs, accurately predict the trajectories of vertical OKR across stimulus conditions. Thus, asymmetric tuning across competing sensory channels can critically shape behavior. Available here are the data associated with these findings.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.7272/Q6RV0KZ3",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Asymmetric retinal direction tuning predicts optokinetic eye movements across stimulus conditions"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Harris, Scott",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, San Francisco"
              }
            ]
          },
          {
            "creatorName": "Dunn, Felice",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, San Francisco"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-02-27",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Across species, the optokinetic reflex (OKR) stabilizes vision during self-motion. OKR occurs when ON direction-selective retinal ganglion cells (oDSGCs) detect slow, global image motion on the retina. How oDSGC activity is integrated centrally to generate behavior remains unknown. Here, we discover mechanisms that contribute to motion encoding in vertically-tuned oDSGCs and leverage these findings to empirically define signal transformation between retinal output and vertical OKR behavior. We demonstrate that motion encoding in vertically-tuned oDSGCs is contrast-sensitive and asymmetric for oDSGC types that prefer opposite directions. These phenomena arise from the interplay between spike threshold nonlinearities and differences in synaptic input weights, including shifts in the balance of excitation and inhibition. In behaving mice, these neurophysiological observations, along with a central subtraction of oDSGC outputs, accurately predict the trajectories of vertical OKR across stimulus conditions. Thus, asymmetric tuning across competing sensory channels can critically shape behavior. Available here are the data associated with these findings.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "optokinetic nystagmus"
          },
          {
            "subjectValue": "accessory optic system"
          },
          {
            "subjectValue": "direction selective"
          },
          {
            "subjectValue": "retinal ganglion cells"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "660.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 692689305,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7683399",
    "created": "1677551580"
  },
  {
    "id": 47,
    "canonicalId": "utl1qsnug6rtn9sek7y5b4jr",
    "datasetId": "utl1qsnug6rtn9sek7y5b4jr",
    "doi": "10.5061/dryad.xsj3tx9gx",
    "title": "Datasets for neuronal imaging, extracellular recordings and behavioral rig code",
    "description": "<p>Rod and cone photoreceptors degenerate in retinitis pigmentosa (RP). While downstream neurons survive, they undergo physiological changes, including accelerated spontaneous firing in retinal ganglion cells (RGCs). Retinoic acid (RA) is the molecular trigger of RGC hyperactivity, but whether this interferes with visual perception is unknown. Here we show that inhibiting RA synthesis with disulfiram, a deterrent of human alcohol abuse, improves behavioral image detection in vision-impaired mice. <i>In vivo</i> Ca<sup>2+</sup> imaging shows that disulfiram sharpens orientation-tuning of visual cortical neurons and strengthens fidelity of responses to natural scenes. An RA receptor inhibitor also reduces RGC hyperactivity, sharpens cortical representations, and improves image detection. These findings suggest that photoreceptor degeneration is not the only cause of vision loss in RP. RA-induced corruption of retinal information processing also degrades vision, pointing to RA synthesis and signaling inhibitors as potential therapeutic tools for improving sight in RP and other retinal degenerative disorders.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Rod and cone photoreceptors degenerate in retinitis pigmentosa (RP). While downstream neurons survive, they undergo physiological changes, including accelerated spontaneous firing in retinal ganglion cells (RGCs). Retinoic acid (RA) is the molecular trigger of RGC hyperactivity, but whether this interferes with visual perception is unknown. Here we show that inhibiting RA synthesis with disulfiram, a deterrent of human alcohol abuse, improves behavioral image detection in vision-impaired mice. <i>In vivo</i> Ca<sup>2+</sup> imaging shows that disulfiram sharpens orientation-tuning of visual cortical neurons and strengthens fidelity of responses to natural scenes. An RA receptor inhibitor also reduces RGC hyperactivity, sharpens cortical representations, and improves image detection. These findings suggest that photoreceptor degeneration is not the only cause of vision loss in RP. RA-induced corruption of retinal information processing also degrades vision, pointing to RA synthesis and signaling inhibitors as potential therapeutic tools for improving sight in RP and other retinal degenerative disorders.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.xsj3tx9gx",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Datasets for neuronal imaging, extracellular recordings and behavioral rig code"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Telias, Michael",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Sit, Kevin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Santa Barbara*"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-02-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Rod and cone photoreceptors degenerate in retinitis pigmentosa (RP). While downstream neurons survive, they undergo physiological changes, including accelerated spontaneous firing in retinal ganglion cells (RGCs). Retinoic acid (RA) is the molecular trigger of RGC hyperactivity, but whether this interferes with visual perception is unknown. Here we show that inhibiting RA synthesis with disulfiram, a deterrent of human alcohol abuse, improves behavioral image detection in vision-impaired mice. <i>In vivo</i> Ca<sup>2+</sup> imaging shows that disulfiram sharpens orientation-tuning of visual cortical neurons and strengthens fidelity of responses to natural scenes. An RA receptor inhibitor also reduces RGC hyperactivity, sharpens cortical representations, and improves image detection. These findings suggest that photoreceptor degeneration is not the only cause of vision loss in RP. RA-induced corruption of retinal information processing also degrades vision, pointing to RA synthesis and signaling inhibitors as potential therapeutic tools for improving sight in RP and other retinal degenerative disorders.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Cellular neuroscience"
          },
          {
            "subjectValue": "Molecular neuroscience"
          },
          {
            "subjectValue": "Inner retina"
          },
          {
            "subjectValue": "retinal degeneration"
          },
          {
            "subjectValue": "Aceria nervisequa faginea (Nalepa 1920)"
          },
          {
            "subjectValue": "in vivo calcium imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1669.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1750912204,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6207299",
    "created": "1645463202"
  },
  {
    "id": 48,
    "canonicalId": "x9f218r4rrqjr0hjitxq5hko",
    "datasetId": "x9f218r4rrqjr0hjitxq5hko",
    "doi": "10.5281/zenodo.17167948",
    "title": "Prototypes and Datasets for Retinal Regeneration: PSI and CELL-MPC Proof-of-Concept",
    "description": "<p>Este reposit&oacute;rio cont&eacute;m os materiais suplementares do artigo <em>&ldquo;Integra&ccedil;&atilde;o de Modelagem Computacional, Biologia de Sistemas e Intelig&ecirc;ncia Artificial para Melhorar a Efici&ecirc;ncia de Diferencia&ccedil;&atilde;o das C&eacute;lulas-Tronco Pluripotentes Induzidas em C&eacute;lulas Retinianas&rdquo;</em>. Inclui:<br>(i) o script Python <strong>psi_simulation_prototype.py</strong>, que calcula os componentes do PSI e aplica um controlador simplificado;<br>(ii) a figura <strong>psi_demo_plot.png</strong>;<br>(iii) o modelo g&ecirc;nico <strong>retina_toggle_switch_sbml.xml</strong> (SBML, compat&iacute;vel com CellDesigner);<br>(iv) arquivos CSV de resultados consolidados (<strong>consolidated_summary.csv</strong>, <strong>summary_*.csv</strong>, <strong>lotemedian_*.csv</strong>).<br>Esses materiais permitem reproduzir a prova de conceito in silico, assegurando transpar&ecirc;ncia metodol&oacute;gica e reuso cient&iacute;fico.</p>",
    "versionTitle": "1.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Este reposit&oacute;rio cont&eacute;m os materiais suplementares do artigo <em>&ldquo;Integra&ccedil;&atilde;o de Modelagem Computacional, Biologia de Sistemas e Intelig&ecirc;ncia Artificial para Melhorar a Efici&ecirc;ncia de Diferencia&ccedil;&atilde;o das C&eacute;lulas-Tronco Pluripotentes Induzidas em C&eacute;lulas Retinianas&rdquo;</em>. Inclui:<br>(i) o script Python <strong>psi_simulation_prototype.py</strong>, que calcula os componentes do PSI e aplica um controlador simplificado;<br>(ii) a figura <strong>psi_demo_plot.png</strong>;<br>(iii) o modelo g&ecirc;nico <strong>retina_toggle_switch_sbml.xml</strong> (SBML, compat&iacute;vel com CellDesigner);<br>(iv) arquivos CSV de resultados consolidados (<strong>consolidated_summary.csv</strong>, <strong>summary_*.csv</strong>, <strong>lotemedian_*.csv</strong>).<br>Esses materiais permitem reproduzir a prova de conceito in silico, assegurando transpar&ecirc;ncia metodol&oacute;gica e reuso cient&iacute;fico.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17167948",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Prototypes and Datasets for Retinal Regeneration: PSI and CELL-MPC Proof-of-Concept"
          }
        ],
        "version": "1.0.0",
        "creator": [
          {
            "creatorName": "Mantovani da Silva, Luiz Ricardo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Este reposit&oacute;rio cont&eacute;m os materiais suplementares do artigo <em>&ldquo;Integra&ccedil;&atilde;o de Modelagem Computacional, Biologia de Sistemas e Intelig&ecirc;ncia Artificial para Melhorar a Efici&ecirc;ncia de Diferencia&ccedil;&atilde;o das C&eacute;lulas-Tronco Pluripotentes Induzidas em C&eacute;lulas Retinianas&rdquo;</em>. Inclui:<br>(i) o script Python <strong>psi_simulation_prototype.py</strong>, que calcula os componentes do PSI e aplica um controlador simplificado;<br>(ii) a figura <strong>psi_demo_plot.png</strong>;<br>(iii) o modelo g&ecirc;nico <strong>retina_toggle_switch_sbml.xml</strong> (SBML, compat&iacute;vel com CellDesigner);<br>(iv) arquivos CSV de resultados consolidados (<strong>consolidated_summary.csv</strong>, <strong>summary_*.csv</strong>, <strong>lotemedian_*.csv</strong>).<br>Esses materiais permitem reproduzir a prova de conceito in silico, assegurando transpar&ecirc;ncia metodol&oacute;gica e reuso cient&iacute;fico.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "induced pluripotent stem cells, retina regeneration, computational modeling, systems biology, artificial intelligence, PSI, CELL-MPC, regenerative medicine, SBML, in silico"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "5.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 5767168,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17167948",
    "created": "1758412563"
  },
  {
    "id": 49,
    "canonicalId": "uxy6y7b5je4vpqhwste3zelf",
    "datasetId": "uxy6y7b5je4vpqhwste3zelf",
    "doi": "10.5068/D1ZD5S",
    "title": "Single cell transcriptomic analyses reveal the impact of bHLH factors on human retinal organoid development",
    "description": "<p>The developing retina expresses multiple bHLH transcription factors.\u00a0Their precise functions and interactions in uncommitted retinal progenitors remain to be fully elucidated.\u00a0Here, we investigate the roles of bHLH factors ATOH7 and Neurog2 in human ES cell-derived retinal organoids.\u00a0 Single-cell transcriptome analyses identify three states of proliferating retinal progenitors: pre-neurogenic, neurogenic, and cell cycle-exiting progenitors.\u00a0Each shows different expression profile of bHLH factors.\u00a0The cell cycle-exiting progenitors feed into a postmitotic heterozygous neuroblast pool that gives rise to early born neuronal lineages.\u00a0Elevating ATOH7 or Neurog2 expression accelerates the transition from the pre-neurogenic to the neurogenic state, and expands the exiting progenitor and neuroblast populations.\u00a0In addition, ATOH7 and Neurog2 significantly, yet differentially, enhance retinal ganglion cell and cone photoreceptor production.\u00a0Moreover, single-cell transcriptome analyses reveal that ATOH7 and Neurog2 assert positive autoregulation, suppress key bHLH factors associated with the neurogenic progenitors, and elevate bHLH factors expressed by exiting progenitors and differentiating neuroblasts.\u00a0This study thus provides novel insight regarding how ATOH7 and Neurog2 impact human retinal progenitor behaviors and neuroblast fate choices.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The developing retina expresses multiple bHLH transcription factors.\u00a0Their precise functions and interactions in uncommitted retinal progenitors remain to be fully elucidated.\u00a0Here, we investigate the roles of bHLH factors ATOH7 and Neurog2 in human ES cell-derived retinal organoids.\u00a0 Single-cell transcriptome analyses identify three states of proliferating retinal progenitors: pre-neurogenic, neurogenic, and cell cycle-exiting progenitors.\u00a0Each shows different expression profile of bHLH factors.\u00a0The cell cycle-exiting progenitors feed into a postmitotic heterozygous neuroblast pool that gives rise to early born neuronal lineages.\u00a0Elevating ATOH7 or Neurog2 expression accelerates the transition from the pre-neurogenic to the neurogenic state, and expands the exiting progenitor and neuroblast populations.\u00a0In addition, ATOH7 and Neurog2 significantly, yet differentially, enhance retinal ganglion cell and cone photoreceptor production.\u00a0Moreover, single-cell transcriptome analyses reveal that ATOH7 and Neurog2 assert positive autoregulation, suppress key bHLH factors associated with the neurogenic progenitors, and elevate bHLH factors expressed by exiting progenitors and differentiating neuroblasts.\u00a0This study thus provides novel insight regarding how ATOH7 and Neurog2 impact human retinal progenitor behaviors and neuroblast fate choices.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5068/D1ZD5S",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Single cell transcriptomic analyses reveal the impact of bHLH factors on human retinal organoid development"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Yang, Xian-Jie",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Zhang, Xiangmei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Mandric, Igor",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Nguyen, Kevin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Nguyen, Thao",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Pellegrini, Matteo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Grove, James",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Barnes, Steven",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Los Angeles"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-04-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The developing retina expresses multiple bHLH transcription factors.\u00a0Their precise functions and interactions in uncommitted retinal progenitors remain to be fully elucidated.\u00a0Here, we investigate the roles of bHLH factors ATOH7 and Neurog2 in human ES cell-derived retinal organoids.\u00a0 Single-cell transcriptome analyses identify three states of proliferating retinal progenitors: pre-neurogenic, neurogenic, and cell cycle-exiting progenitors.\u00a0Each shows different expression profile of bHLH factors.\u00a0The cell cycle-exiting progenitors feed into a postmitotic heterozygous neuroblast pool that gives rise to early born neuronal lineages.\u00a0Elevating ATOH7 or Neurog2 expression accelerates the transition from the pre-neurogenic to the neurogenic state, and expands the exiting progenitor and neuroblast populations.\u00a0In addition, ATOH7 and Neurog2 significantly, yet differentially, enhance retinal ganglion cell and cone photoreceptor production.\u00a0Moreover, single-cell transcriptome analyses reveal that ATOH7 and Neurog2 assert positive autoregulation, suppress key bHLH factors associated with the neurogenic progenitors, and elevate bHLH factors expressed by exiting progenitors and differentiating neuroblasts.\u00a0This study thus provides novel insight regarding how ATOH7 and Neurog2 impact human retinal progenitor behaviors and neuroblast fate choices.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "41.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 43935334,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4707752",
    "created": "1619018928"
  },
  {
    "id": 50,
    "canonicalId": "iyfchc5hn531yv9h0kn3ibtq",
    "datasetId": "iyfchc5hn531yv9h0kn3ibtq",
    "doi": "10.5281/zenodo.12703870",
    "title": "Datasets relating to characterisation of Jumping Spider Rhodopsin-1 after reconstitution with retinal and retinal analogues",
    "description": "<p>001 - UV-Vis spectra JSR1 reconstituted with all-trans retinal, 9-cis retinal, ATR6.11 and 9CR6.11</p>\n<p>002 - UV-Vis spectra of JSR1 reconstituted with ATR6.11 and 9CR6.11, before and after acid denaturation</p>\n<p>003 - UV-Vis spectra of JSR1 reconstituted with ATR6.11, after addition of hydroxylamine</p>\n<p>004 - UV-Vis spectra of JSR1 reconstituted with 9CR6.11, after addition of hydroxylamine</p>\n<p>005 - UV-Vis spectra of illuminated JSR/ATR6.11 and JSR/9CR6.11</p>\n<p>006 - GTPase Glo Assay data measuring catalysis of nucleotide exchange in human G protein heterotrimers by JSR1/ATR6.11, JSR/9CR6.11 and JSR/9-cis retinal, with and without illumination</p>\n<p>007 - Size exclusion chromatograms JSR1/ATR6.11 after incubations with human Gi or human Gq heterotrimers</p>\n<p>008 - SDS-PAGE analysis of size exclusion chromatography peaks with JSR/ATR6.11 and human Gi or human Gq heterotrimers</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>001 - UV-Vis spectra JSR1 reconstituted with all-trans retinal, 9-cis retinal, ATR6.11 and 9CR6.11</p>\n<p>002 - UV-Vis spectra of JSR1 reconstituted with ATR6.11 and 9CR6.11, before and after acid denaturation</p>\n<p>003 - UV-Vis spectra of JSR1 reconstituted with ATR6.11, after addition of hydroxylamine</p>\n<p>004 - UV-Vis spectra of JSR1 reconstituted with 9CR6.11, after addition of hydroxylamine</p>\n<p>005 - UV-Vis spectra of illuminated JSR/ATR6.11 and JSR/9CR6.11</p>\n<p>006 - GTPase Glo Assay data measuring catalysis of nucleotide exchange in human G protein heterotrimers by JSR1/ATR6.11, JSR/9CR6.11 and JSR/9-cis retinal, with and without illumination</p>\n<p>007 - Size exclusion chromatograms JSR1/ATR6.11 after incubations with human Gi or human Gq heterotrimers</p>\n<p>008 - SDS-PAGE analysis of size exclusion chromatography peaks with JSR/ATR6.11 and human Gi or human Gq heterotrimers</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.12703870",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Datasets relating to characterisation of Jumping Spider Rhodopsin-1 after reconstitution with retinal and retinal analogues"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Rodrigues, Matthew",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Paul Scherrer Institute"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-07-10",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>001 - UV-Vis spectra JSR1 reconstituted with all-trans retinal, 9-cis retinal, ATR6.11 and 9CR6.11</p>\n<p>002 - UV-Vis spectra of JSR1 reconstituted with ATR6.11 and 9CR6.11, before and after acid denaturation</p>\n<p>003 - UV-Vis spectra of JSR1 reconstituted with ATR6.11, after addition of hydroxylamine</p>\n<p>004 - UV-Vis spectra of JSR1 reconstituted with 9CR6.11, after addition of hydroxylamine</p>\n<p>005 - UV-Vis spectra of illuminated JSR/ATR6.11 and JSR/9CR6.11</p>\n<p>006 - GTPase Glo Assay data measuring catalysis of nucleotide exchange in human G protein heterotrimers by JSR1/ATR6.11, JSR/9CR6.11 and JSR/9-cis retinal, with and without illumination</p>\n<p>007 - Size exclusion chromatograms JSR1/ATR6.11 after incubations with human Gi or human Gq heterotrimers</p>\n<p>008 - SDS-PAGE analysis of size exclusion chromatography peaks with JSR/ATR6.11 and human Gi or human Gq heterotrimers</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1153433,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/12703870",
    "created": "1720598766"
  },
  {
    "id": 51,
    "canonicalId": "teusoxgcktml5ucwxi0a4as8",
    "datasetId": "teusoxgcktml5ucwxi0a4as8",
    "doi": "10.5061/dryad.1f1rc",
    "title": "Data from: Error-robust modes of the retinal population code",
    "description": "Across the nervous system, certain population spiking patterns are observed far more frequently than others. A hypothesis about this structure is that these collective activity patterns function as population codewords\u2013collective modes\u2013carrying information distinct from that of any single cell. We investigate this phenomenon in recordings of \u223c150 retinal ganglion cells, the retina's output. We develop a novel statistical model that decomposes the population response into modes; it predicts the distribution of spiking activity in the ganglion cell population with high accuracy. We found that the modes represent localized features of the visual stimulus that are distinct from the features represented by single neurons. Modes form clusters of activity states that are readily discriminated from one another. When we repeated the same visual stimulus, we found that the same mode was robustly elicited. These results suggest that retinal ganglion cells' collective signaling is endowed with a form of error-correcting code\u2013a principle that may hold in brain areas beyond retina.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Across the nervous system, certain population spiking patterns are observed far more frequently than others. A hypothesis about this structure is that these collective activity patterns function as population codewords\u2013collective modes\u2013carrying information distinct from that of any single cell. We investigate this phenomenon in recordings of \u223c150 retinal ganglion cells, the retina's output. We develop a novel statistical model that decomposes the population response into modes; it predicts the distribution of spiking activity in the ganglion cell population with high accuracy. We found that the modes represent localized features of the visual stimulus that are distinct from the features represented by single neurons. Modes form clusters of activity states that are readily discriminated from one another. When we repeated the same visual stimulus, we found that the same mode was robustly elicited. These results suggest that retinal ganglion cells' collective signaling is endowed with a form of error-correcting code\u2013a principle that may hold in brain areas beyond retina.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.1f1rc",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Error-robust modes of the retinal population code"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Loback, Adrianna R.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Princeton University"
              }
            ]
          },
          {
            "creatorName": "Tka\u010dik, Ga\u0161per",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Science and Technology Austria"
              }
            ]
          },
          {
            "creatorName": "Prentice, Jason S.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Princeton University"
              }
            ]
          },
          {
            "creatorName": "Ioffe, Mark L.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Princeton University"
              }
            ]
          },
          {
            "creatorName": "Berry II, Michael J.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Marre, Olivier",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institut de la Vision"
              }
            ]
          },
          {
            "creatorName": "Berry, Michael J.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Princeton University"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-10-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Across the nervous system, certain population spiking patterns are observed far more frequently than others. A hypothesis about this structure is that these collective activity patterns function as population codewords\u2013collective modes\u2013carrying information distinct from that of any single cell. We investigate this phenomenon in recordings of \u223c150 retinal ganglion cells, the retina's output. We develop a novel statistical model that decomposes the population response into modes; it predicts the distribution of spiking activity in the ganglion cell population with high accuracy. We found that the modes represent localized features of the visual stimulus that are distinct from the features represented by single neurons. Modes form clusters of activity states that are readily discriminated from one another. When we repeated the same visual stimulus, we found that the same mode was robustly elicited. These results suggest that retinal ganglion cells' collective signaling is endowed with a form of error-correcting code\u2013a principle that may hold in brain areas beyond retina.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Multielectrode array"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "Retinal ganglion cell"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "83.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 87556096,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4976473",
    "created": "1623959728"
  },
  {
    "id": 52,
    "canonicalId": "a4qjuw2l7blyfh9wi40ldwbl",
    "datasetId": "a4qjuw2l7blyfh9wi40ldwbl",
    "doi": "10.5281/zenodo.17103009",
    "title": "Minimal dataset for \"Cellular-resolution OCT reveals layer-specific retinal mosaics and ganglion cell degeneration in mouse retina in vivo\"",
    "description": "<p>This dataset provides the minimal data underlying the findings of the manuscript submitted to Neurophotonics:<br>Cellular-resolution OCT reveals layer-specific retinal mosaics and ganglion cell degeneration in mouse retina in vivo.</p>\n<p>Contents include:</p>\n<p>- Data_Summary.xlsx: raw numerical values supporting Figures (image quality metrics, speckle contrast, SSIM, cross-correlation, RGC soma counts, GCC thickness).</p>\n<p>- Figures folder: original figure panels in TIFF format and a supplementary movie (MPEG).</p>\n<p>- Images folder: representative OCT images, raw and averaged OCT volumes, layer-optimized mosaics, longitudinal ONC datasets, and corresponding immunohistochemistry images.</p>\n<p>Code availability: multi-volume registration and averaging pipeline available at https://github.com/unblindness/oct_alignment</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This dataset provides the minimal data underlying the findings of the manuscript submitted to Neurophotonics:<br>Cellular-resolution OCT reveals layer-specific retinal mosaics and ganglion cell degeneration in mouse retina in vivo.</p>\n<p>Contents include:</p>\n<p>- Data_Summary.xlsx: raw numerical values supporting Figures (image quality metrics, speckle contrast, SSIM, cross-correlation, RGC soma counts, GCC thickness).</p>\n<p>- Figures folder: original figure panels in TIFF format and a supplementary movie (MPEG).</p>\n<p>- Images folder: representative OCT images, raw and averaged OCT volumes, layer-optimized mosaics, longitudinal ONC datasets, and corresponding immunohistochemistry images.</p>\n<p>Code availability: multi-volume registration and averaging pipeline available at https://github.com/unblindness/oct_alignment</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17103009",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Minimal dataset for \"Cellular-resolution OCT reveals layer-specific retinal mosaics and ganglion cell degeneration in mouse retina in vivo\""
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kim, Tae-Hoon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Genentech"
              }
            ]
          },
          {
            "creatorName": "Weimer, Robby",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Genentech"
              }
            ]
          },
          {
            "creatorName": "Elstrott, Justin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Genentech"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-12",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This dataset provides the minimal data underlying the findings of the manuscript submitted to Neurophotonics:<br>Cellular-resolution OCT reveals layer-specific retinal mosaics and ganglion cell degeneration in mouse retina in vivo.</p>\n<p>Contents include:</p>\n<p>- Data_Summary.xlsx: raw numerical values supporting Figures (image quality metrics, speckle contrast, SSIM, cross-correlation, RGC soma counts, GCC thickness).</p>\n<p>- Figures folder: original figure panels in TIFF format and a supplementary movie (MPEG).</p>\n<p>- Images folder: representative OCT images, raw and averaged OCT volumes, layer-optimized mosaics, longitudinal ONC datasets, and corresponding immunohistochemistry images.</p>\n<p>Code availability: multi-volume registration and averaging pipeline available at https://github.com/unblindness/oct_alignment</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "OCT, retina, adaptive optics, retinal ganglion cells, optic nerve crush, Neurophotonics"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "7036.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 7378305024,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17103009",
    "created": "1757651051"
  },
  {
    "id": 53,
    "canonicalId": "h2m600qgvl77zpecia6nfkws",
    "datasetId": "h2m600qgvl77zpecia6nfkws",
    "doi": "10.5061/dryad.163mp",
    "title": "Data from: Rat retinal vasomotion assessed by laser speckle imaging",
    "description": "Vasomotion is spontaneous or induced rhythmic changes in vascular tone or vessel diameter that lead to rhythmic changes in flow. While the vascular research community debates the physiological and pathophysiological consequence of vasomotion, there is a great need for experimental techniques that can address the role and dynamical properties of vasomotion in vivo. We apply laser speckle imaging  to study spontaneous and drug induced vasomotion in retinal network of anesthetized rats. The results reveal a wide variety of dynamical patterns. Wavelet-based analysis shows that (i) spontaneous vasomotion occurs in anesthetized animals and (ii) vasomotion can be initiated by systemic administration of the thromboxane analogue U-46619 and the nitric-oxide donor S-nitroso-acetylDL-penicillamine (SNAP). Although these drugs activate different cellular pathways responsible for vasomotion, our approach can track the dynamical changes they cause.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Vasomotion is spontaneous or induced rhythmic changes in vascular tone or vessel diameter that lead to rhythmic changes in flow. While the vascular research community debates the physiological and pathophysiological consequence of vasomotion, there is a great need for experimental techniques that can address the role and dynamical properties of vasomotion in vivo. We apply laser speckle imaging  to study spontaneous and drug induced vasomotion in retinal network of anesthetized rats. The results reveal a wide variety of dynamical patterns. Wavelet-based analysis shows that (i) spontaneous vasomotion occurs in anesthetized animals and (ii) vasomotion can be initiated by systemic administration of the thromboxane analogue U-46619 and the nitric-oxide donor S-nitroso-acetylDL-penicillamine (SNAP). Although these drugs activate different cellular pathways responsible for vasomotion, our approach can track the dynamical changes they cause.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.163mp",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Rat retinal vasomotion assessed by laser speckle imaging"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Neganova, Anastasiia Y.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Copenhagen"
              }
            ]
          },
          {
            "creatorName": "Postnov, Dmitry D.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Copenhagen"
              }
            ]
          },
          {
            "creatorName": "Sosnovtseva, Olga",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Copenhagen"
              }
            ]
          },
          {
            "creatorName": "Jacobsen, Jens Christian Brings",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Copenhagen"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-03-14",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Vasomotion is spontaneous or induced rhythmic changes in vascular tone or vessel diameter that lead to rhythmic changes in flow. While the vascular research community debates the physiological and pathophysiological consequence of vasomotion, there is a great need for experimental techniques that can address the role and dynamical properties of vasomotion in vivo. We apply laser speckle imaging  to study spontaneous and drug induced vasomotion in retinal network of anesthetized rats. The results reveal a wide variety of dynamical patterns. Wavelet-based analysis shows that (i) spontaneous vasomotion occurs in anesthetized animals and (ii) vasomotion can be initiated by systemic administration of the thromboxane analogue U-46619 and the nitric-oxide donor S-nitroso-acetylDL-penicillamine (SNAP). Although these drugs activate different cellular pathways responsible for vasomotion, our approach can track the dynamical changes they cause.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "laser speckle"
          },
          {
            "subjectValue": "rat"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "vessels"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1059.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1111175987,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4962379",
    "created": "1623812801"
  },
  {
    "id": 54,
    "canonicalId": "onnwsooe46pf1kyvpmth9lun",
    "datasetId": "onnwsooe46pf1kyvpmth9lun",
    "doi": "10.5281/zenodo.14926762",
    "title": "Dataset: FAIR AMD OCT Datasets Paper",
    "description": "<p>This is the dataset associated with the paper titled \"Publicly Available Imaging Datasets for Age-related Macular Degeneration: Evaluation according to the Findable, Accessible, Interoperable, Reproducible (FAIR) Principles\". Age-related macular degeneration (AMD), a leading cause of vision loss among older adults, affects more than 200 million people worldwide. In this paper, We evaluated openly available AMD-related datasets containing optical coherence tomography (OCT) data against the FAIR principles. This is an archive of the repository that contains the data related to our evaluation. See this&nbsp;<a href=\"https://github.com/fairdataihub/FAIR-AMD-OCT-paper-inventory\">inventory</a> for all related resources, including the paper. This dataset is maintained from https://github.com/fairdataihub/FAIR-AMD-OCT-paper-dataset.&nbsp;</p>",
    "versionTitle": "1.2.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This is the dataset associated with the paper titled \"Publicly Available Imaging Datasets for Age-related Macular Degeneration: Evaluation according to the Findable, Accessible, Interoperable, Reproducible (FAIR) Principles\". Age-related macular degeneration (AMD), a leading cause of vision loss among older adults, affects more than 200 million people worldwide. In this paper, We evaluated openly available AMD-related datasets containing optical coherence tomography (OCT) data against the FAIR principles. This is an archive of the repository that contains the data related to our evaluation. See this&nbsp;<a href=\"https://github.com/fairdataihub/FAIR-AMD-OCT-paper-inventory\">inventory</a> for all related resources, including the paper. This dataset is maintained from https://github.com/fairdataihub/FAIR-AMD-OCT-paper-dataset.&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.14926762",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset: FAIR AMD OCT Datasets Paper"
          }
        ],
        "version": "1.2.0",
        "creator": [
          {
            "creatorName": "Gim, Nayoon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Washington"
              }
            ]
          },
          {
            "creatorName": "Ferguson, Alina",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Washington"
              }
            ]
          },
          {
            "creatorName": "Blazes, Marian",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Washington"
              }
            ]
          },
          {
            "creatorName": "Soundarajan, Sanjay",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "California Medical Innovations Institute"
              }
            ]
          },
          {
            "creatorName": "Gasimova, Aydan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "California Medical Innovations Institute"
              }
            ]
          },
          {
            "creatorName": "Patel, Bhavesh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "California Medical Innovations Institute"
              }
            ]
          },
          {
            "creatorName": "Lee, Cecilia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Washington"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-02-25",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This is the dataset associated with the paper titled \"Publicly Available Imaging Datasets for Age-related Macular Degeneration: Evaluation according to the Findable, Accessible, Interoperable, Reproducible (FAIR) Principles\". Age-related macular degeneration (AMD), a leading cause of vision loss among older adults, affects more than 200 million people worldwide. In this paper, We evaluated openly available AMD-related datasets containing optical coherence tomography (OCT) data against the FAIR principles. This is an archive of the repository that contains the data related to our evaluation. See this&nbsp;<a href=\"https://github.com/fairdataihub/FAIR-AMD-OCT-paper-inventory\">inventory</a> for all related resources, including the paper. This dataset is maintained from https://github.com/fairdataihub/FAIR-AMD-OCT-paper-dataset.&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "FAIR"
          },
          {
            "subjectValue": "AMD"
          },
          {
            "subjectValue": "OCT"
          },
          {
            "subjectValue": "Imaging"
          },
          {
            "subjectValue": "Data"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 524288,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/14926762",
    "created": "1740509989"
  },
  {
    "id": 55,
    "canonicalId": "l9c41eska6t3jhjw86fo1770",
    "datasetId": "l9c41eska6t3jhjw86fo1770",
    "doi": "10.5281/zenodo.5084941",
    "title": "Feedback from retinal ganglion cells to the inner retina",
    "description": "<p>This repository contains the data files and codes used in the following paper:</p>\n\n<p>Feedback from retinal ganglion cells to the inner retina<br>\nAnastasiia Vlasiuk, Hiroki Asari<br>\nPLOS One (accepted)</p>\n\n<p>The preprint is also available from bioRxiv 2020.08.30.274514.</p>",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This repository contains the data files and codes used in the following paper:</p>\n\n<p>Feedback from retinal ganglion cells to the inner retina<br>\nAnastasiia Vlasiuk, Hiroki Asari<br>\nPLOS One (accepted)</p>\n\n<p>The preprint is also available from bioRxiv 2020.08.30.274514.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.5084941",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Feedback from retinal ganglion cells to the inner retina"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Vlasiuk, Anastasiia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "EMBL Rome"
              }
            ]
          },
          {
            "creatorName": "Asari, Hiroki",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "EMBL Rome"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-07-04",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This repository contains the data files and codes used in the following paper:</p>\n\n<p>Feedback from retinal ganglion cells to the inner retina<br>\nAnastasiia Vlasiuk, Hiroki Asari<br>\nPLOS One (accepted)</p>\n\n<p>The preprint is also available from bioRxiv 2020.08.30.274514.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retinal ganglion cell"
          },
          {
            "subjectValue": "optic nerve stimulation"
          },
          {
            "subjectValue": "neural circuit modeling"
          },
          {
            "subjectValue": "gap junction"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "113.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 118489088,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5084941",
    "created": "1625835316"
  },
  {
    "id": 56,
    "canonicalId": "fql2yvih9kbdhn8sm951aqen",
    "datasetId": "fql2yvih9kbdhn8sm951aqen",
    "doi": "10.5061/dryad.7q37g6b",
    "title": "Data from: All-optical recording and stimulation of retinal neurons in vivo in retinal degeneration mice",
    "description": "Here we demonstrate the application of a method that could accelerate the development of novel therapies by allowing direct and repeatable visualization of cellular function in the living eye, to study loss of vision in animal models of retinal disease, as well as evaluate the time course of retinal function following therapeutic intervention. We use high-resolution adaptive optics scanning light ophthalmoscopy to image fluorescence from the calcium sensor GCaMP6s. In mice with photoreceptor degeneration (rd10), we measured restored visual responses in ganglion cell layer neurons expressing the red-shifted channelrhodopsin ChrimsonR over a six-week period following significant loss of visual responses. Combining a fluorescent calcium sensor, a channelrhodopsin, and adaptive optics enables all-optical stimulation and recording of retinal neurons in the living eye. Because the retina is an accessible portal to the central nervous system, our method also provides a novel non-invasive method of dissecting neuronal processing in the brain.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Here we demonstrate the application of a method that could accelerate the development of novel therapies by allowing direct and repeatable visualization of cellular function in the living eye, to study loss of vision in animal models of retinal disease, as well as evaluate the time course of retinal function following therapeutic intervention. We use high-resolution adaptive optics scanning light ophthalmoscopy to image fluorescence from the calcium sensor GCaMP6s. In mice with photoreceptor degeneration (rd10), we measured restored visual responses in ganglion cell layer neurons expressing the red-shifted channelrhodopsin ChrimsonR over a six-week period following significant loss of visual responses. Combining a fluorescent calcium sensor, a channelrhodopsin, and adaptive optics enables all-optical stimulation and recording of retinal neurons in the living eye. Because the retina is an accessible portal to the central nervous system, our method also provides a novel non-invasive method of dissecting neuronal processing in the brain.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.7q37g6b",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: All-optical recording and stimulation of retinal neurons in vivo in retinal degeneration mice"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Cheong, Soon Keen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Strazzeri, Jennifer M.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Williams, David R.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Merigan, William H.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          }
        ],
        "publicationYear": "2019",
        "date": [
          {
            "dateValue": "2019-03-16",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Here we demonstrate the application of a method that could accelerate the development of novel therapies by allowing direct and repeatable visualization of cellular function in the living eye, to study loss of vision in animal models of retinal disease, as well as evaluate the time course of retinal function following therapeutic intervention. We use high-resolution adaptive optics scanning light ophthalmoscopy to image fluorescence from the calcium sensor GCaMP6s. In mice with photoreceptor degeneration (rd10), we measured restored visual responses in ganglion cell layer neurons expressing the red-shifted channelrhodopsin ChrimsonR over a six-week period following significant loss of visual responses. Combining a fluorescent calcium sensor, a channelrhodopsin, and adaptive optics enables all-optical stimulation and recording of retinal neurons in the living eye. Because the retina is an accessible portal to the central nervous system, our method also provides a novel non-invasive method of dissecting neuronal processing in the brain.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "AOSLO"
          },
          {
            "subjectValue": "channelrhodopsin"
          },
          {
            "subjectValue": "optogenetics"
          },
          {
            "subjectValue": "Adaptive optics"
          },
          {
            "subjectValue": "vision restoration"
          },
          {
            "subjectValue": "ChrimsonR"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "211.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 221668966,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4980308",
    "created": "1624001789"
  },
  {
    "id": 57,
    "canonicalId": "nfyo8r5bbk66s5polno3zg1q",
    "datasetId": "nfyo8r5bbk66s5polno3zg1q",
    "doi": "10.5281/zenodo.7105232",
    "title": "OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics",
    "description": "<p>Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of $1268$ fundus eye images each with 49&nbsp;OCT scans, and 16&nbsp;biomarkers, along with 3&nbsp;clinical labels and a disease diagnosis of DR or DME. In total, there are 96&nbsp;eyes&#39; data averaged over a period of at least two years with each eye treated for an average of 66&nbsp;weeks and 7&nbsp;injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of $1268$ fundus eye images each with 49&nbsp;OCT scans, and 16&nbsp;biomarkers, along with 3&nbsp;clinical labels and a disease diagnosis of DR or DME. In total, there are 96&nbsp;eyes&#39; data averaged over a period of at least two years with each eye treated for an average of 66&nbsp;weeks and 7&nbsp;injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7105232",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Mohit Prabhushankar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Kiran Kokilepersaud",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Yash-yee Logan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Stephanie Trejo Corona",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Retina Consultants of Texas"
              }
            ]
          },
          {
            "creatorName": "Ghassan AlRegib",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Charles Wykoff",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Retina Consolutants of Texas"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-06-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of $1268$ fundus eye images each with 49&nbsp;OCT scans, and 16&nbsp;biomarkers, along with 3&nbsp;clinical labels and a disease diagnosis of DR or DME. In total, there are 96&nbsp;eyes&#39; data averaged over a period of at least two years with each eye treated for an average of 66&nbsp;weeks and 7&nbsp;injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Biomarkers, Clinical Data, Machine Learning, Multi-Modal Learning,"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "32348.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 33919546163,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7105232",
    "created": "1663858517"
  },
  {
    "id": 58,
    "canonicalId": "ihrahskj7349xp758628h22k",
    "datasetId": "ihrahskj7349xp758628h22k",
    "doi": "10.5281/zenodo.16744782",
    "title": "HRF-Seg+: A Multi-Structure Annotated Fundus Image Dataset with Optic Disc, Cup, Vessels, Alpha and Beta Zones",
    "description": "<p>HRF-Seg+ is an extended version of the public HRF (High-Resolution Fundus) dataset that includes pixel-wise annotated segmentation masks for five key retinal structures:&nbsp;<br>- Optic Disc (purple)<br>- Optic Cup (brown)<br>- Retinal Vessels (orange)<br>- Peripapillary Alpha Zone (blue-green)<br>- Peripapillary Beta Zone (green)</p>\n<p>The dataset includes:</p>\n<p>- Folder 1: Segmented Optic Disc masks<br>- Folder 2: Segmented Optic Cup masks<br>- Folder 3: Segmented Retinal Vessel masks<br>- Folder 4: Segmented Alpha and Beta Zones<br>- Folder 5: Merged Ground Truth masks (all structures in one image)<br>- Folder 6: Original HRF images (from: https://www5.cs.fau.de/research/data/fundus-images/)<br>- CSV file: Color legend for all masks</p>\n<p>This dataset is intended to support research in ophthalmology, glaucoma detection, and deep learning-based segmentation. All annotations were performed manually and reviewed by experts.&nbsp;</p>\n<p>Original data is sourced from the HRF dataset [Budai et al., 2013]. Segmentation annotations were added by the authors of this dataset.</p>",
    "versionTitle": "1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>HRF-Seg+ is an extended version of the public HRF (High-Resolution Fundus) dataset that includes pixel-wise annotated segmentation masks for five key retinal structures:&nbsp;<br>- Optic Disc (purple)<br>- Optic Cup (brown)<br>- Retinal Vessels (orange)<br>- Peripapillary Alpha Zone (blue-green)<br>- Peripapillary Beta Zone (green)</p>\n<p>The dataset includes:</p>\n<p>- Folder 1: Segmented Optic Disc masks<br>- Folder 2: Segmented Optic Cup masks<br>- Folder 3: Segmented Retinal Vessel masks<br>- Folder 4: Segmented Alpha and Beta Zones<br>- Folder 5: Merged Ground Truth masks (all structures in one image)<br>- Folder 6: Original HRF images (from: https://www5.cs.fau.de/research/data/fundus-images/)<br>- CSV file: Color legend for all masks</p>\n<p>This dataset is intended to support research in ophthalmology, glaucoma detection, and deep learning-based segmentation. All annotations were performed manually and reviewed by experts.&nbsp;</p>\n<p>Original data is sourced from the HRF dataset [Budai et al., 2013]. Segmentation annotations were added by the authors of this dataset.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.16744782",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "HRF-Seg+: A Multi-Structure Annotated Fundus Image Dataset with Optic Disc, Cup, Vessels, Alpha and Beta Zones"
          }
        ],
        "version": "1.0",
        "creator": [
          {
            "creatorName": "Kako, Najdavan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Duhok Polytechnic University"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-08-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>HRF-Seg+ is an extended version of the public HRF (High-Resolution Fundus) dataset that includes pixel-wise annotated segmentation masks for five key retinal structures:&nbsp;<br>- Optic Disc (purple)<br>- Optic Cup (brown)<br>- Retinal Vessels (orange)<br>- Peripapillary Alpha Zone (blue-green)<br>- Peripapillary Beta Zone (green)</p>\n<p>The dataset includes:</p>\n<p>- Folder 1: Segmented Optic Disc masks<br>- Folder 2: Segmented Optic Cup masks<br>- Folder 3: Segmented Retinal Vessel masks<br>- Folder 4: Segmented Alpha and Beta Zones<br>- Folder 5: Merged Ground Truth masks (all structures in one image)<br>- Folder 6: Original HRF images (from: https://www5.cs.fau.de/research/data/fundus-images/)<br>- CSV file: Color legend for all masks</p>\n<p>This dataset is intended to support research in ophthalmology, glaucoma detection, and deep learning-based segmentation. All annotations were performed manually and reviewed by experts.&nbsp;</p>\n<p>Original data is sourced from the HRF dataset [Budai et al., 2013]. Segmentation annotations were added by the authors of this dataset.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "HRF"
          },
          {
            "subjectValue": "Fundus Images"
          },
          {
            "subjectValue": "Retinal Segmentation"
          },
          {
            "subjectValue": "Optic Disc"
          },
          {
            "subjectValue": "Optic Cup"
          },
          {
            "subjectValue": "Alpha Zone"
          },
          {
            "subjectValue": "Beta Zone"
          },
          {
            "subjectValue": "Medical Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "3.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 3355443,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/16744782",
    "created": "1760100024"
  },
  {
    "id": 59,
    "canonicalId": "wkaioodx332s9b0g2a1xzkbp",
    "datasetId": "wkaioodx332s9b0g2a1xzkbp",
    "doi": "10.5061/dryad.zcrjdfngp",
    "title": "Data from: Behavior shapes retinal motion statistics during natural locomotion",
    "description": "<p>Walking through an environment generates retinal motion, which humans rely on to perform a variety of visual tasks. Retinal motion patterns are determined by an interconnected set of factors, including gaze location, gaze stabilization, the structure of the environment, and the walker's goals. The characteristics of these motion signals have important consequences for neural organization and behavior. However, to date, there are no empirical <em>in situ</em> measurements of how combined eye and body movements interact with real 3D environments to shape the statistics of retinal motion signals. Here, we collect measurements of the eyes, the body, and the 3D environment during locomotion. We describe properties of the resulting retinal motion patterns. We explain how these patterns are shaped by gaze location in the world, as well as by behavior, and how they may provide a template for the way motion sensitivity and receptive field properties vary across the visual field.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Walking through an environment generates retinal motion, which humans rely on to perform a variety of visual tasks. Retinal motion patterns are determined by an interconnected set of factors, including gaze location, gaze stabilization, the structure of the environment, and the walker's goals. The characteristics of these motion signals have important consequences for neural organization and behavior. However, to date, there are no empirical <em>in situ</em> measurements of how combined eye and body movements interact with real 3D environments to shape the statistics of retinal motion signals. Here, we collect measurements of the eyes, the body, and the 3D environment during locomotion. We describe properties of the resulting retinal motion patterns. We explain how these patterns are shaped by gaze location in the world, as well as by behavior, and how they may provide a template for the way motion sensitivity and receptive field properties vary across the visual field.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.zcrjdfngp",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Behavior shapes retinal motion statistics during natural locomotion"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Hayhoe, Mary",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Texas at Austin"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-04-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Walking through an environment generates retinal motion, which humans rely on to perform a variety of visual tasks. Retinal motion patterns are determined by an interconnected set of factors, including gaze location, gaze stabilization, the structure of the environment, and the walker's goals. The characteristics of these motion signals have important consequences for neural organization and behavior. However, to date, there are no empirical <em>in situ</em> measurements of how combined eye and body movements interact with real 3D environments to shape the statistics of retinal motion signals. Here, we collect measurements of the eyes, the body, and the 3D environment during locomotion. We describe properties of the resulting retinal motion patterns. We explain how these patterns are shaped by gaze location in the world, as well as by behavior, and how they may provide a template for the way motion sensitivity and receptive field properties vary across the visual field.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retinal motion statistics"
          },
          {
            "subjectValue": "natural environments"
          },
          {
            "subjectValue": "Eye movements"
          },
          {
            "subjectValue": "locomotion"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "12907.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 13534389862,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7843422",
    "created": "1681843310"
  },
  {
    "id": 60,
    "canonicalId": "ttszud87y9ftrwjr7l4tpczc",
    "datasetId": "ttszud87y9ftrwjr7l4tpczc",
    "doi": "10.5061/dryad.xpnvx0kb6",
    "title": "Data from: Oxidative stress in the retina and retinal pigment epithelium (RPE): role of aging, and DJ-1",
    "description": "<p>High levels of oxidative radicals generated by daily light exposure and high metabolic rate suggest that the antioxidant machinery of the retina and retinal pigment epithelium (RPE) is crucial for their survival. DJ-1 is a redox-sensitive protein that has been shown to have neuroprotective function in the brain in Parkinson's disease and other neurodegenerative diseases. Here, we analyzed the role of DJ-1 in the retina during oxidative stress and aging. We induced low-level oxidative stress in young (3-month-old) and old (15-month-old) C57BL/6J (WT) and DJ-1 knockout (KO) mice and evaluated effects in the RPE and retina. Absence of DJ-1 resulted in increased retinal dysfunction in response to low levels of oxidative stress. Our findings suggest that loss of DJ-1 affects the RPE antioxidant machinery, rendering it unable to combat and neutralize low-level oxidative stress, irrespective of age. Moreover, they draw a parallel to the retinal degeneration observed in AMD, where the occurrence of genetic variants may leave the retina and RPE unable to fight sustained, low-levels of oxidative stress.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>High levels of oxidative radicals generated by daily light exposure and high metabolic rate suggest that the antioxidant machinery of the retina and retinal pigment epithelium (RPE) is crucial for their survival. DJ-1 is a redox-sensitive protein that has been shown to have neuroprotective function in the brain in Parkinson's disease and other neurodegenerative diseases. Here, we analyzed the role of DJ-1 in the retina during oxidative stress and aging. We induced low-level oxidative stress in young (3-month-old) and old (15-month-old) C57BL/6J (WT) and DJ-1 knockout (KO) mice and evaluated effects in the RPE and retina. Absence of DJ-1 resulted in increased retinal dysfunction in response to low levels of oxidative stress. Our findings suggest that loss of DJ-1 affects the RPE antioxidant machinery, rendering it unable to combat and neutralize low-level oxidative stress, irrespective of age. Moreover, they draw a parallel to the retinal degeneration observed in AMD, where the occurrence of genetic variants may leave the retina and RPE unable to fight sustained, low-levels of oxidative stress.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.xpnvx0kb6",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Oxidative stress in the retina and retinal pigment epithelium (RPE): role of aging, and DJ-1"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Bonilha, Vera",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Cleveland Clinic"
              }
            ]
          },
          {
            "creatorName": "Upadhya, Mala",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Cleveland Clinic"
              }
            ]
          },
          {
            "creatorName": "Milliner, Caroline",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Cleveland Clinic"
              }
            ]
          },
          {
            "creatorName": "Bell, Brent",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-04-20",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>High levels of oxidative radicals generated by daily light exposure and high metabolic rate suggest that the antioxidant machinery of the retina and retinal pigment epithelium (RPE) is crucial for their survival. DJ-1 is a redox-sensitive protein that has been shown to have neuroprotective function in the brain in Parkinson's disease and other neurodegenerative diseases. Here, we analyzed the role of DJ-1 in the retina during oxidative stress and aging. We induced low-level oxidative stress in young (3-month-old) and old (15-month-old) C57BL/6J (WT) and DJ-1 knockout (KO) mice and evaluated effects in the RPE and retina. Absence of DJ-1 resulted in increased retinal dysfunction in response to low levels of oxidative stress. Our findings suggest that loss of DJ-1 affects the RPE antioxidant machinery, rendering it unable to combat and neutralize low-level oxidative stress, irrespective of age. Moreover, they draw a parallel to the retinal degeneration observed in AMD, where the occurrence of genetic variants may leave the retina and RPE unable to fight sustained, low-levels of oxidative stress.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "432.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 453194547,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3998075",
    "created": "1598293534"
  },
  {
    "id": 61,
    "canonicalId": "fw5chds9is0ngy2e98831ihw",
    "datasetId": "fw5chds9is0ngy2e98831ihw",
    "doi": "10.6078/D12D9F",
    "title": "Excitatory neurotransmission activates compartmentalized calcium transients in M\u00fcller glia without affecting lateral process motility",
    "description": "<p>Neural activity has been implicated in the motility and outgrowth of glial cell processes throughout the central nervous system. Here we explore this phenomenon in M\u00fcller glia, which are specialized radial astroglia that are the predominant glial type of the vertebrate retina. M\u00fcller glia extend fine filopodia-like processes into retinal synaptic layers, in similar fashion to brain astrocytes and radial glia which exhibit perisynaptic processes. Using two-photon volumetric imaging, we found that during the second postnatal week, M\u00fcller glial processes were highly dynamic, with rapid extensions and retractions that were mediated by cytoskeletal rearrangements. During this same stage of development, retinal waves led to increases in cytosolic calcium within M\u00fcller glial lateral processes and stalks. These comprised distinct calcium compartments, distinguished by variable participation in waves, timing, and sensitivity to an M1 muscarinic acetylcholine receptor antagonist. However, we found that motility of lateral processes was unaffected by the presence of pharmacological agents that enhanced or blocked wave-associated calcium transients. Finally, we found that mice lacking normal cholinergic waves in the first postnatal week also exhibited normal M\u00fcller glial process morphology. Hence, outgrowth of M\u00fcller glial lateral processes into synaptic layers is determined by factors that are independent of neuronal activity.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Neural activity has been implicated in the motility and outgrowth of glial cell processes throughout the central nervous system. Here we explore this phenomenon in M\u00fcller glia, which are specialized radial astroglia that are the predominant glial type of the vertebrate retina. M\u00fcller glia extend fine filopodia-like processes into retinal synaptic layers, in similar fashion to brain astrocytes and radial glia which exhibit perisynaptic processes. Using two-photon volumetric imaging, we found that during the second postnatal week, M\u00fcller glial processes were highly dynamic, with rapid extensions and retractions that were mediated by cytoskeletal rearrangements. During this same stage of development, retinal waves led to increases in cytosolic calcium within M\u00fcller glial lateral processes and stalks. These comprised distinct calcium compartments, distinguished by variable participation in waves, timing, and sensitivity to an M1 muscarinic acetylcholine receptor antagonist. However, we found that motility of lateral processes was unaffected by the presence of pharmacological agents that enhanced or blocked wave-associated calcium transients. Finally, we found that mice lacking normal cholinergic waves in the first postnatal week also exhibited normal M\u00fcller glial process morphology. Hence, outgrowth of M\u00fcller glial lateral processes into synaptic layers is determined by factors that are independent of neuronal activity.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.6078/D12D9F",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Excitatory neurotransmission activates compartmentalized calcium transients in M\u00fcller glia without affecting lateral process motility"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Tworig, Joshua",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Coate, Chandler",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Feller, Marla",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-01-14",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Neural activity has been implicated in the motility and outgrowth of glial cell processes throughout the central nervous system. Here we explore this phenomenon in M\u00fcller glia, which are specialized radial astroglia that are the predominant glial type of the vertebrate retina. M\u00fcller glia extend fine filopodia-like processes into retinal synaptic layers, in similar fashion to brain astrocytes and radial glia which exhibit perisynaptic processes. Using two-photon volumetric imaging, we found that during the second postnatal week, M\u00fcller glial processes were highly dynamic, with rapid extensions and retractions that were mediated by cytoskeletal rearrangements. During this same stage of development, retinal waves led to increases in cytosolic calcium within M\u00fcller glial lateral processes and stalks. These comprised distinct calcium compartments, distinguished by variable participation in waves, timing, and sensitivity to an M1 muscarinic acetylcholine receptor antagonist. However, we found that motility of lateral processes was unaffected by the presence of pharmacological agents that enhanced or blocked wave-associated calcium transients. Finally, we found that mice lacking normal cholinergic waves in the first postnatal week also exhibited normal M\u00fcller glial process morphology. Hence, outgrowth of M\u00fcller glial lateral processes into synaptic layers is determined by factors that are independent of neuronal activity.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "672.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 704957644,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5851573",
    "created": "1642189526"
  },
  {
    "id": 62,
    "canonicalId": "u031yrohawb5s2y73eip0wk9",
    "datasetId": "u031yrohawb5s2y73eip0wk9",
    "doi": "10.5061/dryad.246qg",
    "title": "Data from: Transformation of stimulus correlations by the retina",
    "description": "Redundancies and correlations in the responses of sensory neurons may seem to waste neural resources, but they can also carry cues about structured stimuli and may help the brain to correct for response errors. To investigate the effect of stimulus structure on redundancy in retina, we measured simultaneous responses from populations of retinal ganglion cells presented with natural and artificial stimuli that varied greatly in correlation structure; these stimuli and recordings are publicly available online. Responding to spatio-temporally structured stimuli such as natural movies, pairs of ganglion cells were modestly more correlated than in response to white noise checkerboards, but they were much less correlated than predicted by a non-adapting functional model of retinal response. Meanwhile, responding to stimuli with purely spatial correlations, pairs of ganglion cells showed increased correlations consistent with a static, non-adapting receptive field and nonlinearity. We found that in response to spatio-temporally correlated stimuli, ganglion cells had faster temporal kernels and tended to have stronger surrounds. These properties of individual cells, along with gain changes that opposed changes in effective contrast at the ganglion cell input, largely explained the pattern of pairwise correlations across stimuli where receptive field measurements were possible.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Redundancies and correlations in the responses of sensory neurons may seem to waste neural resources, but they can also carry cues about structured stimuli and may help the brain to correct for response errors. To investigate the effect of stimulus structure on redundancy in retina, we measured simultaneous responses from populations of retinal ganglion cells presented with natural and artificial stimuli that varied greatly in correlation structure; these stimuli and recordings are publicly available online. Responding to spatio-temporally structured stimuli such as natural movies, pairs of ganglion cells were modestly more correlated than in response to white noise checkerboards, but they were much less correlated than predicted by a non-adapting functional model of retinal response. Meanwhile, responding to stimuli with purely spatial correlations, pairs of ganglion cells showed increased correlations consistent with a static, non-adapting receptive field and nonlinearity. We found that in response to spatio-temporally correlated stimuli, ganglion cells had faster temporal kernels and tended to have stronger surrounds. These properties of individual cells, along with gain changes that opposed changes in effective contrast at the ganglion cell input, largely explained the pattern of pairwise correlations across stimuli where receptive field measurements were possible.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.246qg",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Transformation of stimulus correlations by the retina"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Simmons, Kristina D.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          },
          {
            "creatorName": "Prentice, Jason S.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Princeton University"
              }
            ]
          },
          {
            "creatorName": "Tka\u010dik, Ga\u0161per",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Science and Technology Austria"
              }
            ]
          },
          {
            "creatorName": "Homann, Jan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          },
          {
            "creatorName": "Yee, Heather K.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Chicago"
              }
            ]
          },
          {
            "creatorName": "Palmer, Stephanie E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Chicago"
              }
            ]
          },
          {
            "creatorName": "Nelson, Philip C.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          },
          {
            "creatorName": "Balasubramanian, Vijay",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pennsylvania"
              }
            ]
          }
        ],
        "publicationYear": "2014",
        "date": [
          {
            "dateValue": "2014-11-07",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Redundancies and correlations in the responses of sensory neurons may seem to waste neural resources, but they can also carry cues about structured stimuli and may help the brain to correct for response errors. To investigate the effect of stimulus structure on redundancy in retina, we measured simultaneous responses from populations of retinal ganglion cells presented with natural and artificial stimuli that varied greatly in correlation structure; these stimuli and recordings are publicly available online. Responding to spatio-temporally structured stimuli such as natural movies, pairs of ganglion cells were modestly more correlated than in response to white noise checkerboards, but they were much less correlated than predicted by a non-adapting functional model of retinal response. Meanwhile, responding to stimuli with purely spatial correlations, pairs of ganglion cells showed increased correlations consistent with a static, non-adapting receptive field and nonlinearity. We found that in response to spatio-temporally correlated stimuli, ganglion cells had faster temporal kernels and tended to have stronger surrounds. These properties of individual cells, along with gain changes that opposed changes in effective contrast at the ganglion cell input, largely explained the pattern of pairwise correlations across stimuli where receptive field measurements were possible.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "scale-invariant correlations"
          },
          {
            "subjectValue": "natural scenes"
          },
          {
            "subjectValue": "white noise"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "contrast"
          },
          {
            "subjectValue": "efficient coding"
          },
          {
            "subjectValue": "receptive fields"
          },
          {
            "subjectValue": "full-field flicker"
          },
          {
            "subjectValue": "stimulus correlations"
          },
          {
            "subjectValue": "multi-electrode array"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "609.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 638582784,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4955990",
    "created": "1623757147"
  },
  {
    "id": 63,
    "canonicalId": "v9ecq10noez47kflnwym5os9",
    "datasetId": "v9ecq10noez47kflnwym5os9",
    "doi": "10.5281/zenodo.17219542",
    "title": "Indian Diabetic Retinopathy Image Dataset (IDRiD) \u2014 Segmentation, Grading & Localization ZIP Files",
    "description": "<p>This record contains the original ZIP files of the Indian Diabetic Retinopathy Image Dataset (IDRiD):</p>\n<ul>\n<li>\n<p><code>A. Segmentation.zip</code> &ndash; Original images and groundtruth segmentation masks</p>\n</li>\n<li>\n<p><code>B. Disease Grading.zip</code> &ndash; Fundus images with diabetic retinopathy and macular edema grading labels</p>\n</li>\n<li>\n<p><code>C. Localization.zip</code> &ndash; Fundus images with optic disc and fovea center locations</p>\n</li>\n</ul>\n<p>The original dataset was created and published by Porwal P. et al., IEEE Dataport.&nbsp;<br>The uploader (Tayebeh Kazeminezhad) is not the creator of the dataset and only uploads the files to Zenodo.</p>\n<p>License: CC BY 4.0 (as permitted for redistribution; original license retained)</p>\n<p>Citation for original dataset:<br>Porwal P. et al., Indian Diabetic Retinopathy Image Dataset (IDRiD), IEEE Dataport, 2018.<br>URL: https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This record contains the original ZIP files of the Indian Diabetic Retinopathy Image Dataset (IDRiD):</p>\n<ul>\n<li>\n<p><code>A. Segmentation.zip</code> &ndash; Original images and groundtruth segmentation masks</p>\n</li>\n<li>\n<p><code>B. Disease Grading.zip</code> &ndash; Fundus images with diabetic retinopathy and macular edema grading labels</p>\n</li>\n<li>\n<p><code>C. Localization.zip</code> &ndash; Fundus images with optic disc and fovea center locations</p>\n</li>\n</ul>\n<p>The original dataset was created and published by Porwal P. et al., IEEE Dataport.&nbsp;<br>The uploader (Tayebeh Kazeminezhad) is not the creator of the dataset and only uploads the files to Zenodo.</p>\n<p>License: CC BY 4.0 (as permitted for redistribution; original license retained)</p>\n<p>Citation for original dataset:<br>Porwal P. et al., Indian Diabetic Retinopathy Image Dataset (IDRiD), IEEE Dataport, 2018.<br>URL: https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17219542",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Indian Diabetic Retinopathy Image Dataset (IDRiD) \u2014 Segmentation, Grading & Localization ZIP Files"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kazeminezhad",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This record contains the original ZIP files of the Indian Diabetic Retinopathy Image Dataset (IDRiD):</p>\n<ul>\n<li>\n<p><code>A. Segmentation.zip</code> &ndash; Original images and groundtruth segmentation masks</p>\n</li>\n<li>\n<p><code>B. Disease Grading.zip</code> &ndash; Fundus images with diabetic retinopathy and macular edema grading labels</p>\n</li>\n<li>\n<p><code>C. Localization.zip</code> &ndash; Fundus images with optic disc and fovea center locations</p>\n</li>\n</ul>\n<p>The original dataset was created and published by Porwal P. et al., IEEE Dataport.&nbsp;<br>The uploader (Tayebeh Kazeminezhad) is not the creator of the dataset and only uploads the files to Zenodo.</p>\n<p>License: CC BY 4.0 (as permitted for redistribution; original license retained)</p>\n<p>Citation for original dataset:<br>Porwal P. et al., Indian Diabetic Retinopathy Image Dataset (IDRiD), IEEE Dataport, 2018.<br>URL: https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "962.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1009254400,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17219542",
    "created": "1759159358"
  },
  {
    "id": 64,
    "canonicalId": "arb1tgjhtfvbsoq8w7lv2swf",
    "datasetId": "arb1tgjhtfvbsoq8w7lv2swf",
    "doi": "10.5281/zenodo.14754838",
    "title": "Metabolomics of mouse retina and optic nerve (treated with/without PQQ) (Canovai, 2023)",
    "description": "<p>Raw metabolomics datasets from:</p>\n<p>Canovai A, Tribble JR, J&ouml;e M, Westerlund DY, Amato R, Trounce IA, Dal Monte M, Williams PA. Pyrroloquinoline quinone drives ATP synthesis in vitro and in vivo and provides retinal ganglion cell neuroprotection. Acta Neuropathol Commun. 2023 Sep 8;11(1):146. doi: 10.1186/s40478-023-01642-6. PMID: 37684640; PMCID: PMC10486004.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/37684640/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-023-01642-6</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Raw metabolomics datasets from:</p>\n<p>Canovai A, Tribble JR, J&ouml;e M, Westerlund DY, Amato R, Trounce IA, Dal Monte M, Williams PA. Pyrroloquinoline quinone drives ATP synthesis in vitro and in vivo and provides retinal ganglion cell neuroprotection. Acta Neuropathol Commun. 2023 Sep 8;11(1):146. doi: 10.1186/s40478-023-01642-6. PMID: 37684640; PMCID: PMC10486004.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/37684640/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-023-01642-6</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.14754838",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Metabolomics of mouse retina and optic nerve (treated with/without PQQ) (Canovai, 2023)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "CANOVAI, ALESSIO",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Karolinska Institutet"
              }
            ]
          },
          {
            "creatorName": "Tribble, James",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Karolinska Institutet"
              }
            ]
          },
          {
            "creatorName": "Williams, Pete",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "St Erik Eye Hospital"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Raw metabolomics datasets from:</p>\n<p>Canovai A, Tribble JR, J&ouml;e M, Westerlund DY, Amato R, Trounce IA, Dal Monte M, Williams PA. Pyrroloquinoline quinone drives ATP synthesis in vitro and in vivo and provides retinal ganglion cell neuroprotection. Acta Neuropathol Commun. 2023 Sep 8;11(1):146. doi: 10.1186/s40478-023-01642-6. PMID: 37684640; PMCID: PMC10486004.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/37684640/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-023-01642-6</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "20464.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 21458898124,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/14754838",
    "created": "1738070095"
  },
  {
    "id": 65,
    "canonicalId": "gajwfitgoopmmver79avr578",
    "datasetId": "gajwfitgoopmmver79avr578",
    "doi": "10.5061/dryad.vx0k6djwf",
    "title": "Amacrine cells differentially balance zebrafish colour circuits in the central and peripheral retina",
    "description": "<p>In vertebrate vision, the feature-extracting circuits of the inner retina are driven by photoreceptors whose outputs are already pre-processed. In zebrafish, for example, outer retinal circuits split \"colour\" from \"greyscale\" information across all four cone-photoreceptor types. How does the inner retina process this incoming spectral information while also combining cone signals to shape new greyscale functions?</p>\n<p>We address this question by imaging the light-driven responses of amacrine cells (ACs) and bipolar cells (BCs) in larval zebrafish, in the presence and pharmacological absence of inner retinal inhibition. We find that amacrine cells exert distinct effects on greyscale processing depending on retinal region, as well as contributing to the generation of colour opponency in the central retina. However, in the peripheral retina amacrine cells enhanced opponency in some bipolar cells while at the same time suppressing pre-existing opponency in others, such that the net change in the number of colour-opponent units was essentially zero. To achieve this 'dynamic balance' ACs counteracted intrinsic colour opponency of BCs via the On-channel. Consistent with these observations, Off-stratifying ACs were exclusively achromatic, while all colour opponent ACs stratified in the On-sublamina.</p>\n<p>This study reveals that the central and peripheral retina of larval zebrafish employ fundamentally distinct inhibitory circuits to control the interaction between greyscale- and colour-processing. Differential actions on the On- and Off-channels control the transmission of colour-opponent signals in the periphery.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>In vertebrate vision, the feature-extracting circuits of the inner retina are driven by photoreceptors whose outputs are already pre-processed. In zebrafish, for example, outer retinal circuits split \"colour\" from \"greyscale\" information across all four cone-photoreceptor types. How does the inner retina process this incoming spectral information while also combining cone signals to shape new greyscale functions?</p>\n<p>We address this question by imaging the light-driven responses of amacrine cells (ACs) and bipolar cells (BCs) in larval zebrafish, in the presence and pharmacological absence of inner retinal inhibition. We find that amacrine cells exert distinct effects on greyscale processing depending on retinal region, as well as contributing to the generation of colour opponency in the central retina. However, in the peripheral retina amacrine cells enhanced opponency in some bipolar cells while at the same time suppressing pre-existing opponency in others, such that the net change in the number of colour-opponent units was essentially zero. To achieve this 'dynamic balance' ACs counteracted intrinsic colour opponency of BCs via the On-channel. Consistent with these observations, Off-stratifying ACs were exclusively achromatic, while all colour opponent ACs stratified in the On-sublamina.</p>\n<p>This study reveals that the central and peripheral retina of larval zebrafish employ fundamentally distinct inhibitory circuits to control the interaction between greyscale- and colour-processing. Differential actions on the On- and Off-channels control the transmission of colour-opponent signals in the periphery.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.vx0k6djwf",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Amacrine cells differentially balance zebrafish colour circuits in the central and peripheral retina"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Wang, Xinwei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Roberts, Paul",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Yoshimatsu, Takeshi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Lagnado, Leon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Baden, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-01-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>In vertebrate vision, the feature-extracting circuits of the inner retina are driven by photoreceptors whose outputs are already pre-processed. In zebrafish, for example, outer retinal circuits split \"colour\" from \"greyscale\" information across all four cone-photoreceptor types. How does the inner retina process this incoming spectral information while also combining cone signals to shape new greyscale functions?</p>\n<p>We address this question by imaging the light-driven responses of amacrine cells (ACs) and bipolar cells (BCs) in larval zebrafish, in the presence and pharmacological absence of inner retinal inhibition. We find that amacrine cells exert distinct effects on greyscale processing depending on retinal region, as well as contributing to the generation of colour opponency in the central retina. However, in the peripheral retina amacrine cells enhanced opponency in some bipolar cells while at the same time suppressing pre-existing opponency in others, such that the net change in the number of colour-opponent units was essentially zero. To achieve this 'dynamic balance' ACs counteracted intrinsic colour opponency of BCs via the On-channel. Consistent with these observations, Off-stratifying ACs were exclusively achromatic, while all colour opponent ACs stratified in the On-sublamina.</p>\n<p>This study reveals that the central and peripheral retina of larval zebrafish employ fundamentally distinct inhibitory circuits to control the interaction between greyscale- and colour-processing. Differential actions on the On- and Off-channels control the transmission of colour-opponent signals in the periphery.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "two-photon imaging"
          },
          {
            "subjectValue": "zebrafish"
          },
          {
            "subjectValue": "amacrine cells"
          },
          {
            "subjectValue": "Colour Vision"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1525.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1599078400,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7516807",
    "created": "1673285998"
  },
  {
    "id": 66,
    "canonicalId": "i2gmwmup7qakf74bg3wdwjw4",
    "datasetId": "i2gmwmup7qakf74bg3wdwjw4",
    "doi": "10.5061/dryad.5x69p8d52",
    "title": "Data from: Awake responses suggest inefficient dense coding in the mouse retina",
    "description": "<p><span>The structure and function of the vertebrate retina have been extensively studied across species with an isolated, <em>ex vivo</em> preparation. Retinal function <em>in vivo</em>, however, remains elusive, especially in awake animals. Here we performed single-unit extracellular recordings in the optic tract of head-fixed mice to compare the output of awake, anesthetized, and <em>ex vivo </em>retinas. While the visual response properties were overall similar across conditions, we found that awake retinal output had in general 1) faster kinetics with less variability in the response latencies; 2) a larger dynamic range; and 3) higher firing activity, by ~20 Hz on average, for both baseline and visually evoked responses. Our modeling analyses further showed that such awake response patterns convey comparable total information but less efficiently, and allow for a linear population decoder to perform significantly better than the anesthetized or <em>ex vivo</em> responses. These results highlight distinct retinal behavior in awake states, in particular suggesting that the retina employs dense coding <em>in vivo</em>, rather than sparse efficient coding as has been often assumed from<em> ex vivo</em> studies.</span></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><span>The structure and function of the vertebrate retina have been extensively studied across species with an isolated, <em>ex vivo</em> preparation. Retinal function <em>in vivo</em>, however, remains elusive, especially in awake animals. Here we performed single-unit extracellular recordings in the optic tract of head-fixed mice to compare the output of awake, anesthetized, and <em>ex vivo </em>retinas. While the visual response properties were overall similar across conditions, we found that awake retinal output had in general 1) faster kinetics with less variability in the response latencies; 2) a larger dynamic range; and 3) higher firing activity, by ~20 Hz on average, for both baseline and visually evoked responses. Our modeling analyses further showed that such awake response patterns convey comparable total information but less efficiently, and allow for a linear population decoder to perform significantly better than the anesthetized or <em>ex vivo</em> responses. These results highlight distinct retinal behavior in awake states, in particular suggesting that the retina employs dense coding <em>in vivo</em>, rather than sparse efficient coding as has been often assumed from<em> ex vivo</em> studies.</span></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.5x69p8d52",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Awake responses suggest inefficient dense coding in the mouse retina"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Boissonnet, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "European Molecular Biology Laboratory"
              }
            ]
          },
          {
            "creatorName": "Tripodi, Matteo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "European Molecular Biology Laboratory"
              }
            ]
          },
          {
            "creatorName": "Asari, Hiroki",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "European Molecular Biology Laboratory"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-10-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><span>The structure and function of the vertebrate retina have been extensively studied across species with an isolated, <em>ex vivo</em> preparation. Retinal function <em>in vivo</em>, however, remains elusive, especially in awake animals. Here we performed single-unit extracellular recordings in the optic tract of head-fixed mice to compare the output of awake, anesthetized, and <em>ex vivo </em>retinas. While the visual response properties were overall similar across conditions, we found that awake retinal output had in general 1) faster kinetics with less variability in the response latencies; 2) a larger dynamic range; and 3) higher firing activity, by ~20 Hz on average, for both baseline and visually evoked responses. Our modeling analyses further showed that such awake response patterns convey comparable total information but less efficiently, and allow for a linear population decoder to perform significantly better than the anesthetized or <em>ex vivo</em> responses. These results highlight distinct retinal behavior in awake states, in particular suggesting that the retina employs dense coding <em>in vivo</em>, rather than sparse efficient coding as has been often assumed from<em> ex vivo</em> studies.</span></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Retinal ganglion cells"
          },
          {
            "subjectValue": "in vivo recordings"
          },
          {
            "subjectValue": "awake"
          },
          {
            "subjectValue": "Anesthesia"
          },
          {
            "subjectValue": "efficient coding"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "4655.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 4881226137,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8411859",
    "created": "1696535944"
  },
  {
    "id": 67,
    "canonicalId": "xqe4fb20ed6gy3fy2it4zse9",
    "datasetId": "xqe4fb20ed6gy3fy2it4zse9",
    "doi": "10.5061/dryad.5bc8vd7",
    "title": "Data from: Zebrafish differentially process colour across visual space to match natural scenes",
    "description": "Animal eyes have evolved to process behaviorally important visual information, but how retinas deal with statistical asymmetries in visual space remains poorly understood. Using hyperspectral imaging in the field, in vivo 2-photon imaging of retinal neurons, and anatomy, here we show that larval zebrafish use a highly anisotropic retina to asymmetrically survey their natural visual world. First, different neurons dominate different parts of the eye and are linked to a systematic shift in inner retinal function: above the animal, there is little color in nature, and retinal circuits are largely achromatic. Conversely, the lower visual field and horizon are color rich and are predominately surveyed by chromatic and color-opponent circuits that are spectrally matched to the dominant chromatic axes in nature. Second, in the horizontal and lower visual field, bipolar cell terminals encoding achromatic and color-opponent visual features are systematically arranged into distinct layers of the inner retina. Third, above the frontal horizon, a high-gain UV system piggybacks onto retinal circuits, likely to support prey capture.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Animal eyes have evolved to process behaviorally important visual information, but how retinas deal with statistical asymmetries in visual space remains poorly understood. Using hyperspectral imaging in the field, in vivo 2-photon imaging of retinal neurons, and anatomy, here we show that larval zebrafish use a highly anisotropic retina to asymmetrically survey their natural visual world. First, different neurons dominate different parts of the eye and are linked to a systematic shift in inner retinal function: above the animal, there is little color in nature, and retinal circuits are largely achromatic. Conversely, the lower visual field and horizon are color rich and are predominately surveyed by chromatic and color-opponent circuits that are spectrally matched to the dominant chromatic axes in nature. Second, in the horizontal and lower visual field, bipolar cell terminals encoding achromatic and color-opponent visual features are systematically arranged into distinct layers of the inner retina. Third, above the frontal horizon, a high-gain UV system piggybacks onto retinal circuits, likely to support prey capture.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.5bc8vd7",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Zebrafish differentially process colour across visual space to match natural scenes"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Zimmermann, Maxime J. Y.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Nevala, Noora E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Yoshimatsu, Takeshi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Osorio, Daniel",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Nilsson, Dan-Eric",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Lund University"
              }
            ]
          },
          {
            "creatorName": "Berens, Philipp",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Baden, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Zimmermann, Maxime J.Y.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          }
        ],
        "publicationYear": "2019",
        "date": [
          {
            "dateValue": "2019-05-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Animal eyes have evolved to process behaviorally important visual information, but how retinas deal with statistical asymmetries in visual space remains poorly understood. Using hyperspectral imaging in the field, in vivo 2-photon imaging of retinal neurons, and anatomy, here we show that larval zebrafish use a highly anisotropic retina to asymmetrically survey their natural visual world. First, different neurons dominate different parts of the eye and are linked to a systematic shift in inner retinal function: above the animal, there is little color in nature, and retinal circuits are largely achromatic. Conversely, the lower visual field and horizon are color rich and are predominately surveyed by chromatic and color-opponent circuits that are spectrally matched to the dominant chromatic axes in nature. Second, in the horizontal and lower visual field, bipolar cell terminals encoding achromatic and color-opponent visual features are systematically arranged into distinct layers of the inner retina. Third, above the frontal horizon, a high-gain UV system piggybacks onto retinal circuits, likely to support prey capture.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "natural scenes"
          },
          {
            "subjectValue": "Danio rerio"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "Colour Vision"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "527.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 553228697,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4994446",
    "created": "1624126608"
  },
  {
    "id": 68,
    "canonicalId": "oyuzejmdm9m8sjsutt7rv6x6",
    "datasetId": "oyuzejmdm9m8sjsutt7rv6x6",
    "doi": "10.5061/dryad.c2fqz61hr",
    "title": "NMR spectroscopy-based metabolomics of organotypic retinal explants",
    "description": "<p>The retina consumes massive amounts of energy, yet its metabolism and substrate exploitation remain poorly understood. Here, we used a murine explant model to manipulate retinal energy metabolism under entirely controlled conditions and utilized <sup>1</sup>H-NMR spectroscopy-based metabolomics, in situenzyme detection, and cell viability readouts to uncover the pathways of retinal energy production. Our experimental manipulations resulted in varying degrees of photoreceptor degeneration, while the inner retina and retinal pigment epithelium were essentially unaffected. This selective vulnerability of photoreceptors suggested very specific adaptations in their energy metabolism. Rod photoreceptors were found to rely strongly on oxidative phosphorylation, but only mildly on glycolysis. Conversely, cone photoreceptors were dependent on glycolysis but insensitive to electron transport chain decoupling. Importantly, photoreceptors appeared to uncouple glycolytic and Krebs-cycle metabolism via three different pathways: 1) the mini-Krebs-cycle, fueled by glutamine and branched-chain amino acids, generating N-acetylaspartate; 2) the alanine-generating Cahill-cycle; 3) the lactate-releasing Cori-cycle. Moreover, the metabolomic data indicated a shuttling of taurine and hypotaurine between the retinal pigment epithelium and photoreceptors, likely resulting in an additional net transfer of reducing power to photoreceptors. These findings expand our understanding of retinal physiology and pathology and shed new light on neuronal energy homeostasis and the pathogenesis of neurodegenerative diseases.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The retina consumes massive amounts of energy, yet its metabolism and substrate exploitation remain poorly understood. Here, we used a murine explant model to manipulate retinal energy metabolism under entirely controlled conditions and utilized <sup>1</sup>H-NMR spectroscopy-based metabolomics, in situenzyme detection, and cell viability readouts to uncover the pathways of retinal energy production. Our experimental manipulations resulted in varying degrees of photoreceptor degeneration, while the inner retina and retinal pigment epithelium were essentially unaffected. This selective vulnerability of photoreceptors suggested very specific adaptations in their energy metabolism. Rod photoreceptors were found to rely strongly on oxidative phosphorylation, but only mildly on glycolysis. Conversely, cone photoreceptors were dependent on glycolysis but insensitive to electron transport chain decoupling. Importantly, photoreceptors appeared to uncouple glycolytic and Krebs-cycle metabolism via three different pathways: 1) the mini-Krebs-cycle, fueled by glutamine and branched-chain amino acids, generating N-acetylaspartate; 2) the alanine-generating Cahill-cycle; 3) the lactate-releasing Cori-cycle. Moreover, the metabolomic data indicated a shuttling of taurine and hypotaurine between the retinal pigment epithelium and photoreceptors, likely resulting in an additional net transfer of reducing power to photoreceptors. These findings expand our understanding of retinal physiology and pathology and shed new light on neuronal energy homeostasis and the pathogenesis of neurodegenerative diseases.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.c2fqz61hr",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "NMR spectroscopy-based metabolomics of organotypic retinal explants"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Chen, Yiyi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Zizmare, Laimdota",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Paquet-Durand, Fran\u00e7ois",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Trautwein, Christoph",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of T\u00fcbingen"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-04-30",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The retina consumes massive amounts of energy, yet its metabolism and substrate exploitation remain poorly understood. Here, we used a murine explant model to manipulate retinal energy metabolism under entirely controlled conditions and utilized <sup>1</sup>H-NMR spectroscopy-based metabolomics, in situenzyme detection, and cell viability readouts to uncover the pathways of retinal energy production. Our experimental manipulations resulted in varying degrees of photoreceptor degeneration, while the inner retina and retinal pigment epithelium were essentially unaffected. This selective vulnerability of photoreceptors suggested very specific adaptations in their energy metabolism. Rod photoreceptors were found to rely strongly on oxidative phosphorylation, but only mildly on glycolysis. Conversely, cone photoreceptors were dependent on glycolysis but insensitive to electron transport chain decoupling. Importantly, photoreceptors appeared to uncouple glycolytic and Krebs-cycle metabolism via three different pathways: 1) the mini-Krebs-cycle, fueled by glutamine and branched-chain amino acids, generating N-acetylaspartate; 2) the alanine-generating Cahill-cycle; 3) the lactate-releasing Cori-cycle. Moreover, the metabolomic data indicated a shuttling of taurine and hypotaurine between the retinal pigment epithelium and photoreceptors, likely resulting in an additional net transfer of reducing power to photoreceptors. These findings expand our understanding of retinal physiology and pathology and shed new light on neuronal energy homeostasis and the pathogenesis of neurodegenerative diseases.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Metabolomics"
          },
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Energy metabolism"
          },
          {
            "subjectValue": "Proton NMR spectroscopy"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "9.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 9542041,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/11091057",
    "created": "1714464297"
  },
  {
    "id": 69,
    "canonicalId": "svwqspfmh5api4a6or9ybf17",
    "datasetId": "svwqspfmh5api4a6or9ybf17",
    "doi": "10.5281/zenodo.17016432",
    "title": "the central histaminergic system slows visual processing in the retina and lateral geniculate nucleus of awake mice",
    "description": "<p>This directory contains data sets and codes used in the following paper:</p>\n<p>&nbsp; Tripodi M. and Asari H. (2025)<br>&nbsp; Central histaminergic system reduces response gain and slows visual processing&nbsp;<br>&nbsp; in the retina and lateral geniculate nucleus of awake mice<br>&nbsp; bioRxiv 2024.09.02.610848 (preprint)<br>&nbsp; doi: https://doi.org/10.1101/2024.09.02.610848</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This directory contains data sets and codes used in the following paper:</p>\n<p>&nbsp; Tripodi M. and Asari H. (2025)<br>&nbsp; Central histaminergic system reduces response gain and slows visual processing&nbsp;<br>&nbsp; in the retina and lateral geniculate nucleus of awake mice<br>&nbsp; bioRxiv 2024.09.02.610848 (preprint)<br>&nbsp; doi: https://doi.org/10.1101/2024.09.02.610848</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17016432",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "the central histaminergic system slows visual processing in the retina and lateral geniculate nucleus of awake mice"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Tripodi, Matteo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "European Molecular Biology Laboratory"
              }
            ]
          },
          {
            "creatorName": "Asari, Hiroki",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "European Molecular Biology Laboratory"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This directory contains data sets and codes used in the following paper:</p>\n<p>&nbsp; Tripodi M. and Asari H. (2025)<br>&nbsp; Central histaminergic system reduces response gain and slows visual processing&nbsp;<br>&nbsp; in the retina and lateral geniculate nucleus of awake mice<br>&nbsp; bioRxiv 2024.09.02.610848 (preprint)<br>&nbsp; doi: https://doi.org/10.1101/2024.09.02.610848</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "histaminergic modulation"
          },
          {
            "subjectValue": "Tuberomammillary nucleus"
          },
          {
            "subjectValue": "Lateral geniculate nucleus"
          },
          {
            "subjectValue": "Retinal ganglion cells"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "307.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 322332262,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17016432",
    "created": "1756815683"
  },
  {
    "id": 70,
    "canonicalId": "vi7lxnwc2d2qytylvgk4xw7y",
    "datasetId": "vi7lxnwc2d2qytylvgk4xw7y",
    "doi": "10.5281/zenodo.7957454",
    "title": "Retina OCT glaucoma",
    "description": "<p>This data set is part of the public development data for&nbsp;the&nbsp;<a href=\"http://auc23.grand-challenge.org/\">2023 Automated Universal Classification Challenge</a> (AUC23).&nbsp;The data set&nbsp;was previously introduced and described by&nbsp;<a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">Ishikawa, H. (2018)</a>&nbsp;and concerns&nbsp;the classification of&nbsp;eyes as healthy or glaucomatous from optical coherence tomography&nbsp;(OCT) volumes of the optic nerve head. Data was&nbsp;restructured in compliance with the&nbsp;<a href=\"https://auc23.grand-challenge.org/\">AUC23</a>&nbsp;challenge format, and no images or patient information were added. The data set consists of 884 OCT scans.</p>\n\n<p>Images are 3D tensors:</p>\n\n<ul>\n\t<li>0: 3D OCT volume&nbsp;centered on the&nbsp;optic nerve head</li>\n</ul>\n\n<p>Classification labels:</p>\n\n<ul>\n\t<li>0: Normal</li>\n\t<li>1: Primary open angle glaucoma</li>\n</ul>\n\n<p>Folder structure:</p>\n\n<p>imagesTr (root folder with all patients and studies)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500 POAG_0000_0000.mha &nbsp;(OCT volume for&nbsp;study 0000)<br>\n&nbsp;&nbsp; &nbsp;\u251c\u2500\u2500 POAG_0000_0001.mha &nbsp;(OCT volume for&nbsp;study 0001)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500&nbsp;...</p>\n\n<p>Please cite the following article if you are using the <a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">OCT volumes for glaucoma detection dataset</a>:</p>\n\n<pre><code>Ishikawa, Hiroshi. (2018). OCT volumes for glaucoma detection (1.0.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1481223</code></pre>\n\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This data set is part of the public development data for&nbsp;the&nbsp;<a href=\"http://auc23.grand-challenge.org/\">2023 Automated Universal Classification Challenge</a> (AUC23).&nbsp;The data set&nbsp;was previously introduced and described by&nbsp;<a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">Ishikawa, H. (2018)</a>&nbsp;and concerns&nbsp;the classification of&nbsp;eyes as healthy or glaucomatous from optical coherence tomography&nbsp;(OCT) volumes of the optic nerve head. Data was&nbsp;restructured in compliance with the&nbsp;<a href=\"https://auc23.grand-challenge.org/\">AUC23</a>&nbsp;challenge format, and no images or patient information were added. The data set consists of 884 OCT scans.</p>\n\n<p>Images are 3D tensors:</p>\n\n<ul>\n\t<li>0: 3D OCT volume&nbsp;centered on the&nbsp;optic nerve head</li>\n</ul>\n\n<p>Classification labels:</p>\n\n<ul>\n\t<li>0: Normal</li>\n\t<li>1: Primary open angle glaucoma</li>\n</ul>\n\n<p>Folder structure:</p>\n\n<p>imagesTr (root folder with all patients and studies)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500 POAG_0000_0000.mha &nbsp;(OCT volume for&nbsp;study 0000)<br>\n&nbsp;&nbsp; &nbsp;\u251c\u2500\u2500 POAG_0000_0001.mha &nbsp;(OCT volume for&nbsp;study 0001)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500&nbsp;...</p>\n\n<p>Please cite the following article if you are using the <a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">OCT volumes for glaucoma detection dataset</a>:</p>\n\n<pre><code>Ishikawa, Hiroshi. (2018). OCT volumes for glaucoma detection (1.0.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1481223</code></pre>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7957454",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Retina OCT glaucoma"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Alves, Natalia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Radboud University Medical Center"
              }
            ]
          },
          {
            "creatorName": "Boulogne, Luuk",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Radboud University Medical Center"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-05-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This data set is part of the public development data for&nbsp;the&nbsp;<a href=\"http://auc23.grand-challenge.org/\">2023 Automated Universal Classification Challenge</a> (AUC23).&nbsp;The data set&nbsp;was previously introduced and described by&nbsp;<a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">Ishikawa, H. (2018)</a>&nbsp;and concerns&nbsp;the classification of&nbsp;eyes as healthy or glaucomatous from optical coherence tomography&nbsp;(OCT) volumes of the optic nerve head. Data was&nbsp;restructured in compliance with the&nbsp;<a href=\"https://auc23.grand-challenge.org/\">AUC23</a>&nbsp;challenge format, and no images or patient information were added. The data set consists of 884 OCT scans.</p>\n\n<p>Images are 3D tensors:</p>\n\n<ul>\n\t<li>0: 3D OCT volume&nbsp;centered on the&nbsp;optic nerve head</li>\n</ul>\n\n<p>Classification labels:</p>\n\n<ul>\n\t<li>0: Normal</li>\n\t<li>1: Primary open angle glaucoma</li>\n</ul>\n\n<p>Folder structure:</p>\n\n<p>imagesTr (root folder with all patients and studies)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500 POAG_0000_0000.mha &nbsp;(OCT volume for&nbsp;study 0000)<br>\n&nbsp;&nbsp; &nbsp;\u251c\u2500\u2500 POAG_0000_0001.mha &nbsp;(OCT volume for&nbsp;study 0001)<br>\n&nbsp; &nbsp; \u251c\u2500\u2500&nbsp;...</p>\n\n<p>Please cite the following article if you are using the <a href=\"https://zenodo.org/record/1481223#.ZGtYY3ZBzdl\">OCT volumes for glaucoma detection dataset</a>:</p>\n\n<pre><code>Ishikawa, Hiroshi. (2018). OCT volumes for glaucoma detection (1.0.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1481223</code></pre>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "358.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 376333926,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7957454",
    "created": "1685112389"
  },
  {
    "id": 71,
    "canonicalId": "n853btlndjza8ywdzav84e3n",
    "datasetId": "n853btlndjza8ywdzav84e3n",
    "doi": "10.5061/dryad.3q7p2",
    "title": "Data from: Co-expression of two subtypes of melatonin receptor on rat M1-type intrinsically photosensitive retinal ganglion cells",
    "description": "Intrinsically photosensitive retinal ganglion cells (ipRGCs) are involved in circadian and other non-image forming visual responses. An open question is whether the activity of these neurons may also be under the regulation mediated by the neurohormone melatonin. In the present work, by double-staining immunohistochemical technique, we studied the expression of MT1 and MT2, two known subtypes of mammalian melatonin receptors, in rat ipRGCs. A single subset of retinal ganglion cells labeled by the specific antibody against melanopsin exhibited the morphology typical of M1-type ipRGCs. Immunoreactivity for both MT1 and MT2 receptors was clearly seen in the cytoplasm of all labeled ipRGCs, indicating that these two receptors were co-expressed in each of these neurons. Furthermore, labeling for both the receptors were found in neonatal M1 cells as early as the day of birth. It is therefore highly plausible that retinal melatonin may directly modulate the activity of ipRGCs, thus regulating non-image forming visual functions.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Intrinsically photosensitive retinal ganglion cells (ipRGCs) are involved in circadian and other non-image forming visual responses. An open question is whether the activity of these neurons may also be under the regulation mediated by the neurohormone melatonin. In the present work, by double-staining immunohistochemical technique, we studied the expression of MT1 and MT2, two known subtypes of mammalian melatonin receptors, in rat ipRGCs. A single subset of retinal ganglion cells labeled by the specific antibody against melanopsin exhibited the morphology typical of M1-type ipRGCs. Immunoreactivity for both MT1 and MT2 receptors was clearly seen in the cytoplasm of all labeled ipRGCs, indicating that these two receptors were co-expressed in each of these neurons. Furthermore, labeling for both the receptors were found in neonatal M1 cells as early as the day of birth. It is therefore highly plausible that retinal melatonin may directly modulate the activity of ipRGCs, thus regulating non-image forming visual functions.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.3q7p2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Co-expression of two subtypes of melatonin receptor on rat M1-type intrinsically photosensitive retinal ganglion cells"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Sheng, Wen-Long",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Fudan University"
              }
            ]
          },
          {
            "creatorName": "Chen, Wei-Yi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Fudan University"
              }
            ]
          },
          {
            "creatorName": "Yang, Xiong-Li",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Fudan University"
              }
            ]
          },
          {
            "creatorName": "Zhong, Yong-Mei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Fudan University"
              }
            ]
          },
          {
            "creatorName": "Weng, Shi-Jun",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Fudan University"
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-01-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Intrinsically photosensitive retinal ganglion cells (ipRGCs) are involved in circadian and other non-image forming visual responses. An open question is whether the activity of these neurons may also be under the regulation mediated by the neurohormone melatonin. In the present work, by double-staining immunohistochemical technique, we studied the expression of MT1 and MT2, two known subtypes of mammalian melatonin receptors, in rat ipRGCs. A single subset of retinal ganglion cells labeled by the specific antibody against melanopsin exhibited the morphology typical of M1-type ipRGCs. Immunoreactivity for both MT1 and MT2 receptors was clearly seen in the cytoplasm of all labeled ipRGCs, indicating that these two receptors were co-expressed in each of these neurons. Furthermore, labeling for both the receptors were found in neonatal M1 cells as early as the day of birth. It is therefore highly plausible that retinal melatonin may directly modulate the activity of ipRGCs, thus regulating non-image forming visual functions.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "melanopsin"
          },
          {
            "subjectValue": "rat"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "melatonin receptor"
          },
          {
            "subjectValue": "intrinsically photosensitive retinal ganglion cell"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "2728.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 2860829900,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4995514",
    "created": "1624142956"
  },
  {
    "id": 72,
    "canonicalId": "dhdgjuhzy1gzuk21b41hybch",
    "datasetId": "dhdgjuhzy1gzuk21b41hybch",
    "doi": "10.5061/dryad.rv15dv4bv",
    "title": "Late gene therapy limits the restoration of retinal function in a mouse model of retinitis pigmentosa",
    "description": "<p><span>Retinitis pigmentosa is an inherited photoreceptor degeneration that begins with rod loss followed by cone loss. This cell loss greatly diminishes vision, with most patients becoming legally blind. Gene therapies are being developed, but it is unknown how retinal function depends on the time of intervention. To uncover this dependence, we utilize a mouse model of retinitis pigmentosa capable of artificial genetic rescue. This model enables a benchmark of best-case gene therapy by removing variables that complicate the ability to answer this vital question. Complete genetic rescue was performed at 25%, 50%, and 70% rod loss (early, mid, and late, respectively). Here we show early- and mid-treatment restores retinal function to near wild-type levels, specifically the sensitivity and signal fidelity of retinal ganglion cells, the output neurons of the retina. However, some anatomical defects persist. Late treatment retinas exhibit continued, albeit slowed, loss of sensitivity and signal fidelity among retinal ganglion cells, as well as persistent gliosis. We conclude that gene replacement therapies delivered after 50% rod loss are unlikely to restore visual function to normal. This is critical information for administering gene therapies to rescue vision.</span></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><span>Retinitis pigmentosa is an inherited photoreceptor degeneration that begins with rod loss followed by cone loss. This cell loss greatly diminishes vision, with most patients becoming legally blind. Gene therapies are being developed, but it is unknown how retinal function depends on the time of intervention. To uncover this dependence, we utilize a mouse model of retinitis pigmentosa capable of artificial genetic rescue. This model enables a benchmark of best-case gene therapy by removing variables that complicate the ability to answer this vital question. Complete genetic rescue was performed at 25%, 50%, and 70% rod loss (early, mid, and late, respectively). Here we show early- and mid-treatment restores retinal function to near wild-type levels, specifically the sensitivity and signal fidelity of retinal ganglion cells, the output neurons of the retina. However, some anatomical defects persist. Late treatment retinas exhibit continued, albeit slowed, loss of sensitivity and signal fidelity among retinal ganglion cells, as well as persistent gliosis. We conclude that gene replacement therapies delivered after 50% rod loss are unlikely to restore visual function to normal. This is critical information for administering gene therapies to rescue vision.</span></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.rv15dv4bv",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Late gene therapy limits the restoration of retinal function in a mouse model of retinitis pigmentosa"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Scalabrino, Miranda",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Los Angeles"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-08-30",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><span>Retinitis pigmentosa is an inherited photoreceptor degeneration that begins with rod loss followed by cone loss. This cell loss greatly diminishes vision, with most patients becoming legally blind. Gene therapies are being developed, but it is unknown how retinal function depends on the time of intervention. To uncover this dependence, we utilize a mouse model of retinitis pigmentosa capable of artificial genetic rescue. This model enables a benchmark of best-case gene therapy by removing variables that complicate the ability to answer this vital question. Complete genetic rescue was performed at 25%, 50%, and 70% rod loss (early, mid, and late, respectively). Here we show early- and mid-treatment restores retinal function to near wild-type levels, specifically the sensitivity and signal fidelity of retinal ganglion cells, the output neurons of the retina. However, some anatomical defects persist. Late treatment retinas exhibit continued, albeit slowed, loss of sensitivity and signal fidelity among retinal ganglion cells, as well as persistent gliosis. We conclude that gene replacement therapies delivered after 50% rod loss are unlikely to restore visual function to normal. This is critical information for administering gene therapies to rescue vision.</span></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Retina"
          },
          {
            "subjectValue": "Retinitis pigmentosa"
          },
          {
            "subjectValue": "Confocal microscopy"
          },
          {
            "subjectValue": "Microscopy"
          },
          {
            "subjectValue": "Retinal degeneration"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "13101.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 13737603891,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8302795",
    "created": "1694288669"
  },
  {
    "id": 73,
    "canonicalId": "r3p6aphoy59g7tonvhw4i1p8",
    "datasetId": "r3p6aphoy59g7tonvhw4i1p8",
    "doi": "10.1016/j.neuron.2019.05.044",
    "title": "Thrombospondin-1 Mediates Axon Regeneration in Retinal Ganglion Cells",
    "description": "<p>Novel Targets to Promote RGC Axon Regeneration (Park Lab)</p>\n\n<p>The aim of this project is to idenity genes and lipids that give subtypes of retinal ganglion cells a greate propensity to regenerate axons and form function synaptic connections. This work has been published in the following manuscripts:</p>\n\n<ul>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31255486/\">Thrombospondin-1 Mediates Axon Regeneration in Retinal Ganglion Cells</a>&nbsp;(2019-08-21)</li>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31508459/\">Lipid profiling dataset of the Wnt3a-induced optic nerve regeneration</a>&nbsp;(2019-05-24)</li>\n</ul>\n\n<p>Our results identify cell-type-specific induction of Thbs1 as a novel gene conferring high regenerative capacity.</p>\n\n<p>See the following visualizations to explore the findings:</p>\n\n<ul>\n\t<li><a href=\"https://viz.stjude.cloud/stjude/visualization/thrombospondin-1-mediates-axon-regeneration-in-retinal-ganglion-cells~55\">Thrombospondin-1 Mediates Axon Regeneration in RGCs</a></li>\n</ul>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Novel Targets to Promote RGC Axon Regeneration (Park Lab)</p>\n\n<p>The aim of this project is to idenity genes and lipids that give subtypes of retinal ganglion cells a greate propensity to regenerate axons and form function synaptic connections. This work has been published in the following manuscripts:</p>\n\n<ul>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31255486/\">Thrombospondin-1 Mediates Axon Regeneration in Retinal Ganglion Cells</a>&nbsp;(2019-08-21)</li>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31508459/\">Lipid profiling dataset of the Wnt3a-induced optic nerve regeneration</a>&nbsp;(2019-05-24)</li>\n</ul>\n\n<p>Our results identify cell-type-specific induction of Thbs1 as a novel gene conferring high regenerative capacity.</p>\n\n<p>See the following visualizations to explore the findings:</p>\n\n<ul>\n\t<li><a href=\"https://viz.stjude.cloud/stjude/visualization/thrombospondin-1-mediates-axon-regeneration-in-retinal-ganglion-cells~55\">Thrombospondin-1 Mediates Axon Regeneration in RGCs</a></li>\n</ul>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.1016/j.neuron.2019.05.044",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Thrombospondin-1 Mediates Axon Regeneration in Retinal Ganglion Cells"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Eric R Bray , Benjamin J Yungher , Konstantin Levay , Marcio Ribeiro , Gennady Dvoryanchikov , Ana C Ayupe , Kinjal Thakor , Victoria Marks , Michael Randolph , Matt C Danzi , Tiffany M Schmidt , Nirupa Chaudhari , Vance P Lemmon , Samer Hattar , Kevin K Park",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-06-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Novel Targets to Promote RGC Axon Regeneration (Park Lab)</p>\n\n<p>The aim of this project is to idenity genes and lipids that give subtypes of retinal ganglion cells a greate propensity to regenerate axons and form function synaptic connections. This work has been published in the following manuscripts:</p>\n\n<ul>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31255486/\">Thrombospondin-1 Mediates Axon Regeneration in Retinal Ganglion Cells</a>&nbsp;(2019-08-21)</li>\n\t<li><a href=\"https://pubmed.ncbi.nlm.nih.gov/31508459/\">Lipid profiling dataset of the Wnt3a-induced optic nerve regeneration</a>&nbsp;(2019-05-24)</li>\n</ul>\n\n<p>Our results identify cell-type-specific induction of Thbs1 as a novel gene conferring high regenerative capacity.</p>\n\n<p>See the following visualizations to explore the findings:</p>\n\n<ul>\n\t<li><a href=\"https://viz.stjude.cloud/stjude/visualization/thrombospondin-1-mediates-axon-regeneration-in-retinal-ganglion-cells~55\">Thrombospondin-1 Mediates Axon Regeneration in RGCs</a></li>\n</ul>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "18187.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 19070766284,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8096968",
    "created": "1689348082"
  },
  {
    "id": 74,
    "canonicalId": "clz1aqczsmdgvzq7o0y2kpri",
    "datasetId": "clz1aqczsmdgvzq7o0y2kpri",
    "doi": "10.5281/zenodo.14753987",
    "title": "Metabolomics of ocular hypertensive rat optic nerve (with/without nicotinamide treatment) (Cimaglia, 2024)",
    "description": "<p>Raw metabolomics datasets from:</p>\n<p>Cimaglia G, Tribble JR, Votruba M, Williams PA, Morgan JE. Oral nicotinamide provides robust, dose-dependent structural and metabolic neuroprotection of retinal ganglion cells in experimental glaucoma. Acta Neuropathol Commun. 2024 Aug 23;12(1):137. doi: 10.1186/s40478-024-01850-8. PMID: 39180087; PMCID: PMC11342512.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/39180087/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-024-01850-8</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Raw metabolomics datasets from:</p>\n<p>Cimaglia G, Tribble JR, Votruba M, Williams PA, Morgan JE. Oral nicotinamide provides robust, dose-dependent structural and metabolic neuroprotection of retinal ganglion cells in experimental glaucoma. Acta Neuropathol Commun. 2024 Aug 23;12(1):137. doi: 10.1186/s40478-024-01850-8. PMID: 39180087; PMCID: PMC11342512.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/39180087/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-024-01850-8</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.14753987",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Metabolomics of ocular hypertensive rat optic nerve (with/without nicotinamide treatment) (Cimaglia, 2024)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Cimaglia, Gloria",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Karolinska Institutet"
              }
            ]
          },
          {
            "creatorName": "Williams, Pete",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "St Erik Eye Hospital"
              }
            ]
          },
          {
            "creatorName": "Tribble, James",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Karolinska Institutet"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Raw metabolomics datasets from:</p>\n<p>Cimaglia G, Tribble JR, Votruba M, Williams PA, Morgan JE. Oral nicotinamide provides robust, dose-dependent structural and metabolic neuroprotection of retinal ganglion cells in experimental glaucoma. Acta Neuropathol Commun. 2024 Aug 23;12(1):137. doi: 10.1186/s40478-024-01850-8. PMID: 39180087; PMCID: PMC11342512.</p>\n<p>https://pubmed.ncbi.nlm.nih.gov/39180087/</p>\n<p>https://actaneurocomms.biomedcentral.com/articles/10.1186/s40478-024-01850-8</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "19509.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 20456774041,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/14753987",
    "created": "1738062754"
  },
  {
    "id": 75,
    "canonicalId": "m1x3d8osv1owd8a54neseh6g",
    "datasetId": "m1x3d8osv1owd8a54neseh6g",
    "doi": "10.5281/zenodo.7806898",
    "title": "High-resolution structural and functional retinal imaging in the awake behaving mouse",
    "description": "<p>Source data presented in Communications Biology paper Feng et al. 2023 &ndash; &lsquo;High-resolution structural and functional retinal imaging in the awake behaving mouse&rsquo;. Complete processed data presented in Figures 2 and 3 are uploaded along with the raw simultaneously recorded rotary encoder data. Raw SLO video data of only one animal is provided due to the large size. All raw OCT&nbsp;and AOSLO&nbsp; images presented in Figures 4,6-10 are included. Codes for data analysis are available at GitHub: https://github.com/GP-Feng/Awake_Mouse_Imaging_CommBio. For additional data or questions, please contact authors Guanping Feng (gfeng4@ur.rochester.edu) or Jesse Schallek (jschall3@ur.rochester.edu).</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Source data presented in Communications Biology paper Feng et al. 2023 &ndash; &lsquo;High-resolution structural and functional retinal imaging in the awake behaving mouse&rsquo;. Complete processed data presented in Figures 2 and 3 are uploaded along with the raw simultaneously recorded rotary encoder data. Raw SLO video data of only one animal is provided due to the large size. All raw OCT&nbsp;and AOSLO&nbsp; images presented in Figures 4,6-10 are included. Codes for data analysis are available at GitHub: https://github.com/GP-Feng/Awake_Mouse_Imaging_CommBio. For additional data or questions, please contact authors Guanping Feng (gfeng4@ur.rochester.edu) or Jesse Schallek (jschall3@ur.rochester.edu).</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7806898",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "High-resolution structural and functional retinal imaging in the awake behaving mouse"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Guanping Feng",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Aby Joseph",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Kosha Dholakia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Fei Shang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Charles W. Pfeifer",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Derek Power",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Krishnan Padmanabhan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Jesse Schallek",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-04-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Source data presented in Communications Biology paper Feng et al. 2023 &ndash; &lsquo;High-resolution structural and functional retinal imaging in the awake behaving mouse&rsquo;. Complete processed data presented in Figures 2 and 3 are uploaded along with the raw simultaneously recorded rotary encoder data. Raw SLO video data of only one animal is provided due to the large size. All raw OCT&nbsp;and AOSLO&nbsp; images presented in Figures 4,6-10 are included. Codes for data analysis are available at GitHub: https://github.com/GP-Feng/Awake_Mouse_Imaging_CommBio. For additional data or questions, please contact authors Guanping Feng (gfeng4@ur.rochester.edu) or Jesse Schallek (jschall3@ur.rochester.edu).</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "13477.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 14132183040,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7806898",
    "created": "1681060947"
  },
  {
    "id": 76,
    "canonicalId": "rjj0eh506x3vaytvdyhjqpsa",
    "datasetId": "rjj0eh506x3vaytvdyhjqpsa",
    "doi": "10.5061/dryad.d9v38",
    "title": "Data from: The functional diversity of retinal ganglion cells in the mouse",
    "description": "In the vertebrate visual system, all output of the retina is carried by retinal ganglion cells. Each type encodes distinct visual features in parallel for transmission to the brain. How many such 'output channels' exist and what each encodes are areas of intense debate. In the mouse, anatomical estimates range from 15 to 20 channels, and only a handful are functionally understood. By combining two-photon calcium imaging to obtain dense retinal recordings and unsupervised clustering of the resulting sample of more than 11,000 cells, here we show that the mouse retina harbours substantially more than 30 functional output channels. These include all known and several new ganglion cell types, as verified by genetic and anatomical criteria. Therefore, information channels from the mouse eye to the mouse brain are considerably more diverse than shown thus far by anatomical studies, suggesting an encoding strategy resembling that used in state-of-the-art artificial vision systems.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "In the vertebrate visual system, all output of the retina is carried by retinal ganglion cells. Each type encodes distinct visual features in parallel for transmission to the brain. How many such 'output channels' exist and what each encodes are areas of intense debate. In the mouse, anatomical estimates range from 15 to 20 channels, and only a handful are functionally understood. By combining two-photon calcium imaging to obtain dense retinal recordings and unsupervised clustering of the resulting sample of more than 11,000 cells, here we show that the mouse retina harbours substantially more than 30 functional output channels. These include all known and several new ganglion cell types, as verified by genetic and anatomical criteria. Therefore, information channels from the mouse eye to the mouse brain are considerably more diverse than shown thus far by anatomical studies, suggesting an encoding strategy resembling that used in state-of-the-art artificial vision systems.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.d9v38",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: The functional diversity of retinal ganglion cells in the mouse"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Baden, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Berens, Philipp",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Franke, Katrin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Rom\u00e1n Ros\u00f3n, Miroslav",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Bethge, Matthias",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Euler, Thomas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Bernstein Center for Computational Neuroscience T\u00fcbingen"
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-12-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "In the vertebrate visual system, all output of the retina is carried by retinal ganglion cells. Each type encodes distinct visual features in parallel for transmission to the brain. How many such 'output channels' exist and what each encodes are areas of intense debate. In the mouse, anatomical estimates range from 15 to 20 channels, and only a handful are functionally understood. By combining two-photon calcium imaging to obtain dense retinal recordings and unsupervised clustering of the resulting sample of more than 11,000 cells, here we show that the mouse retina harbours substantially more than 30 functional output channels. These include all known and several new ganglion cell types, as verified by genetic and anatomical criteria. Therefore, information channels from the mouse eye to the mouse brain are considerably more diverse than shown thus far by anatomical studies, suggesting an encoding strategy resembling that used in state-of-the-art artificial vision systems.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "mouse"
          },
          {
            "subjectValue": "two-photon"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "early vision"
          },
          {
            "subjectValue": "mammal"
          },
          {
            "subjectValue": "cell types"
          },
          {
            "subjectValue": "ganglion cell"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "406.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 426141286,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5000442",
    "created": "1624208584"
  },
  {
    "id": 77,
    "canonicalId": "rxxgbmpoxxsd2xpsfc5l5p8i",
    "datasetId": "rxxgbmpoxxsd2xpsfc5l5p8i",
    "doi": "10.5281/zenodo.17187000",
    "title": "Retinal S-cone specific anatomical and physiological data in the thirteen-lined ground squirrel",
    "description": "<p>Original data corresponding to the PNAS (2025) publication titled \"S-cone specific circuitry in the outer plexiform layer of a cone-dominant mammal.\"</p>\n<p>Data is organized by type: Reconstructions, Electrophysiology, Electroretinography, and Immunohistochemistry. Additonal software may be required to view data files. Where applicable, README files have been included with brief explanations.</p>\n<p>A 3D PDF is also included that allows customizable interaction with the reconstructions presented in this publication. Downloading the file and viewing it in Adobe Acrobat Reader is required for interactivity, and you may be prompted to \"enable\" or \"trust\" the 3D media first. See the accompanying guide for additional details.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Original data corresponding to the PNAS (2025) publication titled \"S-cone specific circuitry in the outer plexiform layer of a cone-dominant mammal.\"</p>\n<p>Data is organized by type: Reconstructions, Electrophysiology, Electroretinography, and Immunohistochemistry. Additonal software may be required to view data files. Where applicable, README files have been included with brief explanations.</p>\n<p>A 3D PDF is also included that allows customizable interaction with the reconstructions presented in this publication. Downloading the file and viewing it in Adobe Acrobat Reader is required for interactivity, and you may be prompted to \"enable\" or \"trust\" the 3D media first. See the accompanying guide for additional details.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17187000",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Retinal S-cone specific anatomical and physiological data in the thirteen-lined ground squirrel"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Zhang, Yizhen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Institute of Dental and Craniofacial Research"
              }
            ]
          },
          {
            "creatorName": "Chen, Shan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          },
          {
            "creatorName": "Qian, Haohua",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          },
          {
            "creatorName": "Li, Wei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          },
          {
            "creatorName": "Ball, John",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National Eye Institute"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-09-23",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Original data corresponding to the PNAS (2025) publication titled \"S-cone specific circuitry in the outer plexiform layer of a cone-dominant mammal.\"</p>\n<p>Data is organized by type: Reconstructions, Electrophysiology, Electroretinography, and Immunohistochemistry. Additonal software may be required to view data files. Where applicable, README files have been included with brief explanations.</p>\n<p>A 3D PDF is also included that allows customizable interaction with the reconstructions presented in this publication. Downloading the file and viewing it in Adobe Acrobat Reader is required for interactivity, and you may be prompted to \"enable\" or \"trust\" the 3D media first. See the accompanying guide for additional details.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Thirteen-lined ground squirrel"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "29681.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 31122889113,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17187000",
    "created": "1763402419"
  },
  {
    "id": 78,
    "canonicalId": "zb6tbxja2ffqh2gsu1vygl0a",
    "datasetId": "zb6tbxja2ffqh2gsu1vygl0a",
    "doi": "10.5061/dryad.12jm63z04",
    "title": "Multivesicular Release LIF Spike Data",
    "description": "<p>The statistics of vesicle release determine how information is transferred in neural 12 circuits. The classical model is of Poisson synapses releasing vesicles 13 independently but ribbon synapses transmit early sensory signals by 14 multivesicular release (MVR) when two or more vesicles are coordinated as a single 15 synaptic event. To investigate the impact of MVR on the spike code we used leaky 16 integrate-and-fire models with inputs simulating the statistics of vesicle release 17 measured experimentally from retinal bipolar cells. Comparing these with models 18 of independent release we find that MVR increases spike generation and the 19 efficiency of information transfer (bits per spike) over a range of conditions that 20 mimic retinal ganglion cells of different time-constant receiving different number of 21 synaptic inputs of different strengths. When a single input drives a neuron with 22 short time-constant, as occurs when hair cells transmit auditory signals, MVR 23 increases information transfer whenever spike generation requires depolarization 24 greater than that caused by a single vesicle. This study demonstrates how 25 presynaptic integration of vesicles by MVR can compensate for less effective 26 summation post-synaptically to increase the efficiency with which sensory 27 information is transmitted at the synapse.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The statistics of vesicle release determine how information is transferred in neural 12 circuits. The classical model is of Poisson synapses releasing vesicles 13 independently but ribbon synapses transmit early sensory signals by 14 multivesicular release (MVR) when two or more vesicles are coordinated as a single 15 synaptic event. To investigate the impact of MVR on the spike code we used leaky 16 integrate-and-fire models with inputs simulating the statistics of vesicle release 17 measured experimentally from retinal bipolar cells. Comparing these with models 18 of independent release we find that MVR increases spike generation and the 19 efficiency of information transfer (bits per spike) over a range of conditions that 20 mimic retinal ganglion cells of different time-constant receiving different number of 21 synaptic inputs of different strengths. When a single input drives a neuron with 22 short time-constant, as occurs when hair cells transmit auditory signals, MVR 23 increases information transfer whenever spike generation requires depolarization 24 greater than that caused by a single vesicle. This study demonstrates how 25 presynaptic integration of vesicles by MVR can compensate for less effective 26 summation post-synaptically to increase the efficiency with which sensory 27 information is transmitted at the synapse.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.12jm63z04",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Multivesicular Release LIF Spike Data"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "James, Benjamin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Lagnado, Leon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Sussex"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-11-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The statistics of vesicle release determine how information is transferred in neural 12 circuits. The classical model is of Poisson synapses releasing vesicles 13 independently but ribbon synapses transmit early sensory signals by 14 multivesicular release (MVR) when two or more vesicles are coordinated as a single 15 synaptic event. To investigate the impact of MVR on the spike code we used leaky 16 integrate-and-fire models with inputs simulating the statistics of vesicle release 17 measured experimentally from retinal bipolar cells. Comparing these with models 18 of independent release we find that MVR increases spike generation and the 19 efficiency of information transfer (bits per spike) over a range of conditions that 20 mimic retinal ganglion cells of different time-constant receiving different number of 21 synaptic inputs of different strengths. When a single input drives a neuron with 22 short time-constant, as occurs when hair cells transmit auditory signals, MVR 23 increases information transfer whenever spike generation requires depolarization 24 greater than that caused by a single vesicle. This study demonstrates how 25 presynaptic integration of vesicles by MVR can compensate for less effective 26 summation post-synaptically to increase the efficiency with which sensory 27 information is transmitted at the synapse.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "8304.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 8708213964,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5719204",
    "created": "1637609017"
  },
  {
    "id": 79,
    "canonicalId": "w634kun74iwoyw4ttaad2bjs",
    "datasetId": "w634kun74iwoyw4ttaad2bjs",
    "doi": "10.5281/zenodo.3820402",
    "title": "Optogenetic response of mouse and macaque retinal ganglion cells",
    "description": "<p>Data presented in &quot;Towards optogenetic vision restoration with high resolution&quot;<br>\nBioRxiv https://www.biorxiv.org/content/10.1101/470773v1,&nbsp;to appear in PLOS Computational Biology.<br>\nPlease cite this paper if the data have been useful for you.</p>\n\n<p>Data contains multi-electrode array recordings of optogenetic response of&nbsp;retinal ganglion cells subject to checkerboard visual stimulation.<br>\nMouse: blind rd1 mice (4-5 weeks old) with an AAV2&nbsp; encoding ReaChR-mCitrine under a pan-neuronal hSyn promoter&nbsp;via intravitreal injections.&nbsp;<br>\nMacaque:&nbsp;retinal ganglion cells have been targeted with an AAV2 encoding&nbsp;a human codon optimized CatCh under a strong, RGC-specific promoter.&nbsp;Retinas were harvested three months after injection of the virus&nbsp;in the adult macaque retina.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Data presented in &quot;Towards optogenetic vision restoration with high resolution&quot;<br>\nBioRxiv https://www.biorxiv.org/content/10.1101/470773v1,&nbsp;to appear in PLOS Computational Biology.<br>\nPlease cite this paper if the data have been useful for you.</p>\n\n<p>Data contains multi-electrode array recordings of optogenetic response of&nbsp;retinal ganglion cells subject to checkerboard visual stimulation.<br>\nMouse: blind rd1 mice (4-5 weeks old) with an AAV2&nbsp; encoding ReaChR-mCitrine under a pan-neuronal hSyn promoter&nbsp;via intravitreal injections.&nbsp;<br>\nMacaque:&nbsp;retinal ganglion cells have been targeted with an AAV2 encoding&nbsp;a human codon optimized CatCh under a strong, RGC-specific promoter.&nbsp;Retinas were harvested three months after injection of the virus&nbsp;in the adult macaque retina.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.3820402",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Optogenetic response of mouse and macaque retinal ganglion cells"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ulisse Ferrari",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institut de la Vision, CNRS, SU, INSERM"
              }
            ]
          },
          {
            "creatorName": "St\u00e9phane Deny",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institut de la Vision, CNRS, SU, INSERM"
              }
            ]
          },
          {
            "creatorName": "Olivier Marre",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institut de la Vision, CNRS, SU, INSERM"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-05-11",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Data presented in &quot;Towards optogenetic vision restoration with high resolution&quot;<br>\nBioRxiv https://www.biorxiv.org/content/10.1101/470773v1,&nbsp;to appear in PLOS Computational Biology.<br>\nPlease cite this paper if the data have been useful for you.</p>\n\n<p>Data contains multi-electrode array recordings of optogenetic response of&nbsp;retinal ganglion cells subject to checkerboard visual stimulation.<br>\nMouse: blind rd1 mice (4-5 weeks old) with an AAV2&nbsp; encoding ReaChR-mCitrine under a pan-neuronal hSyn promoter&nbsp;via intravitreal injections.&nbsp;<br>\nMacaque:&nbsp;retinal ganglion cells have been targeted with an AAV2 encoding&nbsp;a human codon optimized CatCh under a strong, RGC-specific promoter.&nbsp;Retinas were harvested three months after injection of the virus&nbsp;in the adult macaque retina.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retina ganglion cells"
          },
          {
            "subjectValue": "optogenetic response"
          },
          {
            "subjectValue": "multielectrode array"
          },
          {
            "subjectValue": "white-noise (checkerboard) stimulus"
          },
          {
            "subjectValue": "macaque"
          },
          {
            "subjectValue": "mouse"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "98.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 102865305,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3820402",
    "created": "1589213080"
  },
  {
    "id": 80,
    "canonicalId": "c3drb5cyif61i2lhrdv8ypjz",
    "datasetId": "c3drb5cyif61i2lhrdv8ypjz",
    "doi": "10.5061/dryad.h18931zr2",
    "title": "Circuit mechanisms underlying embryonic retinal waves",
    "description": "<p>Spontaneous activity is a hallmark of developing neural systems. In the retina, spontaneous activity comes in the form of retinal waves, comprised of three stages persisting from embryonic day 16 (E16) to eye opening at postnatal day 14 (P14). Though postnatal retinal waves have been well characterized, little is known about the spatiotemporal properties or the mechanisms mediating embryonic retinal waves, designated Stage 1 waves. Using a custom-built macroscope to record spontaneous calcium transients from whole embryonic retinas, we show that Stage 1 waves are initiated at several locations across the retina and propagate across a broad range of areas. A gap junction antagonist, meclofenamic acid, reduced the frequency and size of Stage 1 waves, nearly abolishing them. The general nAChR antagonist, hexamethonium similarly nearly abolished Stage 1 waves. Application of the \u03b14\u03b22 nAChR antagonist dihydro-\u00df-erythroidine only slightly reduced the frequency of waves but significantly reduced the number of cells that participated in waves. Thus, Stage 1 waves are mediated by a complex circuitry involving subtypes of nAChRs and gap junctions. Stage 1 waves in mice lacking the \u03b22 subunit of the nAChRs (\u03b22-nAChR-KO) were reduced, but in contrast to WT mice, they persisted in the hexamethonium and were completely blocked by meclofenamic acid. To assay the impact of Stage 1 waves on retinal development, we compared the spatial distribution of a subtype of retinal ganglion cells, intrinsically photosensitive retinal ganglion cells (ipRGCs), which undergo a significant amount of cell death, in WT and \u03b22-nAChR-KO mice. We found that the developmental decrease of ipRGC density is preserved between WT and \u03b22-nAChR-KO mice, indicating that processes regulating ipRGC distribution are not influenced by spontaneous activity.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Spontaneous activity is a hallmark of developing neural systems. In the retina, spontaneous activity comes in the form of retinal waves, comprised of three stages persisting from embryonic day 16 (E16) to eye opening at postnatal day 14 (P14). Though postnatal retinal waves have been well characterized, little is known about the spatiotemporal properties or the mechanisms mediating embryonic retinal waves, designated Stage 1 waves. Using a custom-built macroscope to record spontaneous calcium transients from whole embryonic retinas, we show that Stage 1 waves are initiated at several locations across the retina and propagate across a broad range of areas. A gap junction antagonist, meclofenamic acid, reduced the frequency and size of Stage 1 waves, nearly abolishing them. The general nAChR antagonist, hexamethonium similarly nearly abolished Stage 1 waves. Application of the \u03b14\u03b22 nAChR antagonist dihydro-\u00df-erythroidine only slightly reduced the frequency of waves but significantly reduced the number of cells that participated in waves. Thus, Stage 1 waves are mediated by a complex circuitry involving subtypes of nAChRs and gap junctions. Stage 1 waves in mice lacking the \u03b22 subunit of the nAChRs (\u03b22-nAChR-KO) were reduced, but in contrast to WT mice, they persisted in the hexamethonium and were completely blocked by meclofenamic acid. To assay the impact of Stage 1 waves on retinal development, we compared the spatial distribution of a subtype of retinal ganglion cells, intrinsically photosensitive retinal ganglion cells (ipRGCs), which undergo a significant amount of cell death, in WT and \u03b22-nAChR-KO mice. We found that the developmental decrease of ipRGC density is preserved between WT and \u03b22-nAChR-KO mice, indicating that processes regulating ipRGC distribution are not influenced by spontaneous activity.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.h18931zr2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Circuit mechanisms underlying embryonic retinal waves"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Voufo, Christiane",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Chen, Andy",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Smith, Benjamin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Yan, Rongshan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Feller, Marla",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Tiriac, Alexandre",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Vanderbilt University"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-01-31",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Spontaneous activity is a hallmark of developing neural systems. In the retina, spontaneous activity comes in the form of retinal waves, comprised of three stages persisting from embryonic day 16 (E16) to eye opening at postnatal day 14 (P14). Though postnatal retinal waves have been well characterized, little is known about the spatiotemporal properties or the mechanisms mediating embryonic retinal waves, designated Stage 1 waves. Using a custom-built macroscope to record spontaneous calcium transients from whole embryonic retinas, we show that Stage 1 waves are initiated at several locations across the retina and propagate across a broad range of areas. A gap junction antagonist, meclofenamic acid, reduced the frequency and size of Stage 1 waves, nearly abolishing them. The general nAChR antagonist, hexamethonium similarly nearly abolished Stage 1 waves. Application of the \u03b14\u03b22 nAChR antagonist dihydro-\u00df-erythroidine only slightly reduced the frequency of waves but significantly reduced the number of cells that participated in waves. Thus, Stage 1 waves are mediated by a complex circuitry involving subtypes of nAChRs and gap junctions. Stage 1 waves in mice lacking the \u03b22 subunit of the nAChRs (\u03b22-nAChR-KO) were reduced, but in contrast to WT mice, they persisted in the hexamethonium and were completely blocked by meclofenamic acid. To assay the impact of Stage 1 waves on retinal development, we compared the spatial distribution of a subtype of retinal ganglion cells, intrinsically photosensitive retinal ganglion cells (ipRGCs), which undergo a significant amount of cell death, in WT and \u03b22-nAChR-KO mice. We found that the developmental decrease of ipRGC density is preserved between WT and \u03b22-nAChR-KO mice, indicating that processes regulating ipRGC distribution are not influenced by spontaneous activity.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "spontaneous activity"
          },
          {
            "subjectValue": "embryonic"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "calcium imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "7556.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 7923879116,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7592834",
    "created": "1675222774"
  },
  {
    "id": 81,
    "canonicalId": "e30ua81x2l8f3ptbnq13l55f",
    "datasetId": "e30ua81x2l8f3ptbnq13l55f",
    "doi": "10.5061/dryad.66t1g1k1x",
    "title": "Examples of misclassified images of papilledema severity by the Deep Learning System",
    "description": "<p>The study's objective is to evaluate the performance of a deep learning system (DLS) in classifying the severity of papilledema associated with increased intracranial pressure, on standard retinal fundus photographs.</p>\n\n<p>A DLS was trained to automatically classify papilledema severity in 965 patients (2103 mydriatic fundus photographs), representing a multiethnic cohort of patients with confirmed elevated intracranial pressure. Training was performed on 1052 photographs with mild/moderate papilledema (MP) and 1051 photographs with severe papilledema (SP) classified by a panel of experts, and the performance of the DLS was tested\u00a0in 111 patients (214 photographs, 92 with MP and 122 with SP).</p>\n\n<p>In this dataset, we provide illustrative examples of misclassified images by the DLS, two examples of images wrongly classified as moderate papilledema instead of severe (figure 1A and 1B), and two examples of images wrongly classified as severe papilledema instead of moderate (figure 2A and 2B).</p>\n\n<p>Unsurprisingly, DLS errors occurred more often in patients with moderate papilledema (Fris\u00e9n 3 severity), a situation already encountered in clinical studies.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The study's objective is to evaluate the performance of a deep learning system (DLS) in classifying the severity of papilledema associated with increased intracranial pressure, on standard retinal fundus photographs.</p>\n\n<p>A DLS was trained to automatically classify papilledema severity in 965 patients (2103 mydriatic fundus photographs), representing a multiethnic cohort of patients with confirmed elevated intracranial pressure. Training was performed on 1052 photographs with mild/moderate papilledema (MP) and 1051 photographs with severe papilledema (SP) classified by a panel of experts, and the performance of the DLS was tested\u00a0in 111 patients (214 photographs, 92 with MP and 122 with SP).</p>\n\n<p>In this dataset, we provide illustrative examples of misclassified images by the DLS, two examples of images wrongly classified as moderate papilledema instead of severe (figure 1A and 1B), and two examples of images wrongly classified as severe papilledema instead of moderate (figure 2A and 2B).</p>\n\n<p>Unsurprisingly, DLS errors occurred more often in patients with moderate papilledema (Fris\u00e9n 3 severity), a situation already encountered in clinical studies.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.66t1g1k1x",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Examples of misclassified images of papilledema severity by the Deep Learning System"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Vasseneix, Caroline",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Najjar, Raymond P",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          },
          {
            "creatorName": "Milea, Dan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Singapore Eye Research Institute"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-04-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The study's objective is to evaluate the performance of a deep learning system (DLS) in classifying the severity of papilledema associated with increased intracranial pressure, on standard retinal fundus photographs.</p>\n\n<p>A DLS was trained to automatically classify papilledema severity in 965 patients (2103 mydriatic fundus photographs), representing a multiethnic cohort of patients with confirmed elevated intracranial pressure. Training was performed on 1052 photographs with mild/moderate papilledema (MP) and 1051 photographs with severe papilledema (SP) classified by a panel of experts, and the performance of the DLS was tested\u00a0in 111 patients (214 photographs, 92 with MP and 122 with SP).</p>\n\n<p>In this dataset, we provide illustrative examples of misclassified images by the DLS, two examples of images wrongly classified as moderate papilledema instead of severe (figure 1A and 1B), and two examples of images wrongly classified as severe papilledema instead of moderate (figure 2A and 2B).</p>\n\n<p>Unsurprisingly, DLS errors occurred more often in patients with moderate papilledema (Fris\u00e9n 3 severity), a situation already encountered in clinical studies.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1782579,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6506353",
    "created": "1651258888"
  },
  {
    "id": 82,
    "canonicalId": "j2w5obk2ye6hqk567jliqhep",
    "datasetId": "j2w5obk2ye6hqk567jliqhep",
    "doi": "10.5061/dryad.56f1h",
    "title": "Data from: Quantitative analysis of fundus-image sequences reveals phase of spontaneous venous pulsations",
    "description": "Purpose: Spontaneous venous pulsation correlates negatively with elevated intracranial pressure and papilledema, and it relates to glaucoma. Yet, its etiology remains unclear. A key element to elucidate its underlying mechanism is the time at which collapse occurs with respect to the heart cycle, but previous reports are contradictory. We assessed this question in healthy subjects using quantitative measurements of both vein diameters and artery lateral displacements; the latter being used as the marker of the ocular systole time. Methods: We recorded 5-second fundus sequences with a near-infrared scanning laser ophthalmoscope in 12 young healthy subjects. The image sequences were coregistered, cleaned from microsaccades, and filtered via a principal component analysis to remove nonpulsatile dynamic features. Time courses of arterial lateral displacement and of diameter at sites of spontaneous venous pulsation or proximal to the disk were retrieved from those image sequences and compared. Results: Four subjects displayed both arterial and venous pulsatile waveforms. On those, we observed venous diameter waveforms differing markedly among the subjects, ranging from a waveform matching the typical intraocular pressure waveform to a close replica of the arterial waveform. Conclusions: The heterogeneity in waveforms and arteriovenous phases suggests that the mechanism governing the venous outflow resistance differs among healthy subjects. Translational relevance: Further characterizations are necessary to understand the heterogeneous mechanisms governing the venous outflow resistance as this resistance is altered in glaucoma and is instrumental when monitoring intracranial hypertension based on fundus observations.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Purpose: Spontaneous venous pulsation correlates negatively with elevated intracranial pressure and papilledema, and it relates to glaucoma. Yet, its etiology remains unclear. A key element to elucidate its underlying mechanism is the time at which collapse occurs with respect to the heart cycle, but previous reports are contradictory. We assessed this question in healthy subjects using quantitative measurements of both vein diameters and artery lateral displacements; the latter being used as the marker of the ocular systole time. Methods: We recorded 5-second fundus sequences with a near-infrared scanning laser ophthalmoscope in 12 young healthy subjects. The image sequences were coregistered, cleaned from microsaccades, and filtered via a principal component analysis to remove nonpulsatile dynamic features. Time courses of arterial lateral displacement and of diameter at sites of spontaneous venous pulsation or proximal to the disk were retrieved from those image sequences and compared. Results: Four subjects displayed both arterial and venous pulsatile waveforms. On those, we observed venous diameter waveforms differing markedly among the subjects, ranging from a waveform matching the typical intraocular pressure waveform to a close replica of the arterial waveform. Conclusions: The heterogeneity in waveforms and arteriovenous phases suggests that the mechanism governing the venous outflow resistance differs among healthy subjects. Translational relevance: Further characterizations are necessary to understand the heterogeneous mechanisms governing the venous outflow resistance as this resistance is altered in glaucoma and is instrumental when monitoring intracranial hypertension based on fundus observations.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.56f1h",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Quantitative analysis of fundus-image sequences reveals phase of spontaneous venous pulsations"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Moret, Fabrice",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University Medical Center Freiburg"
              }
            ]
          },
          {
            "creatorName": "Reiff, Charlotte M.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University Medical Center Freiburg"
              }
            ]
          },
          {
            "creatorName": "Lagr\u00e8ze, Wolf A.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University Medical Center Freiburg"
              }
            ]
          },
          {
            "creatorName": "Bach, Michael",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University Medical Center Freiburg"
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-07-08",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Purpose: Spontaneous venous pulsation correlates negatively with elevated intracranial pressure and papilledema, and it relates to glaucoma. Yet, its etiology remains unclear. A key element to elucidate its underlying mechanism is the time at which collapse occurs with respect to the heart cycle, but previous reports are contradictory. We assessed this question in healthy subjects using quantitative measurements of both vein diameters and artery lateral displacements; the latter being used as the marker of the ocular systole time. Methods: We recorded 5-second fundus sequences with a near-infrared scanning laser ophthalmoscope in 12 young healthy subjects. The image sequences were coregistered, cleaned from microsaccades, and filtered via a principal component analysis to remove nonpulsatile dynamic features. Time courses of arterial lateral displacement and of diameter at sites of spontaneous venous pulsation or proximal to the disk were retrieved from those image sequences and compared. Results: Four subjects displayed both arterial and venous pulsatile waveforms. On those, we observed venous diameter waveforms differing markedly among the subjects, ranging from a waveform matching the typical intraocular pressure waveform to a close replica of the arterial waveform. Conclusions: The heterogeneity in waveforms and arteriovenous phases suggests that the mechanism governing the venous outflow resistance differs among healthy subjects. Translational relevance: Further characterizations are necessary to understand the heterogeneous mechanisms governing the venous outflow resistance as this resistance is altered in glaucoma and is instrumental when monitoring intracranial hypertension based on fundus observations.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "fundus"
          },
          {
            "subjectValue": "intracranial pressure"
          },
          {
            "subjectValue": "scanning laser opthalmoscope"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "retinal blood flow"
          },
          {
            "subjectValue": "Homo Sapiens"
          },
          {
            "subjectValue": "spontaneous venous pulsation"
          },
          {
            "subjectValue": "pulsation"
          },
          {
            "subjectValue": "spontaneous retinal venous pulsation"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "113.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 118593945,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4970545",
    "created": "1623896498"
  },
  {
    "id": 83,
    "canonicalId": "baxi4co7snv6000i886m6634",
    "datasetId": "baxi4co7snv6000i886m6634",
    "doi": "10.5061/dryad.msbcc2ftc",
    "title": "A practical approach to functional optical coherence tomography shows abnormal retinal responses in Alzheimer's disease",
    "description": "<p>Spectral-domain optical coherence tomography (SD-OCT) is an accessible clinical tool for measuring structural changes to the retina, and increasingly as a biomarker for brain-predominant neurodegenerative diseases like Alzheimer's. Information about retinal function can also be extracted from OCT images, but is under-studied, with literature examples often employing challenging protocols or requiring specialized hardware. The first goal of this study was to verify that functional retinal imaging was feasible with a commercially-available SD-OCT device and a clinically practical protocol. Inspired by methods from other functional imaging modalities, we acquired images while repeatedly cycling lights on and off, and spatially normalized retinas to facilitate intra- and inter-individual analyses. In eight healthy young adults, light-dependent increases in reflectivity were easily demonstrated at photoreceptor inner and outer segments, changing by ~7% in bright light and ~3% in dim light. Bright light elicited a subtle (~2%) but consistent light-dependent decrease in reflectivity through much of the rest of the retina, including the avascular outer nuclear layer (ONL). We speculated that some of these changes are influenced by glial function \u2013 as through water management \u2013 a topic of high interest in neurodegenerative diseases that may involve the glymphatic system. Functional abnormalities in patients with antibodies against aquaporin-4 (n=3) supported this interpretation. We next compared patients with early-onset Alzheimer's disease (n=14) to age-matched controls (n=14), revealing that patients had a relatively exaggerated light-induced change in ONL reflectivity (p&lt;0.05). Because these measurements can be obtained within thirty minutes, regular use in research and limited clinical settings is feasible.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Spectral-domain optical coherence tomography (SD-OCT) is an accessible clinical tool for measuring structural changes to the retina, and increasingly as a biomarker for brain-predominant neurodegenerative diseases like Alzheimer's. Information about retinal function can also be extracted from OCT images, but is under-studied, with literature examples often employing challenging protocols or requiring specialized hardware. The first goal of this study was to verify that functional retinal imaging was feasible with a commercially-available SD-OCT device and a clinically practical protocol. Inspired by methods from other functional imaging modalities, we acquired images while repeatedly cycling lights on and off, and spatially normalized retinas to facilitate intra- and inter-individual analyses. In eight healthy young adults, light-dependent increases in reflectivity were easily demonstrated at photoreceptor inner and outer segments, changing by ~7% in bright light and ~3% in dim light. Bright light elicited a subtle (~2%) but consistent light-dependent decrease in reflectivity through much of the rest of the retina, including the avascular outer nuclear layer (ONL). We speculated that some of these changes are influenced by glial function \u2013 as through water management \u2013 a topic of high interest in neurodegenerative diseases that may involve the glymphatic system. Functional abnormalities in patients with antibodies against aquaporin-4 (n=3) supported this interpretation. We next compared patients with early-onset Alzheimer's disease (n=14) to age-matched controls (n=14), revealing that patients had a relatively exaggerated light-induced change in ONL reflectivity (p&lt;0.05). Because these measurements can be obtained within thirty minutes, regular use in research and limited clinical settings is feasible.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.msbcc2ftc",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A practical approach to functional optical coherence tomography shows abnormal retinal responses in Alzheimer's disease"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Bissig, David",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Oregon Health & Science University"
              }
            ]
          },
          {
            "creatorName": "Zhou, Clarice",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Oregon Health & Science University"
              }
            ]
          },
          {
            "creatorName": "Le, Vy",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Oregon Health & Science University"
              }
            ]
          },
          {
            "creatorName": "Bernard, Jacqueline",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Oregon Health & Science University"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-03-13",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Spectral-domain optical coherence tomography (SD-OCT) is an accessible clinical tool for measuring structural changes to the retina, and increasingly as a biomarker for brain-predominant neurodegenerative diseases like Alzheimer's. Information about retinal function can also be extracted from OCT images, but is under-studied, with literature examples often employing challenging protocols or requiring specialized hardware. The first goal of this study was to verify that functional retinal imaging was feasible with a commercially-available SD-OCT device and a clinically practical protocol. Inspired by methods from other functional imaging modalities, we acquired images while repeatedly cycling lights on and off, and spatially normalized retinas to facilitate intra- and inter-individual analyses. In eight healthy young adults, light-dependent increases in reflectivity were easily demonstrated at photoreceptor inner and outer segments, changing by ~7% in bright light and ~3% in dim light. Bright light elicited a subtle (~2%) but consistent light-dependent decrease in reflectivity through much of the rest of the retina, including the avascular outer nuclear layer (ONL). We speculated that some of these changes are influenced by glial function \u2013 as through water management \u2013 a topic of high interest in neurodegenerative diseases that may involve the glymphatic system. Functional abnormalities in patients with antibodies against aquaporin-4 (n=3) supported this interpretation. We next compared patients with early-onset Alzheimer's disease (n=14) to age-matched controls (n=14), revealing that patients had a relatively exaggerated light-induced change in ONL reflectivity (p&lt;0.05). Because these measurements can be obtained within thirty minutes, regular use in research and limited clinical settings is feasible.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "optical coherence tomography"
          },
          {
            "subjectValue": "retinal function"
          },
          {
            "subjectValue": "light-dependent"
          },
          {
            "subjectValue": "retinal reflectivity"
          },
          {
            "subjectValue": "Neuromyelitis Optica"
          },
          {
            "subjectValue": "aquaporin"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "804.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 843474534,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4935639",
    "created": "1623496985"
  },
  {
    "id": 84,
    "canonicalId": "qn9kqnatun2cpqwdzo0hnu7x",
    "datasetId": "qn9kqnatun2cpqwdzo0hnu7x",
    "doi": "10.5061/dryad.2z34tmpnc",
    "title": "Visual pursuit behavior in mice maintains the pursued prey on the retinal region with least optic flow",
    "description": "<p>Mice have a large visual field that is constantly stabilized by vestibular ocular reflex (VOR) driven eye rotations that counter head-rotations. While maintaining their extensive visual coverage is advantageous for predator detection, mice also track and capture prey using vision. However, in the freely moving animal quantifying object location in the field of view is challenging. Here, we developed a method to digitally reconstruct and quantify the visual scene of freely moving mice performing a visually based prey capture task. By isolating the visual sense and combining a mouse eye optic model with the head and eye rotations, the detailed reconstruction of the digital environment and retinal features were projected onto the corneal surface for comparison, and updated throughout the behavior. By quantifying the spatial location of objects in the visual scene and their motion throughout the behavior, we show that the prey image consistently falls within a small area of the VOR-stabilized visual field. This functional focus coincides with the region of minimal optic flow in the visual field and consequently minimal motion-induced image blur during pursuit. The functional focus lies in the upper-temporal part of the retina and coincides with the reported high density-region of Alpha-ON<sub>\u00a0</sub>sustained retinal ganglion cells.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Mice have a large visual field that is constantly stabilized by vestibular ocular reflex (VOR) driven eye rotations that counter head-rotations. While maintaining their extensive visual coverage is advantageous for predator detection, mice also track and capture prey using vision. However, in the freely moving animal quantifying object location in the field of view is challenging. Here, we developed a method to digitally reconstruct and quantify the visual scene of freely moving mice performing a visually based prey capture task. By isolating the visual sense and combining a mouse eye optic model with the head and eye rotations, the detailed reconstruction of the digital environment and retinal features were projected onto the corneal surface for comparison, and updated throughout the behavior. By quantifying the spatial location of objects in the visual scene and their motion throughout the behavior, we show that the prey image consistently falls within a small area of the VOR-stabilized visual field. This functional focus coincides with the region of minimal optic flow in the visual field and consequently minimal motion-induced image blur during pursuit. The functional focus lies in the upper-temporal part of the retina and coincides with the reported high density-region of Alpha-ON<sub>\u00a0</sub>sustained retinal ganglion cells.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.2z34tmpnc",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Visual pursuit behavior in mice maintains the pursued prey on the retinal region with least optic flow"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Holmgren, Carl D.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Stahr, Paul",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Wallace, Damian J.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Voit, Kay-Michael",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Matheson, Emily J.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Sawinski, Juergen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Bassetto, Giacomo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          },
          {
            "creatorName": "Kerr, Jason N.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Center of Advanced European Studies and Research"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-10-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Mice have a large visual field that is constantly stabilized by vestibular ocular reflex (VOR) driven eye rotations that counter head-rotations. While maintaining their extensive visual coverage is advantageous for predator detection, mice also track and capture prey using vision. However, in the freely moving animal quantifying object location in the field of view is challenging. Here, we developed a method to digitally reconstruct and quantify the visual scene of freely moving mice performing a visually based prey capture task. By isolating the visual sense and combining a mouse eye optic model with the head and eye rotations, the detailed reconstruction of the digital environment and retinal features were projected onto the corneal surface for comparison, and updated throughout the behavior. By quantifying the spatial location of objects in the visual scene and their motion throughout the behavior, we show that the prey image consistently falls within a small area of the VOR-stabilized visual field. This functional focus coincides with the region of minimal optic flow in the visual field and consequently minimal motion-induced image blur during pursuit. The functional focus lies in the upper-temporal part of the retina and coincides with the reported high density-region of Alpha-ON<sub>\u00a0</sub>sustained retinal ganglion cells.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "949.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 995518054,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5576696",
    "created": "1634591664"
  },
  {
    "id": 85,
    "canonicalId": "q7bu3rzk5bmw7bvjf7bl7auv",
    "datasetId": "q7bu3rzk5bmw7bvjf7bl7auv",
    "doi": "10.5281/zenodo.15873244",
    "title": "Involvement of Microglia in RGC Injury Induced by IOP Elevation in a Rat Ex Vivo Model",
    "description": "<p>This dataset supports the findings of the manuscript titled:</p>\n<p>\"Involvement of microglia in retinal ganglion cell injury induced by IOP elevation in a rat ex vivo acute glaucoma model.\"</p>\n<p>In this study, we used a rat ex vivo model of acute intraocular pressure (IOP) elevation to investigate the role of microglia in retinal ganglion cell (RGC) injury. Our results demonstrate that pressure-induced activation of the TLR4-NLRP3 inflammasome cascade contributes to microglial proliferation and IL-1&beta; production, leading to RGC apoptosis. Pharmacological depletion of microglia using the CSF-1R inhibitor PLX5622 suppressed this inflammatory pathway and preserved RGCs.</p>\n<p>The uploaded files include:<br>- Raw and normalized data from flat-mounted immunostaining (RGC and microglia counts), Western blot membranes (protein expression), TUNEL-stained cryosections (apoptosis), and Epon-embedded sections (NFL thickness and NDS evaluation).<br>- Source data used to generate the figures.</p>\n<p>This dataset is made available to promote transparency and reproducibility in glaucoma research, particularly in studies involving retinal neuroinflammation and microglial modulation.</p>",
    "versionTitle": "Version 2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This dataset supports the findings of the manuscript titled:</p>\n<p>\"Involvement of microglia in retinal ganglion cell injury induced by IOP elevation in a rat ex vivo acute glaucoma model.\"</p>\n<p>In this study, we used a rat ex vivo model of acute intraocular pressure (IOP) elevation to investigate the role of microglia in retinal ganglion cell (RGC) injury. Our results demonstrate that pressure-induced activation of the TLR4-NLRP3 inflammasome cascade contributes to microglial proliferation and IL-1&beta; production, leading to RGC apoptosis. Pharmacological depletion of microglia using the CSF-1R inhibitor PLX5622 suppressed this inflammatory pathway and preserved RGCs.</p>\n<p>The uploaded files include:<br>- Raw and normalized data from flat-mounted immunostaining (RGC and microglia counts), Western blot membranes (protein expression), TUNEL-stained cryosections (apoptosis), and Epon-embedded sections (NFL thickness and NDS evaluation).<br>- Source data used to generate the figures.</p>\n<p>This dataset is made available to promote transparency and reproducibility in glaucoma research, particularly in studies involving retinal neuroinflammation and microglial modulation.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.15873244",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Involvement of Microglia in RGC Injury Induced by IOP Elevation in a Rat Ex Vivo Model"
          }
        ],
        "version": "Version 2",
        "creator": [
          {
            "creatorName": "Sato, Taimu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Ishikawa, Makoto",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Izumi, Yukitoshi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Psychiatry, Washington University School of Medicine, St. Louis, MO, USA"
              }
            ]
          },
          {
            "creatorName": "Shibata, Naoya",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Aoba Eye Clinic, Akita, Japan"
              }
            ]
          },
          {
            "creatorName": "Kota, Sato",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Ohno-Oishi, Michiko",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Tawarayama, Hiroshi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Kunikata, Hiroshi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          },
          {
            "creatorName": "Zorumski, Charles F.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Psychiatry, Washington University School of Medicine, St. Louis, MO, USA"
              }
            ]
          },
          {
            "creatorName": "Nakawaza, Toru",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Tohoku University Graduate School of Medicine, Japan"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-05-25",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This dataset supports the findings of the manuscript titled:</p>\n<p>\"Involvement of microglia in retinal ganglion cell injury induced by IOP elevation in a rat ex vivo acute glaucoma model.\"</p>\n<p>In this study, we used a rat ex vivo model of acute intraocular pressure (IOP) elevation to investigate the role of microglia in retinal ganglion cell (RGC) injury. Our results demonstrate that pressure-induced activation of the TLR4-NLRP3 inflammasome cascade contributes to microglial proliferation and IL-1&beta; production, leading to RGC apoptosis. Pharmacological depletion of microglia using the CSF-1R inhibitor PLX5622 suppressed this inflammatory pathway and preserved RGCs.</p>\n<p>The uploaded files include:<br>- Raw and normalized data from flat-mounted immunostaining (RGC and microglia counts), Western blot membranes (protein expression), TUNEL-stained cryosections (apoptosis), and Epon-embedded sections (NFL thickness and NDS evaluation).<br>- Source data used to generate the figures.</p>\n<p>This dataset is made available to promote transparency and reproducibility in glaucoma research, particularly in studies involving retinal neuroinflammation and microglial modulation.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Glaucoma"
          },
          {
            "subjectValue": "Microglia"
          },
          {
            "subjectValue": "PLX5622"
          },
          {
            "subjectValue": "RGC"
          },
          {
            "subjectValue": "NLRP3"
          },
          {
            "subjectValue": "Neuroinflammation"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "186.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 195873996,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/15873244",
    "created": "1752409541"
  },
  {
    "id": 86,
    "canonicalId": "unliw2vsbljv5cuxqc1f7bzq",
    "datasetId": "unliw2vsbljv5cuxqc1f7bzq",
    "doi": "10.5061/dryad.7r7n7",
    "title": "Data from: Spike-triggered covariance analysis reveals phenomenological diversity of contrast adaptation in the retina",
    "description": "When visual contrast changes, retinal ganglion cells adapt by adjusting their sensitivity as well as their temporal filtering characteristics. The latter has classically been described by contrast-induced gain changes that depend on temporal frequency. Here, we explored a new perspective on contrast-induced changes in temporal filtering by using spike-triggered covariance analysis to extract multiple parallel temporal filters for individual ganglion cells. Based on multielectrode-array recordings from ganglion cells in the isolated salamander retina, we found that contrast adaptation of temporal filtering can largely be captured by contrast-invariant sets of filters with contrast-dependent weights. Moreover, differences among the ganglion cells in the filter sets and their contrast-dependent contributions allowed us to phenomenologically distinguish three types of filter changes. The first type is characterized by newly emerging features at higher contrast, which can be reproduced by computational models that contain response-triggered gain-control mechanisms. The second type follows from stronger adaptation in the Off pathway as compared to the On pathway in On-Off-type ganglion cells. Finally, we found that, in a subset of neurons, contrast-induced filter changes are governed by particularly strong spike-timing dynamics, in particular by pronounced stimulus-dependent latency shifts that can be observed in these cells. Together, our results show that the contrast dependence of temporal filtering in retinal ganglion cells has a multifaceted phenomenology and that a multi-filter analysis can provide a useful basis for capturing the underlying signal-processing dynamics.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "When visual contrast changes, retinal ganglion cells adapt by adjusting their sensitivity as well as their temporal filtering characteristics. The latter has classically been described by contrast-induced gain changes that depend on temporal frequency. Here, we explored a new perspective on contrast-induced changes in temporal filtering by using spike-triggered covariance analysis to extract multiple parallel temporal filters for individual ganglion cells. Based on multielectrode-array recordings from ganglion cells in the isolated salamander retina, we found that contrast adaptation of temporal filtering can largely be captured by contrast-invariant sets of filters with contrast-dependent weights. Moreover, differences among the ganglion cells in the filter sets and their contrast-dependent contributions allowed us to phenomenologically distinguish three types of filter changes. The first type is characterized by newly emerging features at higher contrast, which can be reproduced by computational models that contain response-triggered gain-control mechanisms. The second type follows from stronger adaptation in the Off pathway as compared to the On pathway in On-Off-type ganglion cells. Finally, we found that, in a subset of neurons, contrast-induced filter changes are governed by particularly strong spike-timing dynamics, in particular by pronounced stimulus-dependent latency shifts that can be observed in these cells. Together, our results show that the contrast dependence of temporal filtering in retinal ganglion cells has a multifaceted phenomenology and that a multi-filter analysis can provide a useful basis for capturing the underlying signal-processing dynamics.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.7r7n7",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Spike-triggered covariance analysis reveals phenomenological diversity of contrast adaptation in the retina"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Liu, Jian K.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universit\u00e4tsmedizin G\u00f6ttingen"
              }
            ]
          },
          {
            "creatorName": "Gollisch, Tim",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universit\u00e4tsmedizin G\u00f6ttingen"
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-06-27",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "When visual contrast changes, retinal ganglion cells adapt by adjusting their sensitivity as well as their temporal filtering characteristics. The latter has classically been described by contrast-induced gain changes that depend on temporal frequency. Here, we explored a new perspective on contrast-induced changes in temporal filtering by using spike-triggered covariance analysis to extract multiple parallel temporal filters for individual ganglion cells. Based on multielectrode-array recordings from ganglion cells in the isolated salamander retina, we found that contrast adaptation of temporal filtering can largely be captured by contrast-invariant sets of filters with contrast-dependent weights. Moreover, differences among the ganglion cells in the filter sets and their contrast-dependent contributions allowed us to phenomenologically distinguish three types of filter changes. The first type is characterized by newly emerging features at higher contrast, which can be reproduced by computational models that contain response-triggered gain-control mechanisms. The second type follows from stronger adaptation in the Off pathway as compared to the On pathway in On-Off-type ganglion cells. Finally, we found that, in a subset of neurons, contrast-induced filter changes are governed by particularly strong spike-timing dynamics, in particular by pronounced stimulus-dependent latency shifts that can be observed in these cells. Together, our results show that the contrast dependence of temporal filtering in retinal ganglion cells has a multifaceted phenomenology and that a multi-filter analysis can provide a useful basis for capturing the underlying signal-processing dynamics.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "spike-triggered average"
          },
          {
            "subjectValue": "spike timing dynamics"
          },
          {
            "subjectValue": "contrast adaptation"
          },
          {
            "subjectValue": "retina"
          },
          {
            "subjectValue": "ganglion cell"
          },
          {
            "subjectValue": "spike-triggered covariance"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "60.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 63229132,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4988269",
    "created": "1624052495"
  },
  {
    "id": 87,
    "canonicalId": "xnpuzcdcelpj0wer4iu0zsqm",
    "datasetId": "xnpuzcdcelpj0wer4iu0zsqm",
    "doi": "10.7272/Q6RF5RZX",
    "title": "Retinofugal projections from melanopsin-expressing retinal ganglion cells revealed by intraocular injections of Cre-dependent virus",
    "description": "To understand visual functions mediated by intrinsically photosensitive melanopsin-expressing retinal ganglion cells (mRGCs), it is important to elucidate axonal projections from these cells into the brain.  Initial studies reported that melanopsin is expressed only in retinal ganglion cells within the eye. However, recent studies in Opn4-Cre mice revealed Cre-mediated marker expression in multiple brain areas. These discoveries complicate the use of melanopsin-driven genetic labeling techniques to identify retinofugal projections specifically from mRGCs. To restrict labeling to mRGCs, we developed a recombinant adeno-associated virus (AAV) carrying a Cre-dependent reporter (human placental alkaline phosphatase) that was injected into the vitreous of Opn4-Cre mouse eyes. The labeling observed in the brain of these mice was necessarily restricted specifically to retinofugal projections from mRGCs in the injected eye. We found that mRGCs innervate multiple nuclei in the basal forebrain, hypothalamus, amygdala, thalamus and midbrain. Midline structures tended to be bilaterally innervated whereas the lateral structures received mostly contralateral innervation. As validation of our approach, we found projection patterns largely corresponded with previously published results; however, we have also identified a few novel targets. Our discovery of projections to the central amygdala suggests a possible direct neural pathway for aversive responses to light in neonates. In addition, projections to the accessory optic system suggest that mRGCs play a direct role in visual tracking, responses that were previously attributed to other classes of retinal ganglion cells. Moreover, projections to the zona incerta raise the possibility that mRGCs could regulate visceral and sensory functions.  However, additional studies are needed to investigate the actual photosensitivity of mRGCs that project to the different brain areas. Also, there is a concern of \"overlabeling\" with very sensitive reporters that uncover low levels of expression.  Light evoked signaling from these cells must be shown to be of sufficient sensitivity to elicit physiologically relevant responses.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "To understand visual functions mediated by intrinsically photosensitive melanopsin-expressing retinal ganglion cells (mRGCs), it is important to elucidate axonal projections from these cells into the brain.  Initial studies reported that melanopsin is expressed only in retinal ganglion cells within the eye. However, recent studies in Opn4-Cre mice revealed Cre-mediated marker expression in multiple brain areas. These discoveries complicate the use of melanopsin-driven genetic labeling techniques to identify retinofugal projections specifically from mRGCs. To restrict labeling to mRGCs, we developed a recombinant adeno-associated virus (AAV) carrying a Cre-dependent reporter (human placental alkaline phosphatase) that was injected into the vitreous of Opn4-Cre mouse eyes. The labeling observed in the brain of these mice was necessarily restricted specifically to retinofugal projections from mRGCs in the injected eye. We found that mRGCs innervate multiple nuclei in the basal forebrain, hypothalamus, amygdala, thalamus and midbrain. Midline structures tended to be bilaterally innervated whereas the lateral structures received mostly contralateral innervation. As validation of our approach, we found projection patterns largely corresponded with previously published results; however, we have also identified a few novel targets. Our discovery of projections to the central amygdala suggests a possible direct neural pathway for aversive responses to light in neonates. In addition, projections to the accessory optic system suggest that mRGCs play a direct role in visual tracking, responses that were previously attributed to other classes of retinal ganglion cells. Moreover, projections to the zona incerta raise the possibility that mRGCs could regulate visceral and sensory functions.  However, additional studies are needed to investigate the actual photosensitivity of mRGCs that project to the different brain areas. Also, there is a concern of \"overlabeling\" with very sensitive reporters that uncover low levels of expression.  Light evoked signaling from these cells must be shown to be of sufficient sensitivity to elicit physiologically relevant responses.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.7272/Q6RF5RZX",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Retinofugal projections from melanopsin-expressing retinal ganglion cells revealed by intraocular injections of Cre-dependent virus"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Copenhagen, David R",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Delwig, Anton",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2016",
        "date": [
          {
            "dateValue": "2016-01-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "To understand visual functions mediated by intrinsically photosensitive melanopsin-expressing retinal ganglion cells (mRGCs), it is important to elucidate axonal projections from these cells into the brain.  Initial studies reported that melanopsin is expressed only in retinal ganglion cells within the eye. However, recent studies in Opn4-Cre mice revealed Cre-mediated marker expression in multiple brain areas. These discoveries complicate the use of melanopsin-driven genetic labeling techniques to identify retinofugal projections specifically from mRGCs. To restrict labeling to mRGCs, we developed a recombinant adeno-associated virus (AAV) carrying a Cre-dependent reporter (human placental alkaline phosphatase) that was injected into the vitreous of Opn4-Cre mouse eyes. The labeling observed in the brain of these mice was necessarily restricted specifically to retinofugal projections from mRGCs in the injected eye. We found that mRGCs innervate multiple nuclei in the basal forebrain, hypothalamus, amygdala, thalamus and midbrain. Midline structures tended to be bilaterally innervated whereas the lateral structures received mostly contralateral innervation. As validation of our approach, we found projection patterns largely corresponded with previously published results; however, we have also identified a few novel targets. Our discovery of projections to the central amygdala suggests a possible direct neural pathway for aversive responses to light in neonates. In addition, projections to the accessory optic system suggest that mRGCs play a direct role in visual tracking, responses that were previously attributed to other classes of retinal ganglion cells. Moreover, projections to the zona incerta raise the possibility that mRGCs could regulate visceral and sensory functions.  However, additional studies are needed to investigate the actual photosensitivity of mRGCs that project to the different brain areas. Also, there is a concern of \"overlabeling\" with very sensitive reporters that uncover low levels of expression.  Light evoked signaling from these cells must be shown to be of sufficient sensitivity to elicit physiologically relevant responses.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "melanopsin ganglion cell"
          },
          {
            "subjectValue": "retinofugal projections"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "21.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 22439526,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4990921",
    "created": "1656538821"
  },
  {
    "id": 88,
    "canonicalId": "r0v617alowo939x0e4dw398c",
    "datasetId": "r0v617alowo939x0e4dw398c",
    "doi": "10.5281/zenodo.12779552",
    "title": "Fundus vessel phenotypes",
    "description": "<p>Related publication (2024): <a href=\"https://www.nature.com/articles/s41467-024-52334-1\">https://www.nature.com/articles/s41467-024-52334-1</a></p>\n<p><strong>Sumstat column information</strong></p>\n<ul>\n<li>beta: refers to the effect of having an extra copy of A2</li>\n<li>af: frequency of A2</li>\n<li>A2 is not necessarily the minor allele</li>\n<li>The sumstats are curated bgenie output. official bgenie manual: https://jmarchini.org/bgenie/#output-file-options</li>\n</ul>\n<p><strong>Label map </strong>(maps names used in the paper to the sumstat labels)</p>\n<p>tif_dict = { 'A temporal angle': 'mean_angle_taa', 'V temporal angle': 'mean_angle_tva', 'V tortuosity': 'tau1_vein', 'A tortuosity': 'tau1_artery', 'ratio tortuosity': 'ratio_AV_DF', 'A central retinal eq': 'eq_CRAE', 'ratio central retinal eq': 'ratio_CRAE_CRVE', 'A std diameter': 'D_A_std', 'V std diameter': 'D_V_std', 'V central retinal eq': 'eq_CRVE', 'ratio vascular density': 'ratio_VD', 'A vascular density': 'VD_orig_artery', 'bifurcations': 'bifurcations', 'V vascular density': 'VD_orig_vein', 'A median diameter': 'medianDiameter_artery', 'V median diameter': 'medianDiameter_vein', 'ratio median diameter': 'ratio_AV_medianDiameter' }</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Related publication (2024): <a href=\"https://www.nature.com/articles/s41467-024-52334-1\">https://www.nature.com/articles/s41467-024-52334-1</a></p>\n<p><strong>Sumstat column information</strong></p>\n<ul>\n<li>beta: refers to the effect of having an extra copy of A2</li>\n<li>af: frequency of A2</li>\n<li>A2 is not necessarily the minor allele</li>\n<li>The sumstats are curated bgenie output. official bgenie manual: https://jmarchini.org/bgenie/#output-file-options</li>\n</ul>\n<p><strong>Label map </strong>(maps names used in the paper to the sumstat labels)</p>\n<p>tif_dict = { 'A temporal angle': 'mean_angle_taa', 'V temporal angle': 'mean_angle_tva', 'V tortuosity': 'tau1_vein', 'A tortuosity': 'tau1_artery', 'ratio tortuosity': 'ratio_AV_DF', 'A central retinal eq': 'eq_CRAE', 'ratio central retinal eq': 'ratio_CRAE_CRVE', 'A std diameter': 'D_A_std', 'V std diameter': 'D_V_std', 'V central retinal eq': 'eq_CRVE', 'ratio vascular density': 'ratio_VD', 'A vascular density': 'VD_orig_artery', 'bifurcations': 'bifurcations', 'V vascular density': 'VD_orig_vein', 'A median diameter': 'medianDiameter_artery', 'V median diameter': 'medianDiameter_vein', 'ratio median diameter': 'ratio_AV_medianDiameter' }</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.12779552",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Fundus vessel phenotypes"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Beyeler, Michael",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Lausanne"
              }
            ]
          },
          {
            "creatorName": "Ortin Vela, Sofia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Lausanne"
              }
            ]
          },
          {
            "creatorName": "Bergmann, Sven",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Lausanne"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-07-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Related publication (2024): <a href=\"https://www.nature.com/articles/s41467-024-52334-1\">https://www.nature.com/articles/s41467-024-52334-1</a></p>\n<p><strong>Sumstat column information</strong></p>\n<ul>\n<li>beta: refers to the effect of having an extra copy of A2</li>\n<li>af: frequency of A2</li>\n<li>A2 is not necessarily the minor allele</li>\n<li>The sumstats are curated bgenie output. official bgenie manual: https://jmarchini.org/bgenie/#output-file-options</li>\n</ul>\n<p><strong>Label map </strong>(maps names used in the paper to the sumstat labels)</p>\n<p>tif_dict = { 'A temporal angle': 'mean_angle_taa', 'V temporal angle': 'mean_angle_tva', 'V tortuosity': 'tau1_vein', 'A tortuosity': 'tau1_artery', 'ratio tortuosity': 'ratio_AV_DF', 'A central retinal eq': 'eq_CRAE', 'ratio central retinal eq': 'ratio_CRAE_CRVE', 'A std diameter': 'D_A_std', 'V std diameter': 'D_V_std', 'V central retinal eq': 'eq_CRVE', 'ratio vascular density': 'ratio_VD', 'A vascular density': 'VD_orig_artery', 'bifurcations': 'bifurcations', 'V vascular density': 'VD_orig_vein', 'A median diameter': 'medianDiameter_artery', 'V median diameter': 'medianDiameter_vein', 'ratio median diameter': 'ratio_AV_medianDiameter' }</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "8422.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 8831945932,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/12779552",
    "created": "1721375545"
  },
  {
    "id": 89,
    "canonicalId": "wvpriy3t5z01oiyn0xr218su",
    "datasetId": "wvpriy3t5z01oiyn0xr218su",
    "doi": "10.5281/zenodo.10839556",
    "title": "OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods",
    "description": "<p>Optical coherence tomography (OCT) is a non-invasive imaging technique that has extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology.</p>\n<p>The dataset consists of the following categories and images:<br>- Age-Related Macular Degeneration - 1231 images;<br>- Diabetic Macular Edema - 147 images;<br>- Epiretinal Membrane- 155 images;<br>- Normal - 332 images;<br>- Retinal Artery Occlusion - 22 images;<br>- Retinal Vein Occlusion - 101 images;<br>- Vitreomacular Interface Disease - 76 images.</p>\n<p>This dataset is published to provide researchers and developers with access to a large set of labeled images, which contributes to the development and improvement of algorithms for the automatic processing and analysis of OCT images for early diagnosis and monitoring of eye diseases. CSV file consists of file_name, disease, subcategory, condition, patient_id, eye, sex, year, image_width, and image_height. The dataset will be updated periodically.</p>\n<p>&nbsp;</p>\n<p>For more information and details about the dataset see:</p>\n<p>https://rdcu.be/dELrE</p>\n<p>https://arxiv.org/abs/2312.08255</p>\n<pre>@article{kulyabin2024octdl,\n  title={OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods},\n  author={Kulyabin, Mikhail and Zhdanov, Aleksei and Nikiforova, Anastasia and Stepichev, Andrey <br>          and Kuznetsova, Anna and Ronkin, Mikhail and Borisov, Vasilii and Bogachev, Alexander <br>          and Korotkich, Sergey and Constable, Paul A and Maier, Andreas},\n  journal={Scientific Data},\n  volume={11},\n  number={1},\n  pages={365},\n  year={2024},\n  publisher={Nature Publishing Group UK London},<br>  doi={https://doi.org/10.1038/s41597-024-03182-7}\n}\n</pre>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Optical coherence tomography (OCT) is a non-invasive imaging technique that has extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology.</p>\n<p>The dataset consists of the following categories and images:<br>- Age-Related Macular Degeneration - 1231 images;<br>- Diabetic Macular Edema - 147 images;<br>- Epiretinal Membrane- 155 images;<br>- Normal - 332 images;<br>- Retinal Artery Occlusion - 22 images;<br>- Retinal Vein Occlusion - 101 images;<br>- Vitreomacular Interface Disease - 76 images.</p>\n<p>This dataset is published to provide researchers and developers with access to a large set of labeled images, which contributes to the development and improvement of algorithms for the automatic processing and analysis of OCT images for early diagnosis and monitoring of eye diseases. CSV file consists of file_name, disease, subcategory, condition, patient_id, eye, sex, year, image_width, and image_height. The dataset will be updated periodically.</p>\n<p>&nbsp;</p>\n<p>For more information and details about the dataset see:</p>\n<p>https://rdcu.be/dELrE</p>\n<p>https://arxiv.org/abs/2312.08255</p>\n<pre>@article{kulyabin2024octdl,\n  title={OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods},\n  author={Kulyabin, Mikhail and Zhdanov, Aleksei and Nikiforova, Anastasia and Stepichev, Andrey <br>          and Kuznetsova, Anna and Ronkin, Mikhail and Borisov, Vasilii and Bogachev, Alexander <br>          and Korotkich, Sergey and Constable, Paul A and Maier, Andreas},\n  journal={Scientific Data},\n  volume={11},\n  number={1},\n  pages={365},\n  year={2024},\n  publisher={Nature Publishing Group UK London},<br>  doi={https://doi.org/10.1038/s41597-024-03182-7}\n}\n</pre>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.10839556",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kulyabin, Mikhail",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg"
              }
            ]
          },
          {
            "creatorName": "Zhdanov, Aleksei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural Federal University"
              }
            ]
          },
          {
            "creatorName": "Nikiforova, Anastasia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural State Medical University"
              }
            ]
          },
          {
            "creatorName": "Stepichev, Andrey",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural State Medical University"
              }
            ]
          },
          {
            "creatorName": "Kuznetsova, Anna",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural State Medical University"
              }
            ]
          },
          {
            "creatorName": "Borisov, Vasilii",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural Federal University"
              }
            ]
          },
          {
            "creatorName": "Ronkin, Mikhail",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural Federal University"
              }
            ]
          },
          {
            "creatorName": "Bogachev, Alexander",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural State Medical University"
              }
            ]
          },
          {
            "creatorName": "Korotkich, Sergey",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ural State Medical University"
              }
            ]
          },
          {
            "creatorName": "Maier, Andreas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-03-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Optical coherence tomography (OCT) is a non-invasive imaging technique that has extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology.</p>\n<p>The dataset consists of the following categories and images:<br>- Age-Related Macular Degeneration - 1231 images;<br>- Diabetic Macular Edema - 147 images;<br>- Epiretinal Membrane- 155 images;<br>- Normal - 332 images;<br>- Retinal Artery Occlusion - 22 images;<br>- Retinal Vein Occlusion - 101 images;<br>- Vitreomacular Interface Disease - 76 images.</p>\n<p>This dataset is published to provide researchers and developers with access to a large set of labeled images, which contributes to the development and improvement of algorithms for the automatic processing and analysis of OCT images for early diagnosis and monitoring of eye diseases. CSV file consists of file_name, disease, subcategory, condition, patient_id, eye, sex, year, image_width, and image_height. The dataset will be updated periodically.</p>\n<p>&nbsp;</p>\n<p>For more information and details about the dataset see:</p>\n<p>https://rdcu.be/dELrE</p>\n<p>https://arxiv.org/abs/2312.08255</p>\n<pre>@article{kulyabin2024octdl,\n  title={OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods},\n  author={Kulyabin, Mikhail and Zhdanov, Aleksei and Nikiforova, Anastasia and Stepichev, Andrey <br>          and Kuznetsova, Anna and Ronkin, Mikhail and Borisov, Vasilii and Bogachev, Alexander <br>          and Korotkich, Sergey and Constable, Paul A and Maier, Andreas},\n  journal={Scientific Data},\n  volume={11},\n  number={1},\n  pages={365},\n  year={2024},\n  publisher={Nature Publishing Group UK London},<br>  doi={https://doi.org/10.1038/s41597-024-03182-7}\n}\n</pre>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "OCT"
          },
          {
            "subjectValue": "Deep Learning"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "380.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 398668595,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/10839556",
    "created": "1710861972"
  },
  {
    "id": 90,
    "canonicalId": "qneuwift6ba287o7c6y2zpv7",
    "datasetId": "qneuwift6ba287o7c6y2zpv7",
    "doi": "10.5281/zenodo.6332896",
    "title": "Efficient embryoid-based method to improve generation of optic vesicles from human induced pluripotent stem cells data",
    "description": "<p>Animal models have provided many insights into ocular development and disease, but they remain suboptimal for understanding human oculogenesis. Eye development requires spatiotemporal gene expression patterns and disease phenotypes can differ significantly between humans and animal models, with patient-associated mutations causing embryonic lethality reported in some animal models. The emergence of human induced pluripotent stem cell (hiPSC) technology has provided a new resource for dissecting the complex nature of early eye morphogenesis through the generation of three-dimensional (3D) cellular models. By using patient-specific hiPSCs to generate <em>in vitro </em>optic vesicle-like models, we can enhance the understanding of early developmental eye disorders and provide a pre-clinical platform for disease modelling and therapeutics testing. A major challenge of <em>in vitro </em>optic vesicle generation is the low efficiency of differentiation in 3D cultures. To address this, we adapted a previously published protocol of retinal organoid differentiation to improve embryoid body formation using a microwell plate. Established morphology, upregulated transcript levels of known early eye-field transcription factors and protein expression of standard retinal progenitor markers confirmed the optic vesicle/presumptive optic cup identity of <em>in vitro </em>models between day 20 and 50 of culture. This adapted protocol is relevant to researchers seeking a physiologically relevant model of early human ocular development and disease with a view to replacing animal models.</p>",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Animal models have provided many insights into ocular development and disease, but they remain suboptimal for understanding human oculogenesis. Eye development requires spatiotemporal gene expression patterns and disease phenotypes can differ significantly between humans and animal models, with patient-associated mutations causing embryonic lethality reported in some animal models. The emergence of human induced pluripotent stem cell (hiPSC) technology has provided a new resource for dissecting the complex nature of early eye morphogenesis through the generation of three-dimensional (3D) cellular models. By using patient-specific hiPSCs to generate <em>in vitro </em>optic vesicle-like models, we can enhance the understanding of early developmental eye disorders and provide a pre-clinical platform for disease modelling and therapeutics testing. A major challenge of <em>in vitro </em>optic vesicle generation is the low efficiency of differentiation in 3D cultures. To address this, we adapted a previously published protocol of retinal organoid differentiation to improve embryoid body formation using a microwell plate. Established morphology, upregulated transcript levels of known early eye-field transcription factors and protein expression of standard retinal progenitor markers confirmed the optic vesicle/presumptive optic cup identity of <em>in vitro </em>models between day 20 and 50 of culture. This adapted protocol is relevant to researchers seeking a physiologically relevant model of early human ocular development and disease with a view to replacing animal models.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6332896",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Efficient embryoid-based method to improve generation of optic vesicles from human induced pluripotent stem cells data"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Eintracht, Jonathan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UCL Institute of Ophthalmology"
              }
            ]
          },
          {
            "creatorName": "Harding, Pippa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UCL Institute of Ophthalmology"
              }
            ]
          },
          {
            "creatorName": "Lima Cunha, Dulce",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UCL Institute of Ophthalmology"
              }
            ]
          },
          {
            "creatorName": "Moosajee, Mariya",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "UCL Institute of Ophthalmology"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-03-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Animal models have provided many insights into ocular development and disease, but they remain suboptimal for understanding human oculogenesis. Eye development requires spatiotemporal gene expression patterns and disease phenotypes can differ significantly between humans and animal models, with patient-associated mutations causing embryonic lethality reported in some animal models. The emergence of human induced pluripotent stem cell (hiPSC) technology has provided a new resource for dissecting the complex nature of early eye morphogenesis through the generation of three-dimensional (3D) cellular models. By using patient-specific hiPSCs to generate <em>in vitro </em>optic vesicle-like models, we can enhance the understanding of early developmental eye disorders and provide a pre-clinical platform for disease modelling and therapeutics testing. A major challenge of <em>in vitro </em>optic vesicle generation is the low efficiency of differentiation in 3D cultures. To address this, we adapted a previously published protocol of retinal organoid differentiation to improve embryoid body formation using a microwell plate. Established morphology, upregulated transcript levels of known early eye-field transcription factors and protein expression of standard retinal progenitor markers confirmed the optic vesicle/presumptive optic cup identity of <em>in vitro </em>models between day 20 and 50 of culture. This adapted protocol is relevant to researchers seeking a physiologically relevant model of early human ocular development and disease with a view to replacing animal models.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Embryoid bodies, optic vesicles, iPSCs, retinal differentiation, VSX2, PAX6"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "7.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 7444889,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6332896",
    "created": "1646604507"
  },
  {
    "id": 91,
    "canonicalId": "sd9tojq82ifu0fmqxfolev02",
    "datasetId": "sd9tojq82ifu0fmqxfolev02",
    "doi": "10.5281/zenodo.4891308",
    "title": "Dataset from fundus images for the study of diabetic retinopathy",
    "description": "<p>This database containing 1437 color fundus images that were acquired at the Department of Ophthalmology of the Hospital de Cl&iacute;nicas, Facultad de Ciencias M&eacute;dicas, Universidad Nacional de Asunci&oacute;n, Paraguay.</p>\n\n<p>The acquisition of retinal images was done taking into account a clinical procedure.&nbsp;The acquisition of the retinographies was made through the Visucam 500 camera of the Zeiss brand. Expert ophthalmologists have classified the dataset. These data can help doctors and researchers in the detection of cases of Non-Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopathy (PDR), in their different stages.&nbsp;</p>\n\n<p>The classification of fundus images have been done in 7 categories:&nbsp;1. No DR signs (711 images),&nbsp;2.&nbsp;Mild (or early) NPDR (6 images),&nbsp;3.&nbsp;Moderate NPDR (110 images),&nbsp;4.&nbsp;Severe NPDR (210 images),&nbsp;5.&nbsp;Very Severe NPDR (139 images),&nbsp;6. PDR (116 images)&nbsp;and&nbsp;7. Advanced PDR (145 images).</p>\n\n<p><em>If you use the dataset, please cite the paper:</em></p>\n\n<p>V. E. Castillo Ben&iacute;tez, I. Castro Matto, J. C. Mello Rom&aacute;n, J. L. V&aacute;zquez Noguera, M. Garc&iacute;a-Torres, J. Ayala, D. P. Pinto-Roa, P. E. Gardel-Sotomayor, J. Facon, and S. A. Grillo, <strong>Dataset from fundus images for the study of diabetic retinopathy</strong>,&nbsp;Data in Brief, vol. 36, p. 107068, Jun. 2021. doi:&nbsp;https://doi.org/10.1016/j.dib.2021.107068</p>",
    "versionTitle": "0.3",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This database containing 1437 color fundus images that were acquired at the Department of Ophthalmology of the Hospital de Cl&iacute;nicas, Facultad de Ciencias M&eacute;dicas, Universidad Nacional de Asunci&oacute;n, Paraguay.</p>\n\n<p>The acquisition of retinal images was done taking into account a clinical procedure.&nbsp;The acquisition of the retinographies was made through the Visucam 500 camera of the Zeiss brand. Expert ophthalmologists have classified the dataset. These data can help doctors and researchers in the detection of cases of Non-Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopathy (PDR), in their different stages.&nbsp;</p>\n\n<p>The classification of fundus images have been done in 7 categories:&nbsp;1. No DR signs (711 images),&nbsp;2.&nbsp;Mild (or early) NPDR (6 images),&nbsp;3.&nbsp;Moderate NPDR (110 images),&nbsp;4.&nbsp;Severe NPDR (210 images),&nbsp;5.&nbsp;Very Severe NPDR (139 images),&nbsp;6. PDR (116 images)&nbsp;and&nbsp;7. Advanced PDR (145 images).</p>\n\n<p><em>If you use the dataset, please cite the paper:</em></p>\n\n<p>V. E. Castillo Ben&iacute;tez, I. Castro Matto, J. C. Mello Rom&aacute;n, J. L. V&aacute;zquez Noguera, M. Garc&iacute;a-Torres, J. Ayala, D. P. Pinto-Roa, P. E. Gardel-Sotomayor, J. Facon, and S. A. Grillo, <strong>Dataset from fundus images for the study of diabetic retinopathy</strong>,&nbsp;Data in Brief, vol. 36, p. 107068, Jun. 2021. doi:&nbsp;https://doi.org/10.1016/j.dib.2021.107068</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.4891308",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset from fundus images for the study of diabetic retinopathy"
          }
        ],
        "version": "0.3",
        "creator": [
          {
            "creatorName": "Veronica Elisa Castillo Ben\u00edtez",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Hospital de Cl\u00ednicas, Facultad de Ciencias M\u00e9dicas, Universidad Nacional de Asunci\u00f3n, San Lorenzo 2160, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Ingrid Castro Matto",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, Hospital de Cl\u00ednicas, Facultad de Ciencias M\u00e9dicas, Universidad Nacional de Asunci\u00f3n, San Lorenzo 2160, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Julio C\u00e9sar Mello Rom\u00e1n",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Facultad de Ciencias Exactas y Tecnol\u00f3gicas, Universidad Nacional de Concepci\u00f3n, Concepci\u00f3n 8700, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Jos\u00e9 Luis V\u00e1zquez Noguera",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Computer Engineer Department, Universidad Americana, Asunci\u00f3n 1029, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Miguel Garc\u00eda-Torres",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Division of Computer Science, Universidad Pablo de Olavide, ES-41013 Seville, Spain"
              }
            ]
          },
          {
            "creatorName": "Jordan Ayala",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Computer Engineer Department, Universidad Americana, Asunci\u00f3n 1029, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Diego P. Pinto-Roa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Facultad Polit\u00e9cnica, Universidad Nacional de Asunci\u00f3n, San Lorenzo 2160, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Pedro E. Gardel-Sotomayor",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Computer Engineer Department, Universidad Americana, Asunci\u00f3n 1029, Paraguay"
              }
            ]
          },
          {
            "creatorName": "Jacques Facon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Computer and Electronics, Universidade Federal do Esp\u00edrito Santo, S\u00e3o Mateus, Brazil"
              }
            ]
          },
          {
            "creatorName": "Sebastian Alberto Grillo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universidad Aut\u00f3noma de Asunci\u00f3n, Asunci\u00f3n, Paraguay"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-02-10",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This database containing 1437 color fundus images that were acquired at the Department of Ophthalmology of the Hospital de Cl&iacute;nicas, Facultad de Ciencias M&eacute;dicas, Universidad Nacional de Asunci&oacute;n, Paraguay.</p>\n\n<p>The acquisition of retinal images was done taking into account a clinical procedure.&nbsp;The acquisition of the retinographies was made through the Visucam 500 camera of the Zeiss brand. Expert ophthalmologists have classified the dataset. These data can help doctors and researchers in the detection of cases of Non-Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopathy (PDR), in their different stages.&nbsp;</p>\n\n<p>The classification of fundus images have been done in 7 categories:&nbsp;1. No DR signs (711 images),&nbsp;2.&nbsp;Mild (or early) NPDR (6 images),&nbsp;3.&nbsp;Moderate NPDR (110 images),&nbsp;4.&nbsp;Severe NPDR (210 images),&nbsp;5.&nbsp;Very Severe NPDR (139 images),&nbsp;6. PDR (116 images)&nbsp;and&nbsp;7. Advanced PDR (145 images).</p>\n\n<p><em>If you use the dataset, please cite the paper:</em></p>\n\n<p>V. E. Castillo Ben&iacute;tez, I. Castro Matto, J. C. Mello Rom&aacute;n, J. L. V&aacute;zquez Noguera, M. Garc&iacute;a-Torres, J. Ayala, D. P. Pinto-Roa, P. E. Gardel-Sotomayor, J. Facon, and S. A. Grillo, <strong>Dataset from fundus images for the study of diabetic retinopathy</strong>,&nbsp;Data in Brief, vol. 36, p. 107068, Jun. 2021. doi:&nbsp;https://doi.org/10.1016/j.dib.2021.107068</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Fundus Images"
          },
          {
            "subjectValue": "Non-Proliferative Diabetic Retinopathy"
          },
          {
            "subjectValue": "Proliferative Diabetic Retinopathy"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1453.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1523580928,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4891308",
    "created": "1622575371"
  },
  {
    "id": 92,
    "canonicalId": "p9opsrnkxt9b30lx8f0j8779",
    "datasetId": "p9opsrnkxt9b30lx8f0j8779",
    "doi": "10.5061/dryad.4ch10",
    "title": "Data from: Using matrix and tensor factorizations for the single-trial analysis of population spike trains",
    "description": "Advances in neuronal recording techniques are leading to ever larger numbers of simultaneously monitored neurons. This poses the important analytical challenge of how to capture compactly all sensory information that neural population codes carry in their spatial dimension (differences in stimulus tuning across neurons at different locations), in their temporal dimension (temporal neural response variations), or in their combination (temporally coordinated neural population firing). Here we investigate the utility of tensor factorizations of population spike trains along space and time. These factorizations decompose a dataset of single-trial population spike trains into spatial firing patterns (combinations of neurons firing together), temporal firing patterns (temporal activation of these groups of neurons) and trial-dependent activation coefficients (strength of recruitment of such neural patterns on each trial). We validated various factorization methods on simulated data and on populations of ganglion cells simultaneously recorded in the salamander retina. We found that single-trial tensor space-by-time decompositions provided low-dimensional data-robust representations of spike trains that capture efficiently both their spatial and temporal information about sensory stimuli. Tensor decompositions with orthogonality constraints were the most efficient in extracting sensory information, whereas non-negative tensor decompositions worked well even on non-independent and overlapping spike patterns, and retrieved informative firing patterns expressed by the same population in response to novel stimuli. Our method showed that populations of retinal ganglion cells carried information in their spike timing on the ten-milliseconds-scale about spatial details of natural images. This information could not be recovered from the spike counts of these cells. First-spike latencies carried the majority of information provided by the whole spike train about fine-scale image features, and supplied almost as much information about coarse natural image features as firing rates. Together, these results highlight the importance of spike timing, and particularly of first-spike latencies, in retinal coding.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Advances in neuronal recording techniques are leading to ever larger numbers of simultaneously monitored neurons. This poses the important analytical challenge of how to capture compactly all sensory information that neural population codes carry in their spatial dimension (differences in stimulus tuning across neurons at different locations), in their temporal dimension (temporal neural response variations), or in their combination (temporally coordinated neural population firing). Here we investigate the utility of tensor factorizations of population spike trains along space and time. These factorizations decompose a dataset of single-trial population spike trains into spatial firing patterns (combinations of neurons firing together), temporal firing patterns (temporal activation of these groups of neurons) and trial-dependent activation coefficients (strength of recruitment of such neural patterns on each trial). We validated various factorization methods on simulated data and on populations of ganglion cells simultaneously recorded in the salamander retina. We found that single-trial tensor space-by-time decompositions provided low-dimensional data-robust representations of spike trains that capture efficiently both their spatial and temporal information about sensory stimuli. Tensor decompositions with orthogonality constraints were the most efficient in extracting sensory information, whereas non-negative tensor decompositions worked well even on non-independent and overlapping spike patterns, and retrieved informative firing patterns expressed by the same population in response to novel stimuli. Our method showed that populations of retinal ganglion cells carried information in their spike timing on the ten-milliseconds-scale about spatial details of natural images. This information could not be recovered from the spike counts of these cells. First-spike latencies carried the majority of information provided by the whole spike train about fine-scale image features, and supplied almost as much information about coarse natural image features as firing rates. Together, these results highlight the importance of spike timing, and particularly of first-spike latencies, in retinal coding.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.4ch10",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Using matrix and tensor factorizations for the single-trial analysis of population spike trains"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Onken, Arno",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Italian Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Liu, Jian K.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universit\u00e4tsmedizin G\u00f6ttingen"
              }
            ]
          },
          {
            "creatorName": "Karunasekara, P. P. Chamanthi R.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Trento"
              }
            ]
          },
          {
            "creatorName": "Delis, Ioannis",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Columbia University"
              }
            ]
          },
          {
            "creatorName": "Gollisch, Tim",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Universit\u00e4tsmedizin G\u00f6ttingen"
              }
            ]
          },
          {
            "creatorName": "Panzeri, Stefano",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Italian Institute of Technology"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-10-13",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Advances in neuronal recording techniques are leading to ever larger numbers of simultaneously monitored neurons. This poses the important analytical challenge of how to capture compactly all sensory information that neural population codes carry in their spatial dimension (differences in stimulus tuning across neurons at different locations), in their temporal dimension (temporal neural response variations), or in their combination (temporally coordinated neural population firing). Here we investigate the utility of tensor factorizations of population spike trains along space and time. These factorizations decompose a dataset of single-trial population spike trains into spatial firing patterns (combinations of neurons firing together), temporal firing patterns (temporal activation of these groups of neurons) and trial-dependent activation coefficients (strength of recruitment of such neural patterns on each trial). We validated various factorization methods on simulated data and on populations of ganglion cells simultaneously recorded in the salamander retina. We found that single-trial tensor space-by-time decompositions provided low-dimensional data-robust representations of spike trains that capture efficiently both their spatial and temporal information about sensory stimuli. Tensor decompositions with orthogonality constraints were the most efficient in extracting sensory information, whereas non-negative tensor decompositions worked well even on non-independent and overlapping spike patterns, and retrieved informative firing patterns expressed by the same population in response to novel stimuli. Our method showed that populations of retinal ganglion cells carried information in their spike timing on the ten-milliseconds-scale about spatial details of natural images. This information could not be recovered from the spike counts of these cells. First-spike latencies carried the majority of information provided by the whole spike train about fine-scale image features, and supplied almost as much information about coarse natural image features as firing rates. Together, these results highlight the importance of spike timing, and particularly of first-spike latencies, in retinal coding.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "factor analysis"
          },
          {
            "subjectValue": "independent component analysis"
          },
          {
            "subjectValue": "retinal ganglion cells"
          },
          {
            "subjectValue": "Ambystoma mexicanum"
          },
          {
            "subjectValue": "tensor factorization"
          },
          {
            "subjectValue": "natural stimuli"
          },
          {
            "subjectValue": "non-negative matrix factorization"
          },
          {
            "subjectValue": "principal component analysis"
          },
          {
            "subjectValue": "population coding"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "260.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 273049190,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5009403",
    "created": "1624324732"
  },
  {
    "id": 93,
    "canonicalId": "o7q4jojsx1pzc0xgceqarogd",
    "datasetId": "o7q4jojsx1pzc0xgceqarogd",
    "doi": "10.5281/zenodo.7779499",
    "title": "rwave-4096 - Retinal Wave Dataset",
    "description": "<p>4096 classes of retinal waves, 2000 images per class.</p>\n\n<ul>\n\t<li>4096_split.tar.gz: Images split into train/test/val sets (80%/10%/10%).</li>\n\t<li>4096_info.zip: Retinal Wave Simulator Parameters per Class.</li>\n</ul>\n\n<p>Generated using Retinal Wave Simulator&nbsp;https://github.com/BennyCa/Retinal-Wave-Simulator adapted from&nbsp;https://swindale.ecc.ubc.ca/home-page/software/retinal-wave-models/</p>\n\n<p>Used in project&nbsp;Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development: https://github.com/BennyCa/ReWaRD</p>",
    "versionTitle": "1.0.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>4096 classes of retinal waves, 2000 images per class.</p>\n\n<ul>\n\t<li>4096_split.tar.gz: Images split into train/test/val sets (80%/10%/10%).</li>\n\t<li>4096_info.zip: Retinal Wave Simulator Parameters per Class.</li>\n</ul>\n\n<p>Generated using Retinal Wave Simulator&nbsp;https://github.com/BennyCa/Retinal-Wave-Simulator adapted from&nbsp;https://swindale.ecc.ubc.ca/home-page/software/retinal-wave-models/</p>\n\n<p>Used in project&nbsp;Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development: https://github.com/BennyCa/ReWaRD</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7779499",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "rwave-4096 - Retinal Wave Dataset"
          }
        ],
        "version": "1.0.0",
        "creator": [
          {
            "creatorName": "Cappell, Benjamin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-04-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>4096 classes of retinal waves, 2000 images per class.</p>\n\n<ul>\n\t<li>4096_split.tar.gz: Images split into train/test/val sets (80%/10%/10%).</li>\n\t<li>4096_info.zip: Retinal Wave Simulator Parameters per Class.</li>\n</ul>\n\n<p>Generated using Retinal Wave Simulator&nbsp;https://github.com/BennyCa/Retinal-Wave-Simulator adapted from&nbsp;https://swindale.ecc.ubc.ca/home-page/software/retinal-wave-models/</p>\n\n<p>Used in project&nbsp;Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development: https://github.com/BennyCa/ReWaRD</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retinal wave"
          },
          {
            "subjectValue": "pre-training"
          },
          {
            "subjectValue": "non-natural images"
          },
          {
            "subjectValue": "image dataset"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "11515.7 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 12075086643,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7779499",
    "created": "1681059006"
  },
  {
    "id": 94,
    "canonicalId": "x7rkb2l8o9bu2wzg5658f2l0",
    "datasetId": "x7rkb2l8o9bu2wzg5658f2l0",
    "doi": "10.5281/zenodo.7769863",
    "title": "Photodynamic Ocular Drug Delivery System with Optical Coherence Tomography Oriented Microscale Robots",
    "description": "<p>Retinal diseases are the most common reason for irreversible blindness worldwide. Common retinal diseases, such as diabetic retinopathy, macular degeneration, retinal vein occlusion, and non-infectious uveitis, mostly require intravitreal steroid injections in clinical management. Unfortunately, intravitreal steroid injections lead to other important ocular problems, such as elevation of intraocular pressure, intravitreal infections, and the progression of cataracts. Because of these reasons, targeted drug delivery systems are necessary for the medical treatment of retinal diseases. PHOTodynamic Ocular Drug delivery system with Optical Coherence Tomography Oriented microscale Robots (PHOTODOCTOR) project combines three novel technologies to solve this important clinical problem. The first technological advancement for the project is dexamethasone-saturated hyaluronic acid (HA) hydrogels with organo-ruthenium complexes, which enables controllable degradation under visible light exposure. With the help of visible light-controlled degradation, dexamethasone will be released with the light pulses. Then the hydrogels will be supplemented with superparamagnetic iron oxide nanoparticles (SPIONs), and 3D printed as helical micro-swimmers. Insertion of the SPIONs will help the magnetic actuation of the hydrogel-based microrobots in the intraocular space. Lastly, SPION-supplemented HA hydrogels will be observed with high-resolution optical coherence tomography (OCT) system in the intraocular space to guide them in diseased areas of the retina. While HA-based structure increases penetration in the vitreous and helps the attachment on the retina, SPIONs will increase the visibility of the microrobots in OCT imaging. In this way, a magnetic-driven, visible light-triggered intraocular drug release system with OCT guidance will be produced, and it could be used not only for intravitreal steroid delivery but also for the treatment of retinal tumours and other retinal problems.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Retinal diseases are the most common reason for irreversible blindness worldwide. Common retinal diseases, such as diabetic retinopathy, macular degeneration, retinal vein occlusion, and non-infectious uveitis, mostly require intravitreal steroid injections in clinical management. Unfortunately, intravitreal steroid injections lead to other important ocular problems, such as elevation of intraocular pressure, intravitreal infections, and the progression of cataracts. Because of these reasons, targeted drug delivery systems are necessary for the medical treatment of retinal diseases. PHOTodynamic Ocular Drug delivery system with Optical Coherence Tomography Oriented microscale Robots (PHOTODOCTOR) project combines three novel technologies to solve this important clinical problem. The first technological advancement for the project is dexamethasone-saturated hyaluronic acid (HA) hydrogels with organo-ruthenium complexes, which enables controllable degradation under visible light exposure. With the help of visible light-controlled degradation, dexamethasone will be released with the light pulses. Then the hydrogels will be supplemented with superparamagnetic iron oxide nanoparticles (SPIONs), and 3D printed as helical micro-swimmers. Insertion of the SPIONs will help the magnetic actuation of the hydrogel-based microrobots in the intraocular space. Lastly, SPION-supplemented HA hydrogels will be observed with high-resolution optical coherence tomography (OCT) system in the intraocular space to guide them in diseased areas of the retina. While HA-based structure increases penetration in the vitreous and helps the attachment on the retina, SPIONs will increase the visibility of the microrobots in OCT imaging. In this way, a magnetic-driven, visible light-triggered intraocular drug release system with OCT guidance will be produced, and it could be used not only for intravitreal steroid delivery but also for the treatment of retinal tumours and other retinal problems.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7769863",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Photodynamic Ocular Drug Delivery System with Optical Coherence Tomography Oriented Microscale Robots"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Yildiz, Erdost",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Max Planck Institute for Intelligent Systems"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-10-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Retinal diseases are the most common reason for irreversible blindness worldwide. Common retinal diseases, such as diabetic retinopathy, macular degeneration, retinal vein occlusion, and non-infectious uveitis, mostly require intravitreal steroid injections in clinical management. Unfortunately, intravitreal steroid injections lead to other important ocular problems, such as elevation of intraocular pressure, intravitreal infections, and the progression of cataracts. Because of these reasons, targeted drug delivery systems are necessary for the medical treatment of retinal diseases. PHOTodynamic Ocular Drug delivery system with Optical Coherence Tomography Oriented microscale Robots (PHOTODOCTOR) project combines three novel technologies to solve this important clinical problem. The first technological advancement for the project is dexamethasone-saturated hyaluronic acid (HA) hydrogels with organo-ruthenium complexes, which enables controllable degradation under visible light exposure. With the help of visible light-controlled degradation, dexamethasone will be released with the light pulses. Then the hydrogels will be supplemented with superparamagnetic iron oxide nanoparticles (SPIONs), and 3D printed as helical micro-swimmers. Insertion of the SPIONs will help the magnetic actuation of the hydrogel-based microrobots in the intraocular space. Lastly, SPION-supplemented HA hydrogels will be observed with high-resolution optical coherence tomography (OCT) system in the intraocular space to guide them in diseased areas of the retina. While HA-based structure increases penetration in the vitreous and helps the attachment on the retina, SPIONs will increase the visibility of the microrobots in OCT imaging. In this way, a magnetic-driven, visible light-triggered intraocular drug release system with OCT guidance will be produced, and it could be used not only for intravitreal steroid delivery but also for the treatment of retinal tumours and other retinal problems.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1702.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1785200640,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7769863",
    "created": "1728901617"
  },
  {
    "id": 95,
    "canonicalId": "vy9ds1tv36bzzy96wh7b0lts",
    "datasetId": "vy9ds1tv36bzzy96wh7b0lts",
    "doi": "10.5061/dryad.245j1p8",
    "title": "Data from: Predicted tracking error triggers catch-up saccades during smooth pursuit",
    "description": "For foveated animals, visual tracking of moving stimuli requires the synergy between saccades and smooth pursuit eye movements. Deciding to trigger a catch-up saccade during pursuit influences the quality of visual input. This decision is a trade-off between tolerating sustained position error when no saccade is triggered or a transient loss of vision during the saccade due to saccadic suppression. Although catch-up saccades have been extensively investigated, it remains unclear how the trigger decision is made by the brain. de Brouwer et al (2002) demonstrated that catch-up saccades were less likely to occur when the expected time to foveate a target using pursuit alone is between 40 and 180ms into the future, referred to as the smooth zone. However, this descriptive result lacks a mechanistic explanation for how the trigger decision is made. More recently, we proposed a decision model (Coutinho et al., 2018) that relies on a probabilistic estimation of predicted position error (PEpred) during visual tracking. To test the model predictions, we investigated how human participants combined predicted position error, retinal slip, and the uncertainty in those estimates to make trigger decisions. We found a significant effect of the pre-saccadic magnitude of PEpred on trigger time and occurrence of catch-up saccades. To test the role of uncertainty, we blurred the moving target which led to longer and more variable saccade trigger times and more smooth pursuit trials, consistent with model predictions. As predicted by our model, large PEpred (&gt;10deg) produced early saccades regardless of the level of uncertainty while saccades preceded by small PEpred (&lt;10deg) were significantly modulated by high uncertainty. Our model also predicted increased signal dependent noise as retinal slip increases, which resulted in longer saccade trigger times and more smooth trials. In conclusion, the data supports our hypothesized role of PEpred in deciding when to trigger a catch-up saccade during smooth pursuit while taking into account uncertainty in sensory estimates.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "For foveated animals, visual tracking of moving stimuli requires the synergy between saccades and smooth pursuit eye movements. Deciding to trigger a catch-up saccade during pursuit influences the quality of visual input. This decision is a trade-off between tolerating sustained position error when no saccade is triggered or a transient loss of vision during the saccade due to saccadic suppression. Although catch-up saccades have been extensively investigated, it remains unclear how the trigger decision is made by the brain. de Brouwer et al (2002) demonstrated that catch-up saccades were less likely to occur when the expected time to foveate a target using pursuit alone is between 40 and 180ms into the future, referred to as the smooth zone. However, this descriptive result lacks a mechanistic explanation for how the trigger decision is made. More recently, we proposed a decision model (Coutinho et al., 2018) that relies on a probabilistic estimation of predicted position error (PEpred) during visual tracking. To test the model predictions, we investigated how human participants combined predicted position error, retinal slip, and the uncertainty in those estimates to make trigger decisions. We found a significant effect of the pre-saccadic magnitude of PEpred on trigger time and occurrence of catch-up saccades. To test the role of uncertainty, we blurred the moving target which led to longer and more variable saccade trigger times and more smooth pursuit trials, consistent with model predictions. As predicted by our model, large PEpred (&gt;10deg) produced early saccades regardless of the level of uncertainty while saccades preceded by small PEpred (&lt;10deg) were significantly modulated by high uncertainty. Our model also predicted increased signal dependent noise as retinal slip increases, which resulted in longer saccade trigger times and more smooth trials. In conclusion, the data supports our hypothesized role of PEpred in deciding when to trigger a catch-up saccade during smooth pursuit while taking into account uncertainty in sensory estimates.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.245j1p8",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Predicted tracking error triggers catch-up saccades during smooth pursuit"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Nachmani, Omri",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Coutinho, Jonathan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Khan, Aarlenne Z.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Lef\u00e8vre, Philippe",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Blohm, Gunnar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-06-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "For foveated animals, visual tracking of moving stimuli requires the synergy between saccades and smooth pursuit eye movements. Deciding to trigger a catch-up saccade during pursuit influences the quality of visual input. This decision is a trade-off between tolerating sustained position error when no saccade is triggered or a transient loss of vision during the saccade due to saccadic suppression. Although catch-up saccades have been extensively investigated, it remains unclear how the trigger decision is made by the brain. de Brouwer et al (2002) demonstrated that catch-up saccades were less likely to occur when the expected time to foveate a target using pursuit alone is between 40 and 180ms into the future, referred to as the smooth zone. However, this descriptive result lacks a mechanistic explanation for how the trigger decision is made. More recently, we proposed a decision model (Coutinho et al., 2018) that relies on a probabilistic estimation of predicted position error (PEpred) during visual tracking. To test the model predictions, we investigated how human participants combined predicted position error, retinal slip, and the uncertainty in those estimates to make trigger decisions. We found a significant effect of the pre-saccadic magnitude of PEpred on trigger time and occurrence of catch-up saccades. To test the role of uncertainty, we blurred the moving target which led to longer and more variable saccade trigger times and more smooth pursuit trials, consistent with model predictions. As predicted by our model, large PEpred (&gt;10deg) produced early saccades regardless of the level of uncertainty while saccades preceded by small PEpred (&lt;10deg) were significantly modulated by high uncertainty. Our model also predicted increased signal dependent noise as retinal slip increases, which resulted in longer saccade trigger times and more smooth trials. In conclusion, the data supports our hypothesized role of PEpred in deciding when to trigger a catch-up saccade during smooth pursuit while taking into account uncertainty in sensory estimates.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Catch up"
          },
          {
            "subjectValue": "saccades"
          },
          {
            "subjectValue": "Step ramp"
          },
          {
            "subjectValue": "Smooth Pursuit"
          },
          {
            "subjectValue": "eye movement"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "23548.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 24692811366,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3902832",
    "created": "1596146178"
  },
  {
    "id": 96,
    "canonicalId": "eiogpyrgjbdguqsfj75uuzd8",
    "datasetId": "eiogpyrgjbdguqsfj75uuzd8",
    "doi": "10.5281/zenodo.14847200",
    "title": "NACHOS dataset: OCT and Xray",
    "description": "<p>The following datasets are part of a future paper.</p>\n<h1>Kidney OCT dataset</h1>\n<p>Three type of tissues were sampled: cortex, medulla, and pelvis. Image size: 185*210 pixels. The OCT dataset comes from 10 porcine kidneys. For each kidney and tissue, there are 30 volumes. Each volume contain 20 images. Total number of images: 10*3*30*20 = 18,000. The same dataset was partitioned in three different levels: image (folder: split_random), volumen (folder: split_volume), and subject (folder: split_subject).</p>\n<h1>Chest X-ray repository</h1>\n<p>A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. The chest X-ray repository was partitioned into four folds using three different partitioning levels. In image-level partitioning(folder: split1_random), images were randomly distributed across four folds. In patient-level partitioning (folder: split2_patient), all images from the same patient were assigned to the same fold. Finally, in dataset-level partitioning (folder: split3_dataset), each dataset was exclusively allocated to a separate fold.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The following datasets are part of a future paper.</p>\n<h1>Kidney OCT dataset</h1>\n<p>Three type of tissues were sampled: cortex, medulla, and pelvis. Image size: 185*210 pixels. The OCT dataset comes from 10 porcine kidneys. For each kidney and tissue, there are 30 volumes. Each volume contain 20 images. Total number of images: 10*3*30*20 = 18,000. The same dataset was partitioned in three different levels: image (folder: split_random), volumen (folder: split_volume), and subject (folder: split_subject).</p>\n<h1>Chest X-ray repository</h1>\n<p>A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. The chest X-ray repository was partitioned into four folds using three different partitioning levels. In image-level partitioning(folder: split1_random), images were randomly distributed across four folds. In patient-level partitioning (folder: split2_patient), all images from the same patient were assigned to the same fold. Finally, in dataset-level partitioning (folder: split3_dataset), each dataset was exclusively allocated to a separate fold.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.14847200",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "NACHOS dataset: OCT and Xray"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Calle, Paul",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Wang, Chen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Tang, Qinggong",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Pan, Chongle",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-02-10",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The following datasets are part of a future paper.</p>\n<h1>Kidney OCT dataset</h1>\n<p>Three type of tissues were sampled: cortex, medulla, and pelvis. Image size: 185*210 pixels. The OCT dataset comes from 10 porcine kidneys. For each kidney and tissue, there are 30 volumes. Each volume contain 20 images. Total number of images: 10*3*30*20 = 18,000. The same dataset was partitioned in three different levels: image (folder: split_random), volumen (folder: split_volume), and subject (folder: split_subject).</p>\n<h1>Chest X-ray repository</h1>\n<p>A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. A chest X-ray repository was built using the ChestX-ray8 dataset, the CheXpert dataset, the MIMIC-CXR dataset, and the PadChest dataset from the TorchXRayVision library. The chest X-ray repository was partitioned into four folds using three different partitioning levels. In image-level partitioning(folder: split1_random), images were randomly distributed across four folds. In patient-level partitioning (folder: split2_patient), all images from the same patient were assigned to the same fold. Finally, in dataset-level partitioning (folder: split3_dataset), each dataset was exclusively allocated to a separate fold.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "2059.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 2159542272,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/14847200",
    "created": "1739227746"
  },
  {
    "id": 97,
    "canonicalId": "s5vfgtscvc8tt7b4b96hgg6x",
    "datasetId": "s5vfgtscvc8tt7b4b96hgg6x",
    "doi": "10.5281/zenodo.3832094",
    "title": "Measurement of Absolute Retinal Blood Flow Using a Laser Doppler Velocimeter Combined with Adaptive Optics",
    "description": "<p><strong>Purpose</strong>:&nbsp;Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.\\newline<br>\n<strong>Methods</strong>:&nbsp;This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes$^\\copyright$,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy humans were done to assess the device.\\newline<br>\n<strong>Results</strong>:&nbsp;In the in vitro experiment, the calculated flow varied between 1.75&micro;l/min and 25.9&micro;l/min and was highly correlated (r<sup>2</sup>= 0.995) with the imposed flow by a syringe pump.<br>\nIn the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between -11%&nbsp;and 36%&nbsp;(mean&plusmn;sd 5.7&plusmn;18.5%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 0.9&nbsp;&micro;L/min and 13.2&micro;L/min.</p>\n\n<p><strong>Conclusion</strong>:&nbsp;This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
    "versionTitle": "2",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>Purpose</strong>:&nbsp;Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.\\newline<br>\n<strong>Methods</strong>:&nbsp;This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes$^\\copyright$,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy humans were done to assess the device.\\newline<br>\n<strong>Results</strong>:&nbsp;In the in vitro experiment, the calculated flow varied between 1.75&micro;l/min and 25.9&micro;l/min and was highly correlated (r<sup>2</sup>= 0.995) with the imposed flow by a syringe pump.<br>\nIn the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between -11%&nbsp;and 36%&nbsp;(mean&plusmn;sd 5.7&plusmn;18.5%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 0.9&nbsp;&micro;L/min and 13.2&micro;L/min.</p>\n\n<p><strong>Conclusion</strong>:&nbsp;This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.3832094",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Measurement of Absolute Retinal Blood Flow Using a Laser Doppler Velocimeter Combined with Adaptive Optics"
          }
        ],
        "version": "2",
        "creator": [
          {
            "creatorName": "Geiser Martial",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Truffer Frederic",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Chappelet Marc-Antoine",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, University Hospital, Grenoble, France"
              }
            ]
          },
          {
            "creatorName": "Maitre Gilbert",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Amoos Serge",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Aptel Florent",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, University Hospital, Grenoble, France"
              }
            ]
          },
          {
            "creatorName": "Chiquet Christophe",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, University Hospital, Grenoble, France"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-05-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>Purpose</strong>:&nbsp;Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.\\newline<br>\n<strong>Methods</strong>:&nbsp;This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes$^\\copyright$,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy humans were done to assess the device.\\newline<br>\n<strong>Results</strong>:&nbsp;In the in vitro experiment, the calculated flow varied between 1.75&micro;l/min and 25.9&micro;l/min and was highly correlated (r<sup>2</sup>= 0.995) with the imposed flow by a syringe pump.<br>\nIn the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between -11%&nbsp;and 36%&nbsp;(mean&plusmn;sd 5.7&plusmn;18.5%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 0.9&nbsp;&micro;L/min and 13.2&micro;L/min.</p>\n\n<p><strong>Conclusion</strong>:&nbsp;This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "laser Doppler velocimetry, retina vessel"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1322.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1386846617,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3832094",
    "created": "1589808734"
  },
  {
    "id": 98,
    "canonicalId": "cb7t5srp1arfbuspz45k7cmp",
    "datasetId": "cb7t5srp1arfbuspz45k7cmp",
    "doi": "10.5061/dryad.b41j15h",
    "title": "Data from: Human retinal pigment epithelium: in vivo cell morphometry, multispectral autofluorescence, and relationship to cone mosaic",
    "description": "Purpose: To characterize in vivo morphometry and multispectral autofluorescence of the retinal pigment epithelial (RPE) cell mosaic and its relationship to cone cell topography across the macula.\n\nMethods: RPE cell morphometrics were computed in regularly spaced regions of interest (ROIs) from contiguous short-wavelength autofluorescence (SWAF) and photoreceptor reflectance images collected across the macula in one eye of 10 normal participants (23\u201365 years) by using adaptive optics scanning light ophthalmoscopy (AOSLO). Infrared autofluorescence (IRAF) images of the RPE were collected with AOSLO in seven normal participants (22\u201365 years), with participant overlap, and compared to SWAF quantitatively and qualitatively.\n\nResults: RPE cell statistics could be analyzed in 84% of SWAF ROIs. RPE cell density consistently decreased with eccentricity from the fovea (participant mean \u00b1 SD: 6026 \u00b1 1590 cells/mm2 at fovea; 4552 \u00b1 1370 cells/mm2 and 3757 \u00b1 1290 cells/mm2 at 3.5 mm temporally and nasally, respectively). Mean cone-to-RPE cell ratio decreased rapidly from 16.6 at the foveal center to &lt;5 by 1 mm. IRAF revealed cells in six of seven participants, in agreement with SWAF RPE cell size and location. Differences in cell fluorescent structure, contrast, and visibility beneath vasculature were observed between modalities.\n\nConclusions: Improvements in AOSLO autofluorescence imaging permit efficient visualization of RPE cells with safe light exposures, allowing individual characterization of RPE cell morphometry that is variable between participants. The normative dataset and analysis of RPE cell IRAF and SWAF herein are essential for understanding microscopic characteristics of cell fluorescence and may assist in interpreting disease progression in RPE cells.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Purpose: To characterize in vivo morphometry and multispectral autofluorescence of the retinal pigment epithelial (RPE) cell mosaic and its relationship to cone cell topography across the macula.\n\nMethods: RPE cell morphometrics were computed in regularly spaced regions of interest (ROIs) from contiguous short-wavelength autofluorescence (SWAF) and photoreceptor reflectance images collected across the macula in one eye of 10 normal participants (23\u201365 years) by using adaptive optics scanning light ophthalmoscopy (AOSLO). Infrared autofluorescence (IRAF) images of the RPE were collected with AOSLO in seven normal participants (22\u201365 years), with participant overlap, and compared to SWAF quantitatively and qualitatively.\n\nResults: RPE cell statistics could be analyzed in 84% of SWAF ROIs. RPE cell density consistently decreased with eccentricity from the fovea (participant mean \u00b1 SD: 6026 \u00b1 1590 cells/mm2 at fovea; 4552 \u00b1 1370 cells/mm2 and 3757 \u00b1 1290 cells/mm2 at 3.5 mm temporally and nasally, respectively). Mean cone-to-RPE cell ratio decreased rapidly from 16.6 at the foveal center to &lt;5 by 1 mm. IRAF revealed cells in six of seven participants, in agreement with SWAF RPE cell size and location. Differences in cell fluorescent structure, contrast, and visibility beneath vasculature were observed between modalities.\n\nConclusions: Improvements in AOSLO autofluorescence imaging permit efficient visualization of RPE cells with safe light exposures, allowing individual characterization of RPE cell morphometry that is variable between participants. The normative dataset and analysis of RPE cell IRAF and SWAF herein are essential for understanding microscopic characteristics of cell fluorescence and may assist in interpreting disease progression in RPE cells.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.b41j15h",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Human retinal pigment epithelium: in vivo cell morphometry, multispectral autofluorescence, and relationship to cone mosaic"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Granger, Charles E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Yang, Qiang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Song, Hongxin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Capital Medical University"
              }
            ]
          },
          {
            "creatorName": "Saito, Kenichi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Canon U.S.A., Inc., Melville, New York, United States*"
              }
            ]
          },
          {
            "creatorName": "Nozato, Koji",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Canon U.S.A., Inc., Melville, New York, United States*"
              }
            ]
          },
          {
            "creatorName": "Latchney, Lisa R.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester Medical Center"
              }
            ]
          },
          {
            "creatorName": "Leonard, Bianca T.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pittsburgh"
              }
            ]
          },
          {
            "creatorName": "Chung, Mina M.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Williams, David R.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Rochester"
              }
            ]
          },
          {
            "creatorName": "Rossi, Ethan A.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Pittsburgh"
              }
            ]
          }
        ],
        "publicationYear": "2019",
        "date": [
          {
            "dateValue": "2019-03-25",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Purpose: To characterize in vivo morphometry and multispectral autofluorescence of the retinal pigment epithelial (RPE) cell mosaic and its relationship to cone cell topography across the macula.\n\nMethods: RPE cell morphometrics were computed in regularly spaced regions of interest (ROIs) from contiguous short-wavelength autofluorescence (SWAF) and photoreceptor reflectance images collected across the macula in one eye of 10 normal participants (23\u201365 years) by using adaptive optics scanning light ophthalmoscopy (AOSLO). Infrared autofluorescence (IRAF) images of the RPE were collected with AOSLO in seven normal participants (22\u201365 years), with participant overlap, and compared to SWAF quantitatively and qualitatively.\n\nResults: RPE cell statistics could be analyzed in 84% of SWAF ROIs. RPE cell density consistently decreased with eccentricity from the fovea (participant mean \u00b1 SD: 6026 \u00b1 1590 cells/mm2 at fovea; 4552 \u00b1 1370 cells/mm2 and 3757 \u00b1 1290 cells/mm2 at 3.5 mm temporally and nasally, respectively). Mean cone-to-RPE cell ratio decreased rapidly from 16.6 at the foveal center to &lt;5 by 1 mm. IRAF revealed cells in six of seven participants, in agreement with SWAF RPE cell size and location. Differences in cell fluorescent structure, contrast, and visibility beneath vasculature were observed between modalities.\n\nConclusions: Improvements in AOSLO autofluorescence imaging permit efficient visualization of RPE cells with safe light exposures, allowing individual characterization of RPE cell morphometry that is variable between participants. The normative dataset and analysis of RPE cell IRAF and SWAF herein are essential for understanding microscopic characteristics of cell fluorescence and may assist in interpreting disease progression in RPE cells.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "photoreceptors"
          },
          {
            "subjectValue": "Adaptive optics"
          },
          {
            "subjectValue": "autofluorescence"
          },
          {
            "subjectValue": "retinal pigment epithelium"
          },
          {
            "subjectValue": "retinal imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "80.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 84410368,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4992944",
    "created": "1624106962"
  },
  {
    "id": 99,
    "canonicalId": "css1qk7krelnm7obtvwvohkq",
    "datasetId": "css1qk7krelnm7obtvwvohkq",
    "doi": "10.5061/dryad.dncjsxkwn",
    "title": "Longitudinal changes in the retinal microstructures of eyes with chiasmal compression",
    "description": "<p><b>Objective:</b> To test the hypothesis that there was a temporal change in the retinal microstructure after decompression surgery for chiasmal compression, the 1-year longitudinal changes in the inner and outer retinal thickness after decompression surgery were analyzed using spectral-domain optical coherence tomography (SD-OCT) with linear mixed-effects models.</p>\n\n<p><span><b>Methods:</b> SD-OCT was obtained from 87 eyes with chiasmal compression and compared to 100 healthy controls. The preoperative and 1-year postoperative longitudinal changes in the retinal layer thickness were measured. The thickness of each of the following retinal layers was analyzed: the macular retinal nerve fiber layer (RNFL), the ganglion cell layer (GCL), the inner plexiform layer (IPL), the inner nuclear layer, the outer plexiform layer, the outer nuclear layer, and the photoreceptor layer.</span></p>\n\n<p><span><b>Results:</b> The RNFL, GCL, and IPL showed thinning at a rate of 1.068 \u03bcm/year (95% confidence interval [CI], 0.523, 1.613), 1.189 \u03bcm/year (95% CI, 0.452, 1.925), and 1.177 \u03bcm/year (95% CI, 0.645, 1.709), respectively, after decompression surgery. The preoperative thickness of the intra-retinal layer was associated with postoperative visual field (VF) recovery (RNFL, odds ratio [OR] = 1.221, 95% CI, 1.058, 1.410; GCL, OR = 1.133, 95% CI, 1.024, 1.254; and IPL, OR = 1.174, 95% CI, 1.002, 1.376).</span></p>\n\n<p><span><b>Conclusions:</b> The changes in retinal microstructure persisted and progressed in eyes with chiasmal compression after decompression surgery. The findings provide insight into the biological and anatomical sequelae following chiasmal compression. The preoperative thickness of the inner retinal layers was associated with postoperative VF recovery. </span></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><b>Objective:</b> To test the hypothesis that there was a temporal change in the retinal microstructure after decompression surgery for chiasmal compression, the 1-year longitudinal changes in the inner and outer retinal thickness after decompression surgery were analyzed using spectral-domain optical coherence tomography (SD-OCT) with linear mixed-effects models.</p>\n\n<p><span><b>Methods:</b> SD-OCT was obtained from 87 eyes with chiasmal compression and compared to 100 healthy controls. The preoperative and 1-year postoperative longitudinal changes in the retinal layer thickness were measured. The thickness of each of the following retinal layers was analyzed: the macular retinal nerve fiber layer (RNFL), the ganglion cell layer (GCL), the inner plexiform layer (IPL), the inner nuclear layer, the outer plexiform layer, the outer nuclear layer, and the photoreceptor layer.</span></p>\n\n<p><span><b>Results:</b> The RNFL, GCL, and IPL showed thinning at a rate of 1.068 \u03bcm/year (95% confidence interval [CI], 0.523, 1.613), 1.189 \u03bcm/year (95% CI, 0.452, 1.925), and 1.177 \u03bcm/year (95% CI, 0.645, 1.709), respectively, after decompression surgery. The preoperative thickness of the intra-retinal layer was associated with postoperative visual field (VF) recovery (RNFL, odds ratio [OR] = 1.221, 95% CI, 1.058, 1.410; GCL, OR = 1.133, 95% CI, 1.024, 1.254; and IPL, OR = 1.174, 95% CI, 1.002, 1.376).</span></p>\n\n<p><span><b>Conclusions:</b> The changes in retinal microstructure persisted and progressed in eyes with chiasmal compression after decompression surgery. The findings provide insight into the biological and anatomical sequelae following chiasmal compression. The preoperative thickness of the inner retinal layers was associated with postoperative VF recovery. </span></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.dncjsxkwn",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Longitudinal changes in the retinal microstructures of eyes with chiasmal compression"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Oh, Sei Yeul",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Samsung Medical Center"
              }
            ]
          },
          {
            "creatorName": "Lee, Ga-In",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Samsung Medical Center"
              }
            ]
          },
          {
            "creatorName": "Son, Ki Young",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Samsung Medical Center"
              }
            ]
          },
          {
            "creatorName": "Park, Kyung-Ah",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Samsung Medical Center"
              }
            ]
          },
          {
            "creatorName": "Kong, Doo-Sik",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Samsung Medical Center"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-12-07",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><b>Objective:</b> To test the hypothesis that there was a temporal change in the retinal microstructure after decompression surgery for chiasmal compression, the 1-year longitudinal changes in the inner and outer retinal thickness after decompression surgery were analyzed using spectral-domain optical coherence tomography (SD-OCT) with linear mixed-effects models.</p>\n\n<p><span><b>Methods:</b> SD-OCT was obtained from 87 eyes with chiasmal compression and compared to 100 healthy controls. The preoperative and 1-year postoperative longitudinal changes in the retinal layer thickness were measured. The thickness of each of the following retinal layers was analyzed: the macular retinal nerve fiber layer (RNFL), the ganglion cell layer (GCL), the inner plexiform layer (IPL), the inner nuclear layer, the outer plexiform layer, the outer nuclear layer, and the photoreceptor layer.</span></p>\n\n<p><span><b>Results:</b> The RNFL, GCL, and IPL showed thinning at a rate of 1.068 \u03bcm/year (95% confidence interval [CI], 0.523, 1.613), 1.189 \u03bcm/year (95% CI, 0.452, 1.925), and 1.177 \u03bcm/year (95% CI, 0.645, 1.709), respectively, after decompression surgery. The preoperative thickness of the intra-retinal layer was associated with postoperative visual field (VF) recovery (RNFL, odds ratio [OR] = 1.221, 95% CI, 1.058, 1.410; GCL, OR = 1.133, 95% CI, 1.024, 1.254; and IPL, OR = 1.174, 95% CI, 1.002, 1.376).</span></p>\n\n<p><span><b>Conclusions:</b> The changes in retinal microstructure persisted and progressed in eyes with chiasmal compression after decompression surgery. The findings provide insight into the biological and anatomical sequelae following chiasmal compression. The preoperative thickness of the inner retinal layers was associated with postoperative VF recovery. </span></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1363148,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4310119",
    "created": "1607367109"
  },
  {
    "id": 100,
    "canonicalId": "ic31wp0m13dqjis5lx265a9h",
    "datasetId": "ic31wp0m13dqjis5lx265a9h",
    "doi": "10.5281/zenodo.8287928",
    "title": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation",
    "description": "<p>We introduce the first video-based <strong>retinal vessel dataset (RVD)</strong>, a collection of 635 smartphone-based videos with detailed vessel annotation. All captured videos have a frame rate of 25 frames per second, with the duration varying between 2 to 30 seconds. The total number of frames in our dataset is over 130,000. These videos are recorded from four clinics, including patients from 50 to 75 years old. More specifically, 264 males and 151 females are included in the collection process.</p>\n\n<p>The annotations provided in our dataset span two dimensions: spatial and temporal. In the spatial dimension, we offer three distinct levels of annotations: binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks. Each kind of annotation is tailored to specific clinical purposes. In the temporal dimension, we focus on the optic disk regions of videos where the retinal vessel fluctuation normally occurs. We select and annotate frames with the maximal and minimal pulse widths as well as label the existence of spontaneous retinal venous pulsations (SVP).</p>\n\n<p>More detailed information can also be found on our website: https://uq-cvlab.github.io/Retinal-Video-Dataset/</p>\n\n<p>&nbsp;</p>\n\n<p>If you find our RVD dataset is useful in your research, please consider cite:</p>\n\n<pre><code>@article{MD2023RVD,\n  title={RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation},\n  author={MD WAHIDUZZAMAN KHAN, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, \n  Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu},\n  journal={arXiv preprint arXiv:2307.06577},\n  year={2023}\n}\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
    "versionTitle": "v1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>We introduce the first video-based <strong>retinal vessel dataset (RVD)</strong>, a collection of 635 smartphone-based videos with detailed vessel annotation. All captured videos have a frame rate of 25 frames per second, with the duration varying between 2 to 30 seconds. The total number of frames in our dataset is over 130,000. These videos are recorded from four clinics, including patients from 50 to 75 years old. More specifically, 264 males and 151 females are included in the collection process.</p>\n\n<p>The annotations provided in our dataset span two dimensions: spatial and temporal. In the spatial dimension, we offer three distinct levels of annotations: binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks. Each kind of annotation is tailored to specific clinical purposes. In the temporal dimension, we focus on the optic disk regions of videos where the retinal vessel fluctuation normally occurs. We select and annotate frames with the maximal and minimal pulse widths as well as label the existence of spontaneous retinal venous pulsations (SVP).</p>\n\n<p>More detailed information can also be found on our website: https://uq-cvlab.github.io/Retinal-Video-Dataset/</p>\n\n<p>&nbsp;</p>\n\n<p>If you find our RVD dataset is useful in your research, please consider cite:</p>\n\n<pre><code>@article{MD2023RVD,\n  title={RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation},\n  author={MD WAHIDUZZAMAN KHAN, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, \n  Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu},\n  journal={arXiv preprint arXiv:2307.06577},\n  year={2023}\n}\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.8287928",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation"
          }
        ],
        "version": "v1",
        "creator": [
          {
            "creatorName": "Md Wahiduzzaman Khan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of technology Sydney"
              }
            ]
          },
          {
            "creatorName": "Hongwei Sheng",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Technology Sydney, Australia"
              }
            ]
          },
          {
            "creatorName": "Hu Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Queensland, Australia"
              }
            ]
          },
          {
            "creatorName": "Heming Du",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Australian National University, Australia"
              }
            ]
          },
          {
            "creatorName": "Sen Wang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Queensland, Australia"
              }
            ]
          },
          {
            "creatorName": "Minas Theodore Coroneo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of New South Wales, Australia"
              }
            ]
          },
          {
            "creatorName": "Farshid Hajati",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Victoria University, Australia"
              }
            ]
          },
          {
            "creatorName": "Sahar Shariflou",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Technology Sydney, Australia"
              }
            ]
          },
          {
            "creatorName": "Michael Kalloniatis",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Deakin University, Australia"
              }
            ]
          },
          {
            "creatorName": "Jack Phu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of New South Wales, Australia"
              }
            ]
          },
          {
            "creatorName": "Ashish Agar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of New South Wales, Australia"
              }
            ]
          },
          {
            "creatorName": "Zi Huang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Queensland, Australia"
              }
            ]
          },
          {
            "creatorName": "Mojtaba Golzan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Technology Sydney, Australi"
              }
            ]
          },
          {
            "creatorName": "Xin Yu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The University of Queensland, Australia"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-08-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>We introduce the first video-based <strong>retinal vessel dataset (RVD)</strong>, a collection of 635 smartphone-based videos with detailed vessel annotation. All captured videos have a frame rate of 25 frames per second, with the duration varying between 2 to 30 seconds. The total number of frames in our dataset is over 130,000. These videos are recorded from four clinics, including patients from 50 to 75 years old. More specifically, 264 males and 151 females are included in the collection process.</p>\n\n<p>The annotations provided in our dataset span two dimensions: spatial and temporal. In the spatial dimension, we offer three distinct levels of annotations: binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks. Each kind of annotation is tailored to specific clinical purposes. In the temporal dimension, we focus on the optic disk regions of videos where the retinal vessel fluctuation normally occurs. We select and annotate frames with the maximal and minimal pulse widths as well as label the existence of spontaneous retinal venous pulsations (SVP).</p>\n\n<p>More detailed information can also be found on our website: https://uq-cvlab.github.io/Retinal-Video-Dataset/</p>\n\n<p>&nbsp;</p>\n\n<p>If you find our RVD dataset is useful in your research, please consider cite:</p>\n\n<pre><code>@article{MD2023RVD,\n  title={RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation},\n  author={MD WAHIDUZZAMAN KHAN, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, \n  Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu},\n  journal={arXiv preprint arXiv:2307.06577},\n  year={2023}\n}\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Retinal Imaging"
          },
          {
            "subjectValue": "Video dataset"
          },
          {
            "subjectValue": "Handheld Deivce Fundus Imaging"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "24847.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 26054072729,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8287928",
    "created": "1693269520"
  },
  {
    "id": 101,
    "canonicalId": "m4pas9hafs1l7en5cyv6zw1o",
    "datasetId": "m4pas9hafs1l7en5cyv6zw1o",
    "doi": "10.5281/zenodo.8009107",
    "title": "A Fundus Image Dataset for Domain Generalization in Joint Segmentation of Optic Disc and Optic Cup",
    "description": "<p>We provide a fundus image dataset for domain generalization, which includes 5&nbsp;different medical centres.<br>\nThis dataset is based on the REFUGE[1] dataset, Drishti-GS[2] dataset, ORIGA[3] dataset, and RIGA[4] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1-4].</p>\n\n<table>\n\t<caption>Details of this dataset</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>Domain</td>\n\t\t\t<td>Cases in Each Domain<br>\n\t\t\t(Training/Test)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>REFUGE</td>\n\t\t\t<td>320/80</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Drishti-GS</td>\n\t\t\t<td>50/51</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>ORIGA</td>\n\t\t\t<td>500/150</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>BinRushed (RIGA)</td>\n\t\t\t<td>156/39</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Magrabia (RIGA)</td>\n\t\t\t<td>76/19</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1] Orlando J I, Fu H, Breda J B, et al. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs[J]. Medical image analysis, 2020, 59: 101570.</p>\n\n<p>[2]&nbsp;Sivaswamy J, Krishnadas S R, Joshi G D, et al. Drishti-GS: Retinal image dataset for optic nerve head (onh) segmentation[C]//2014 IEEE 11th international symposium on biomedical imaging (ISBI). IEEE, 2014: 53-56.</p>\n\n<p>[3]&nbsp;Zhang Z, Yin F S, Liu J, et al. Origa-light: An online retinal fundus image database for glaucoma analysis and research[C]//2010 Annual international conference of the IEEE engineering in medicine and biology. IEEE, 2010: 3065-3068.</p>\n\n<p>[4]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. SPIE, 2018, 10579: 55-62.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code class=\"language-markdown\">@article{chen2023treasure,\n  title={Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation},\n  author={Chen, Ziyang and Pan, Yongsheng and Ye, Yiwen and Cui, Hengfei and Xia, Yong},\n  booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023},\n  year={2023}\n}</code></pre>\n\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>We provide a fundus image dataset for domain generalization, which includes 5&nbsp;different medical centres.<br>\nThis dataset is based on the REFUGE[1] dataset, Drishti-GS[2] dataset, ORIGA[3] dataset, and RIGA[4] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1-4].</p>\n\n<table>\n\t<caption>Details of this dataset</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>Domain</td>\n\t\t\t<td>Cases in Each Domain<br>\n\t\t\t(Training/Test)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>REFUGE</td>\n\t\t\t<td>320/80</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Drishti-GS</td>\n\t\t\t<td>50/51</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>ORIGA</td>\n\t\t\t<td>500/150</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>BinRushed (RIGA)</td>\n\t\t\t<td>156/39</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Magrabia (RIGA)</td>\n\t\t\t<td>76/19</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1] Orlando J I, Fu H, Breda J B, et al. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs[J]. Medical image analysis, 2020, 59: 101570.</p>\n\n<p>[2]&nbsp;Sivaswamy J, Krishnadas S R, Joshi G D, et al. Drishti-GS: Retinal image dataset for optic nerve head (onh) segmentation[C]//2014 IEEE 11th international symposium on biomedical imaging (ISBI). IEEE, 2014: 53-56.</p>\n\n<p>[3]&nbsp;Zhang Z, Yin F S, Liu J, et al. Origa-light: An online retinal fundus image database for glaucoma analysis and research[C]//2010 Annual international conference of the IEEE engineering in medicine and biology. IEEE, 2010: 3065-3068.</p>\n\n<p>[4]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. SPIE, 2018, 10579: 55-62.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code class=\"language-markdown\">@article{chen2023treasure,\n  title={Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation},\n  author={Chen, Ziyang and Pan, Yongsheng and Ye, Yiwen and Cui, Hengfei and Xia, Yong},\n  booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023},\n  year={2023}\n}</code></pre>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.8009107",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "A Fundus Image Dataset for Domain Generalization in Joint Segmentation of Optic Disc and Optic Cup"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ziyang Chen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Yongsheng Pan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Yiwen Ye",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Hengfei Cui",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Yong Xia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-06-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>We provide a fundus image dataset for domain generalization, which includes 5&nbsp;different medical centres.<br>\nThis dataset is based on the REFUGE[1] dataset, Drishti-GS[2] dataset, ORIGA[3] dataset, and RIGA[4] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1-4].</p>\n\n<table>\n\t<caption>Details of this dataset</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>Domain</td>\n\t\t\t<td>Cases in Each Domain<br>\n\t\t\t(Training/Test)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>REFUGE</td>\n\t\t\t<td>320/80</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Drishti-GS</td>\n\t\t\t<td>50/51</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>ORIGA</td>\n\t\t\t<td>500/150</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>BinRushed (RIGA)</td>\n\t\t\t<td>156/39</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Magrabia (RIGA)</td>\n\t\t\t<td>76/19</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1] Orlando J I, Fu H, Breda J B, et al. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs[J]. Medical image analysis, 2020, 59: 101570.</p>\n\n<p>[2]&nbsp;Sivaswamy J, Krishnadas S R, Joshi G D, et al. Drishti-GS: Retinal image dataset for optic nerve head (onh) segmentation[C]//2014 IEEE 11th international symposium on biomedical imaging (ISBI). IEEE, 2014: 53-56.</p>\n\n<p>[3]&nbsp;Zhang Z, Yin F S, Liu J, et al. Origa-light: An online retinal fundus image database for glaucoma analysis and research[C]//2010 Annual international conference of the IEEE engineering in medicine and biology. IEEE, 2010: 3065-3068.</p>\n\n<p>[4]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. SPIE, 2018, 10579: 55-62.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code class=\"language-markdown\">@article{chen2023treasure,\n  title={Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation},\n  author={Chen, Ziyang and Pan, Yongsheng and Ye, Yiwen and Cui, Hengfei and Xia, Yong},\n  booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023},\n  year={2023}\n}</code></pre>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "587.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 615514112,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8009107",
    "created": "1686032930"
  },
  {
    "id": 102,
    "canonicalId": "f5w7hfqvd1usb1549pdn9oc6",
    "datasetId": "f5w7hfqvd1usb1549pdn9oc6",
    "doi": "10.5281/zenodo.3635402",
    "title": "Measurement of Absolute Retinal Blood Flow Using a Laser Doppler Velocimeter Combined with Adaptive Optics",
    "description": "<p><strong>Data set of measurements related to the following:</strong></p>\n\n<p><strong>Purpose:&nbsp;</strong>Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.</p>\n\n<p><strong>Methods:&nbsp;</strong>This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes&copy;,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy human retinal venous junctions were done to assess the device.</p>\n\n<p><strong>Results:&nbsp;</strong>In the in vitro experiment, the calculated flow varied between 1.75&nbsp;&mu;l/min and 25.9&nbsp;&mu;l/min and was highly correlated (r2&nbsp;= 0.995) with the imposed flow by a syringe pump. In the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between&nbsp;&minus;25% and 17% (mean&plusmn;sd&nbsp;&minus;2&nbsp;&plusmn;&nbsp;17%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 1.3&nbsp;&mu;L/min and 28.7&nbsp;&mu;L/min</p>\n\n<p><strong>Conclusion:&nbsp;</strong>This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>Data set of measurements related to the following:</strong></p>\n\n<p><strong>Purpose:&nbsp;</strong>Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.</p>\n\n<p><strong>Methods:&nbsp;</strong>This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes&copy;,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy human retinal venous junctions were done to assess the device.</p>\n\n<p><strong>Results:&nbsp;</strong>In the in vitro experiment, the calculated flow varied between 1.75&nbsp;&mu;l/min and 25.9&nbsp;&mu;l/min and was highly correlated (r2&nbsp;= 0.995) with the imposed flow by a syringe pump. In the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between&nbsp;&minus;25% and 17% (mean&plusmn;sd&nbsp;&minus;2&nbsp;&plusmn;&nbsp;17%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 1.3&nbsp;&mu;L/min and 28.7&nbsp;&mu;L/min</p>\n\n<p><strong>Conclusion:&nbsp;</strong>This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.3635402",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Measurement of Absolute Retinal Blood Flow Using a Laser Doppler Velocimeter Combined with Adaptive Optics"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Geiser Martial",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Truffer Frederic",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Strese Helene",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Ma\u00eetre Gilbert",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Amoos Serge",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Applied Sciences of Western Switzerland"
              }
            ]
          },
          {
            "creatorName": "Chappelet Marc-Antoine",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Grenoble Alpes University, France"
              }
            ]
          },
          {
            "creatorName": "Aptel Florent",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Grenoble Alpes University"
              }
            ]
          },
          {
            "creatorName": "Chiquet Christophe",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Marc-Antoine"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-02-04",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>Data set of measurements related to the following:</strong></p>\n\n<p><strong>Purpose:&nbsp;</strong>Development and validation of an absolute laser Doppler velocimeter (LDV) based on an adaptive optical fundus camera which provides simultaneously high definition images of the fundus vessels and absolute maximal red blood cells (RBCs) velocity in order to calculate the absolute retinal blood flow.</p>\n\n<p><strong>Methods:&nbsp;</strong>This new absolute laser Doppler velocimeter is combined with the adaptive optics fundus camera (rtx1, Imagine Eyes&copy;,Orsay, France) outside its optical wavefront correction path. A 4 seconds recording includes 40 images, each synchronized with two Doppler shift power spectra. Image analysis provides the vessel diameter close to the probing beam and the velocity of the RBCs in the vessels are extracted from the Doppler spectral analysis. Combination of those values gives an average of the absolute retinal blood flow. An in vitro experiment consisting of latex microspheres flowing in water through a glass-capillary to simulate a blood vessel and in vivo measurements on six healthy human retinal venous junctions were done to assess the device.</p>\n\n<p><strong>Results:&nbsp;</strong>In the in vitro experiment, the calculated flow varied between 1.75&nbsp;&mu;l/min and 25.9&nbsp;&mu;l/min and was highly correlated (r2&nbsp;= 0.995) with the imposed flow by a syringe pump. In the in vivo experiment, the error between the flow in the parent vessel and the sum of the flow in the daughter vessels was between&nbsp;&minus;25% and 17% (mean&plusmn;sd&nbsp;&minus;2&nbsp;&plusmn;&nbsp;17%). Retinal blood flow in the main temporal retinal veins of healthy subjects varied between 1.3&nbsp;&mu;L/min and 28.7&nbsp;&mu;L/min</p>\n\n<p><strong>Conclusion:&nbsp;</strong>This adaptive optics LDV prototype (aoLDV) allows the measurement of absolute retinal blood flow derived from the retinal vessel diameter and the maximum RBCs velocity in that vessel.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "laser Doppler velocimetry, ocular blood flow"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1342.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1408027852,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3635402",
    "created": "1580985541"
  },
  {
    "id": 103,
    "canonicalId": "ozvv5wshnazs894nopmmz176",
    "datasetId": "ozvv5wshnazs894nopmmz176",
    "doi": "10.5281/zenodo.7678656",
    "title": "Topological characterization of the retinal microvascular network visualized by portable fundus camera- effects of chronic disease (TREND2)  database",
    "description": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the&nbsp;<strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND 2</strong>) is a database of digital color eye fundus images created as an addition to TREND&nbsp; database (https://zenodo.org/badge/DOI/10.5281/zenodo.4521044.svg).</p>\n\n<p>TREND 2 databse was created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2023.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal and abnormal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 28&nbsp;color fundus images of old&nbsp;subjects (20 images from subjects with one or more chronic diseases such as type 2 diabetes mellitus, hypertension or Alzheimer&#39;s dementia- O_CD group, and 8 images from subjects with no chronic diseases- O_NCD group). Each image is associated with a corresponding binarized image of the manually segmented microvascular network.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion Criteria</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">O_NCD group</th>\n\t\t\t<th scope=\"col\">O_CD group</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- at least 56 years old</td>\n\t\t\t<td>- at least 56 years old</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>- no history of alcohol, or drug abuse, or psychiatric disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- negative history of any chronic disease</td>\n\t\t\t<td>\n\t\t\t<p>- controlled hypertension (blood pressure&lt;140/90 mmHg), and/or</p>\n\n\t\t\t<p>- controlled type 2 diabetes mellitus, and/or</p>\n\n\t\t\t<p>- Alzheimer&#39;s dementia</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p><strong>Files:</strong></p>\n\n<p>1_OLD WITH CHRONIC DISEASE_RAW (20 images in tif&nbsp;format)</p>\n\n<p>2_OLD WITH CHRONIC DISEASE_SEGMENTED (20 images in png format)</p>\n\n<p>3_OLD WITH NO CHRONIC DISEASE_RAW (8 images in tif&nbsp;format)</p>\n\n<p>4_OLD WITH NO CHRONIC DISEASE SEGMENTED (8 images in png format)</p>\n\n<p>5_ASSOCIATED DATA (xslx format)</p>\n\n<p>6_RETINAL PATHOLOGY (docx format)</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
    "versionTitle": "1.1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the&nbsp;<strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND 2</strong>) is a database of digital color eye fundus images created as an addition to TREND&nbsp; database (https://zenodo.org/badge/DOI/10.5281/zenodo.4521044.svg).</p>\n\n<p>TREND 2 databse was created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2023.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal and abnormal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 28&nbsp;color fundus images of old&nbsp;subjects (20 images from subjects with one or more chronic diseases such as type 2 diabetes mellitus, hypertension or Alzheimer&#39;s dementia- O_CD group, and 8 images from subjects with no chronic diseases- O_NCD group). Each image is associated with a corresponding binarized image of the manually segmented microvascular network.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion Criteria</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">O_NCD group</th>\n\t\t\t<th scope=\"col\">O_CD group</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- at least 56 years old</td>\n\t\t\t<td>- at least 56 years old</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>- no history of alcohol, or drug abuse, or psychiatric disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- negative history of any chronic disease</td>\n\t\t\t<td>\n\t\t\t<p>- controlled hypertension (blood pressure&lt;140/90 mmHg), and/or</p>\n\n\t\t\t<p>- controlled type 2 diabetes mellitus, and/or</p>\n\n\t\t\t<p>- Alzheimer&#39;s dementia</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p><strong>Files:</strong></p>\n\n<p>1_OLD WITH CHRONIC DISEASE_RAW (20 images in tif&nbsp;format)</p>\n\n<p>2_OLD WITH CHRONIC DISEASE_SEGMENTED (20 images in png format)</p>\n\n<p>3_OLD WITH NO CHRONIC DISEASE_RAW (8 images in tif&nbsp;format)</p>\n\n<p>4_OLD WITH NO CHRONIC DISEASE SEGMENTED (8 images in png format)</p>\n\n<p>5_ASSOCIATED DATA (xslx format)</p>\n\n<p>6_RETINAL PATHOLOGY (docx format)</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7678656",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Topological characterization of the retinal microvascular network visualized by portable fundus camera- effects of chronic disease (TREND2)  database"
          }
        ],
        "version": "1.1",
        "creator": [
          {
            "creatorName": "Popovic, Natasa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Vujosevic, Stela",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Milan, Department of Biomedical, Surgical and Dental Sciences"
              }
            ]
          },
          {
            "creatorName": "Radunovic, Miroslav",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Ad\u017ei\u0107 Ze\u010devi\u0107, Antoaneta",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "\u017ddralevi\u0107, Ma\u0161a",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Rov\u010danin Dragovi\u0107, Isidora",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Vuk\u010devi\u0107, Barti\u0107",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Popovic, Tomo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty for Information Systems and Technologies, University of Donja Gorica"
              }
            ]
          },
          {
            "creatorName": "Radulovi\u0107, Ljiljana",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Vukovi\u0107, Tijana",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Clinical Center of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Erakovi\u0107, Jevto",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Clinical Center of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Lazovi\u0107, Ranko",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          },
          {
            "creatorName": "Radunovi\u0107, Miodrag",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-02-26",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the&nbsp;<strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND 2</strong>) is a database of digital color eye fundus images created as an addition to TREND&nbsp; database (https://zenodo.org/badge/DOI/10.5281/zenodo.4521044.svg).</p>\n\n<p>TREND 2 databse was created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2023.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal and abnormal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 28&nbsp;color fundus images of old&nbsp;subjects (20 images from subjects with one or more chronic diseases such as type 2 diabetes mellitus, hypertension or Alzheimer&#39;s dementia- O_CD group, and 8 images from subjects with no chronic diseases- O_NCD group). Each image is associated with a corresponding binarized image of the manually segmented microvascular network.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion Criteria</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">O_NCD group</th>\n\t\t\t<th scope=\"col\">O_CD group</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- at least 56 years old</td>\n\t\t\t<td>- at least 56 years old</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- no current acute disease</p>\n\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- no history of alcohol, or drug abuse, or psychiatric disease</p>\n\t\t\t</td>\n\t\t\t<td>- no history of alcohol, or drug abuse, or psychiatric disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- negative history of any chronic disease</td>\n\t\t\t<td>\n\t\t\t<p>- controlled hypertension (blood pressure&lt;140/90 mmHg), and/or</p>\n\n\t\t\t<p>- controlled type 2 diabetes mellitus, and/or</p>\n\n\t\t\t<p>- Alzheimer&#39;s dementia</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t\t<td><strong>Exclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t<p>- presence of opacities of the transparent media in both eyes affecting</p>\n\n\t\t\t<p>- myopia &ge;5 diopters</p>\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p><strong>Files:</strong></p>\n\n<p>1_OLD WITH CHRONIC DISEASE_RAW (20 images in tif&nbsp;format)</p>\n\n<p>2_OLD WITH CHRONIC DISEASE_SEGMENTED (20 images in png format)</p>\n\n<p>3_OLD WITH NO CHRONIC DISEASE_RAW (8 images in tif&nbsp;format)</p>\n\n<p>4_OLD WITH NO CHRONIC DISEASE SEGMENTED (8 images in png format)</p>\n\n<p>5_ASSOCIATED DATA (xslx format)</p>\n\n<p>6_RETINAL PATHOLOGY (docx format)</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "85.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 90072678,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7678656",
    "created": "1683794450"
  },
  {
    "id": 104,
    "canonicalId": "on4bsaoxtpavi8vxb0bkv58m",
    "datasetId": "on4bsaoxtpavi8vxb0bkv58m",
    "doi": "10.5281/zenodo.8040573",
    "title": "2023 IEEE SPS Video and Image Processing (VIP) Cup: Ophthalmic Biomarker Detection",
    "description": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been <em>generalization</em> and <em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been <em>generalization</em> and <em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.8040573",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "2023 IEEE SPS Video and Image Processing (VIP) Cup: Ophthalmic Biomarker Detection"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ghassan AlRegib",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Mohit Prabhushankar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Kiran Kokilepersaud",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Prithwijit Chowdhury",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Zoe Fowler",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-06-14",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been <em>generalization</em> and <em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "ophthalmology, retinal disease detection, biomarker classification, deep learning dataset"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "33026.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 34630795264,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8040573",
    "created": "1686775176"
  },
  {
    "id": 105,
    "canonicalId": "zbzqjd9ph7lobgp1olu1dmq3",
    "datasetId": "zbzqjd9ph7lobgp1olu1dmq3",
    "doi": "10.5281/zenodo.13857013",
    "title": "High-Capacity Mesoporous Silica Nanocarriers of siRNA for Applications in Retinal Delivery",
    "description": "<p>The main cause of subretinal neovascularisation in wet age-related macular degeneration (AMD) is an abnormal expression in the retinal pigment epithelium (RPE) of the vascular endothelial growth factor (VEGF). Current approaches for the treatment of AMD present considerable issues that could be overcome by encapsulating anti-VEGF drugs in suitable nanocarriers, thus providing better penetration, higher retention times, and sustained release. In this work, the ability of large pore mesoporous silica nanoparticles (LP-MSNs) to transport and protect nucleic acid molecules is exploited to develop an innovative LP-MSN-based nanosystem for the topical administration of anti-VEGF siRNA molecules to RPE cells. siRNA is loaded into LP-MSN mesopores, while the external surface of the nanodevices is functionalised with polyethylenimine (PEI) chains that allow the controlled release of siRNA and promote endosomal escape to facilitate cytosolic delivery of the cargo. The successful results obtained for VEGF silencing in ARPE-19 RPE cells demonstrate that the designed nanodevice is suitable as an siRNA transporter.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>The main cause of subretinal neovascularisation in wet age-related macular degeneration (AMD) is an abnormal expression in the retinal pigment epithelium (RPE) of the vascular endothelial growth factor (VEGF). Current approaches for the treatment of AMD present considerable issues that could be overcome by encapsulating anti-VEGF drugs in suitable nanocarriers, thus providing better penetration, higher retention times, and sustained release. In this work, the ability of large pore mesoporous silica nanoparticles (LP-MSNs) to transport and protect nucleic acid molecules is exploited to develop an innovative LP-MSN-based nanosystem for the topical administration of anti-VEGF siRNA molecules to RPE cells. siRNA is loaded into LP-MSN mesopores, while the external surface of the nanodevices is functionalised with polyethylenimine (PEI) chains that allow the controlled release of siRNA and promote endosomal escape to facilitate cytosolic delivery of the cargo. The successful results obtained for VEGF silencing in ARPE-19 RPE cells demonstrate that the designed nanodevice is suitable as an siRNA transporter.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.13857013",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "High-Capacity Mesoporous Silica Nanocarriers of siRNA for Applications in Retinal Delivery"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ultimo, Amelia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Orzaez, Mar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Santos-Martinez, Maria",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Martinez-Manez, Ramon",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Marcos, Maria",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Sancen\u00f3n, Felix",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Ruiz-Hern\u00e1ndez, Eduardo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-02-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>The main cause of subretinal neovascularisation in wet age-related macular degeneration (AMD) is an abnormal expression in the retinal pigment epithelium (RPE) of the vascular endothelial growth factor (VEGF). Current approaches for the treatment of AMD present considerable issues that could be overcome by encapsulating anti-VEGF drugs in suitable nanocarriers, thus providing better penetration, higher retention times, and sustained release. In this work, the ability of large pore mesoporous silica nanoparticles (LP-MSNs) to transport and protect nucleic acid molecules is exploited to develop an innovative LP-MSN-based nanosystem for the topical administration of anti-VEGF siRNA molecules to RPE cells. siRNA is loaded into LP-MSN mesopores, while the external surface of the nanodevices is functionalised with polyethylenimine (PEI) chains that allow the controlled release of siRNA and promote endosomal escape to facilitate cytosolic delivery of the cargo. The successful results obtained for VEGF silencing in ARPE-19 RPE cells demonstrate that the designed nanodevice is suitable as an siRNA transporter.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1016.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1065667788,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/13857013",
    "created": "1727624873"
  },
  {
    "id": 106,
    "canonicalId": "ezo76sezz3sl77j653bssvyw",
    "datasetId": "ezo76sezz3sl77j653bssvyw",
    "doi": "10.5281/zenodo.14580071",
    "title": "OCTAVE Dataset: Optical Coherence Tomography Annotated Volume Experiment",
    "description": "<div>\n<p>This is a large dataset of OCT 3D image volumes and pixel-level segmentation labels for the development of an deep learning model to autonomously detect and identify anatomic and pathological features of the retina.</p>\n<p>This project was described in the paper:</p>\n<p><strong>&ldquo;Identifying Retinal Features Using a Self\u2011Configuring CNN for Clinical Intervention&rdquo;</strong><br><em>Daniel\u202fS.\u202fKermany, Wesley\u202fPoon, Anaya\u202fBawiskar, Natasha\u202fNehra, Orhun\u202fDavarci, Glori\u202fDas, Matthew\u202fVasquez, Shlomit\u202fSchaal, Raksha\u202fRaghunathan &amp; Stephen\u202fT.\u202fC.\u202fWong Invest. Ophthalmol. Vis. Sci., June\u202f2,\u202f2025; <a href=\"https://iovs.arvojournals.org/article.aspx?articleid=2803144\">PMID\u202f40525921</a></em></p>\nInstructions for using this dataset or replicating the results of the paper can be found on the&nbsp;<a href=\"https://github.com/Translational-Biophotonics-Laboratory/octvision3d\">OCTAVE GitHub page</a></div>\n<div>&nbsp;</div>\n<div>This dataset is made available for use in research only. <strong>Use of this dataset requires appropriate citation of both this dataset (DOI: 10.5281/zenodo.14580071) and the associated paper (DOI: 10.1167/iovs.66.6.55).</strong></div>\n<div>&nbsp;</div>\n<div><strong>Important: Ensure you are using the latest version of this dataset, if multiple versions available<br><br>Acknowledgments</strong></div>\n<div>\n<div>\n<div>Supported by the National Eye Institute F31EY037177 (D.S.K.)</div>\n<div>National Cancer Institute R01CA288613 (S.T.C.W.)</div>\n<div>National Cancer Institute R01NS140292 (S.T.C.W.)</div>\n<div>T.T. and W.F. Chao Foundation (S.T.C.W.)</div>\n<div>John S. Dunn Research Foundation (S.T.C.W.)</div>\n<div>Johnsson Estate (S.T.C.W.).&nbsp;</div>\n</div>\n</div>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<div>\n<p>This is a large dataset of OCT 3D image volumes and pixel-level segmentation labels for the development of an deep learning model to autonomously detect and identify anatomic and pathological features of the retina.</p>\n<p>This project was described in the paper:</p>\n<p><strong>&ldquo;Identifying Retinal Features Using a Self\u2011Configuring CNN for Clinical Intervention&rdquo;</strong><br><em>Daniel\u202fS.\u202fKermany, Wesley\u202fPoon, Anaya\u202fBawiskar, Natasha\u202fNehra, Orhun\u202fDavarci, Glori\u202fDas, Matthew\u202fVasquez, Shlomit\u202fSchaal, Raksha\u202fRaghunathan &amp; Stephen\u202fT.\u202fC.\u202fWong Invest. Ophthalmol. Vis. Sci., June\u202f2,\u202f2025; <a href=\"https://iovs.arvojournals.org/article.aspx?articleid=2803144\">PMID\u202f40525921</a></em></p>\nInstructions for using this dataset or replicating the results of the paper can be found on the&nbsp;<a href=\"https://github.com/Translational-Biophotonics-Laboratory/octvision3d\">OCTAVE GitHub page</a></div>\n<div>&nbsp;</div>\n<div>This dataset is made available for use in research only. <strong>Use of this dataset requires appropriate citation of both this dataset (DOI: 10.5281/zenodo.14580071) and the associated paper (DOI: 10.1167/iovs.66.6.55).</strong></div>\n<div>&nbsp;</div>\n<div><strong>Important: Ensure you are using the latest version of this dataset, if multiple versions available<br><br>Acknowledgments</strong></div>\n<div>\n<div>\n<div>Supported by the National Eye Institute F31EY037177 (D.S.K.)</div>\n<div>National Cancer Institute R01CA288613 (S.T.C.W.)</div>\n<div>National Cancer Institute R01NS140292 (S.T.C.W.)</div>\n<div>T.T. and W.F. Chao Foundation (S.T.C.W.)</div>\n<div>John S. Dunn Research Foundation (S.T.C.W.)</div>\n<div>Johnsson Estate (S.T.C.W.).&nbsp;</div>\n</div>\n</div>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.14580071",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "OCTAVE Dataset: Optical Coherence Tomography Annotated Volume Experiment"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kermany, Daniel",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Poon, Wesley",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Bawiskar, Anaya",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Nehra, Natasha",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Davarci, Orhun",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Das, Glori",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Texas A&M University"
              }
            ]
          },
          {
            "creatorName": "Vasquez, Matthew",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Schaal, Shlomit",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Raghunathan, Raksha",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          },
          {
            "creatorName": "Wong, Stephen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Houston Methodist"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-06-02",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<div>\n<p>This is a large dataset of OCT 3D image volumes and pixel-level segmentation labels for the development of an deep learning model to autonomously detect and identify anatomic and pathological features of the retina.</p>\n<p>This project was described in the paper:</p>\n<p><strong>&ldquo;Identifying Retinal Features Using a Self\u2011Configuring CNN for Clinical Intervention&rdquo;</strong><br><em>Daniel\u202fS.\u202fKermany, Wesley\u202fPoon, Anaya\u202fBawiskar, Natasha\u202fNehra, Orhun\u202fDavarci, Glori\u202fDas, Matthew\u202fVasquez, Shlomit\u202fSchaal, Raksha\u202fRaghunathan &amp; Stephen\u202fT.\u202fC.\u202fWong Invest. Ophthalmol. Vis. Sci., June\u202f2,\u202f2025; <a href=\"https://iovs.arvojournals.org/article.aspx?articleid=2803144\">PMID\u202f40525921</a></em></p>\nInstructions for using this dataset or replicating the results of the paper can be found on the&nbsp;<a href=\"https://github.com/Translational-Biophotonics-Laboratory/octvision3d\">OCTAVE GitHub page</a></div>\n<div>&nbsp;</div>\n<div>This dataset is made available for use in research only. <strong>Use of this dataset requires appropriate citation of both this dataset (DOI: 10.5281/zenodo.14580071) and the associated paper (DOI: 10.1167/iovs.66.6.55).</strong></div>\n<div>&nbsp;</div>\n<div><strong>Important: Ensure you are using the latest version of this dataset, if multiple versions available<br><br>Acknowledgments</strong></div>\n<div>\n<div>\n<div>Supported by the National Eye Institute F31EY037177 (D.S.K.)</div>\n<div>National Cancer Institute R01CA288613 (S.T.C.W.)</div>\n<div>National Cancer Institute R01NS140292 (S.T.C.W.)</div>\n<div>T.T. and W.F. Chao Foundation (S.T.C.W.)</div>\n<div>John S. Dunn Research Foundation (S.T.C.W.)</div>\n<div>Johnsson Estate (S.T.C.W.).&nbsp;</div>\n</div>\n</div>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1150.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1206281830,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/14580071",
    "created": "1751040330"
  },
  {
    "id": 107,
    "canonicalId": "p97mrippdi8l446jo3mj3wk2",
    "datasetId": "p97mrippdi8l446jo3mj3wk2",
    "doi": "10.5061/dryad.gmsbcc2p2",
    "title": "Simple and complex, sexually dimorphic retinal mosaic of fritillary butterflies",
    "description": "<p>Butterflies have variable sets of spectral photoreceptors that underlie colour vision. The photoreceptor organization may be optimised for the detection of body colouration. Fritillaries (Argynnini) are nymphalid butterflies exhibiting varying degrees of sexual dimorphism in wing colouration. In two sister species, the females have orange (<i>Argynnis </i><i>paphia</i>) and dark wings (<i>A. </i><i>sagana</i>), respectively, while the males of both species have orange wings with large patches of pheromone-producing androconia. In spite of the differences in female colouration, the eyes of both species exhibit an identical sexual dimorphism. The female eyeshine is uniform yellow, while the males have a complex retinal mosaic with yellow and red-reflecting ommatidia. We found the basic set of UV-, blue- and green-peaking photoreceptors in both sexes. Males additionally have three more photoreceptor classes, peaking in the green, yellow and red, respectively. The latter is the basal R9, indirectly measured through hyperpolarisations in the green-peaking R1-2. In many nymphalid tribes, including the closely related Heliconiini, the retinal mosaic is complex in both sexes. We hypothesise that the simple mosaic of female Argynnini is a secondary reduction, possibly driven by the use of olfaction for intraspecific recognition, whereas vision remains the primary sense for the task in the males.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Butterflies have variable sets of spectral photoreceptors that underlie colour vision. The photoreceptor organization may be optimised for the detection of body colouration. Fritillaries (Argynnini) are nymphalid butterflies exhibiting varying degrees of sexual dimorphism in wing colouration. In two sister species, the females have orange (<i>Argynnis </i><i>paphia</i>) and dark wings (<i>A. </i><i>sagana</i>), respectively, while the males of both species have orange wings with large patches of pheromone-producing androconia. In spite of the differences in female colouration, the eyes of both species exhibit an identical sexual dimorphism. The female eyeshine is uniform yellow, while the males have a complex retinal mosaic with yellow and red-reflecting ommatidia. We found the basic set of UV-, blue- and green-peaking photoreceptors in both sexes. Males additionally have three more photoreceptor classes, peaking in the green, yellow and red, respectively. The latter is the basal R9, indirectly measured through hyperpolarisations in the green-peaking R1-2. In many nymphalid tribes, including the closely related Heliconiini, the retinal mosaic is complex in both sexes. We hypothesise that the simple mosaic of female Argynnini is a secondary reduction, possibly driven by the use of olfaction for intraspecific recognition, whereas vision remains the primary sense for the task in the males.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.gmsbcc2p2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Simple and complex, sexually dimorphic retinal mosaic of fritillary butterflies"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Belu\u0161i\u010d, Gregor",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Ljubljana"
              }
            ]
          },
          {
            "creatorName": "Arikawa, Kentaro",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "The Graduate University for Advanced Studies, SOKENDAI"
              }
            ]
          },
          {
            "creatorName": "Ili\u0107, Marko",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Ljubljana"
              }
            ]
          },
          {
            "creatorName": "Pirih, Primo\u017e",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Ljubljana"
              }
            ]
          },
          {
            "creatorName": "Chen, Pei-Ju",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Academia Sinica"
              }
            ]
          },
          {
            "creatorName": "Megli\u010d, Andrej",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ljubljana University Medical Centre"
              }
            ]
          },
          {
            "creatorName": "Prevc, Jo\u0161t",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Ljubljana"
              }
            ]
          },
          {
            "creatorName": "Yago, Masaya",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Tokyo"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-01-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Butterflies have variable sets of spectral photoreceptors that underlie colour vision. The photoreceptor organization may be optimised for the detection of body colouration. Fritillaries (Argynnini) are nymphalid butterflies exhibiting varying degrees of sexual dimorphism in wing colouration. In two sister species, the females have orange (<i>Argynnis </i><i>paphia</i>) and dark wings (<i>A. </i><i>sagana</i>), respectively, while the males of both species have orange wings with large patches of pheromone-producing androconia. In spite of the differences in female colouration, the eyes of both species exhibit an identical sexual dimorphism. The female eyeshine is uniform yellow, while the males have a complex retinal mosaic with yellow and red-reflecting ommatidia. We found the basic set of UV-, blue- and green-peaking photoreceptors in both sexes. Males additionally have three more photoreceptor classes, peaking in the green, yellow and red, respectively. The latter is the basal R9, indirectly measured through hyperpolarisations in the green-peaking R1-2. In many nymphalid tribes, including the closely related Heliconiini, the retinal mosaic is complex in both sexes. We hypothesise that the simple mosaic of female Argynnini is a secondary reduction, possibly driven by the use of olfaction for intraspecific recognition, whereas vision remains the primary sense for the task in the males.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Argynnis paphia"
          },
          {
            "subjectValue": "Argynnis sagana"
          },
          {
            "subjectValue": "retinal mosaic"
          },
          {
            "subjectValue": "eyeshine"
          },
          {
            "subjectValue": "Compound eye"
          },
          {
            "subjectValue": "butterfly"
          },
          {
            "subjectValue": "Colour Vision"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "8.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 9227468,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5893429",
    "created": "1642872139"
  },
  {
    "id": 108,
    "canonicalId": "t8vj64zk3fqhor1mscykkcj0",
    "datasetId": "t8vj64zk3fqhor1mscykkcj0",
    "doi": "10.1126/science.abb8598",
    "title": "Gene regulatory networks controlling vertebrate retinal regeneration",
    "description": "<p><strong>For the remaining split tarballs 2,3,5-9, please see below for each:</strong></p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/XPFkPpYk76kVy7qqvGVx6VjF4KfzGYG4GpvZj3x0/NorteDame_data2.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/jP1YK0vF96JVG7Bf2F8B9BZ50QY45zQ4pV02XPg6/NorteDame_data3.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/JXg3Y4BgjFz9q87Kpvf5J7V2J6B6qzKZYfFJk3vK/NorteDame_data5.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/7Vy4fbqy20BXYF4Gj8zk3kp2321q0xBqbgqf9V0g/NorteDame_data6.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/KZ0YXp405XZgv023G6pBzgPjYx8K9Q743XK81PG6/NorteDame_data7.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/xK9VBPYxPF1VbpzFfpPy8zyg76Bgb2xYKfFpqg5p/NorteDame_data8.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/ZYp9v7712Zvp9jZp7k746G6VPqkJ09PvPg3ZZJ69/NorteDame_data9.tar.gz</p>\n\n<p>&nbsp;</p>\n\n<p><strong>See the following visualizations to explore the findings:</strong></p>\n\n<p>&nbsp; *<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-retinal-development-single-cell-rnaseq-analysis~70\">Mouse Retinal Development Single Cell RNAseq analysis</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-nmda-treatment~74\">Retinal Regeneration Single Cell of Mouse retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-nmda-treatment~76\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-light-damage~75\">Retinal Regeneration Single Cell of Mouse Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-light-damage~77\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to light damage</a></p>\n\n<p>&nbsp; *&nbsp;<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-and-zebrafish-retinal-development-rnaseq-with-fpkm~51\">Mouse and Zebrafish retinal development RNAseq with FPKM</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-retina-following-nmdagrowth-factor-gf-insulinfgf-treatment~73\">Retinal Regeneration Single cell of Chick Retina following NMDA/growth factor (GF, insulin+FGF) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-development~78\">Retinal Regeneration Single Cell of Zebrafish Retina development</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-trajectory-of-muller-glia-in-response-to-nmdagf-treatment~83\">Retinal Regeneration Single Cell of Chick Trajectory of Muller glia in response to NMDA/GF treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-light-damage~80\">Retinal Regeneration Single Cell of Zebrafish Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-nmda-treatment~79\">Retinal Regeneration Single Cell of Zebrafish Retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-tnf-and-the-gamma-secretase-inhibitor-ro4929097-tr-treatment~81\">Retinal Regeneration Single Cell of Zebrafish Retina following TNF\u0251 and the gamma secretase inhibitor RO4929097 (T+R) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-trajectory-of-muller-glia-in-response-to-nmda-light-damage-and-tr-treatments~82\">Retinal Regeneration Single Cell of Zebrafish Trajectory of Muller glia in response to NMDA, light damage and T+R treatments</a></p>\n\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>For the remaining split tarballs 2,3,5-9, please see below for each:</strong></p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/XPFkPpYk76kVy7qqvGVx6VjF4KfzGYG4GpvZj3x0/NorteDame_data2.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/jP1YK0vF96JVG7Bf2F8B9BZ50QY45zQ4pV02XPg6/NorteDame_data3.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/JXg3Y4BgjFz9q87Kpvf5J7V2J6B6qzKZYfFJk3vK/NorteDame_data5.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/7Vy4fbqy20BXYF4Gj8zk3kp2321q0xBqbgqf9V0g/NorteDame_data6.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/KZ0YXp405XZgv023G6pBzgPjYx8K9Q743XK81PG6/NorteDame_data7.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/xK9VBPYxPF1VbpzFfpPy8zyg76Bgb2xYKfFpqg5p/NorteDame_data8.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/ZYp9v7712Zvp9jZp7k746G6VPqkJ09PvPg3ZZJ69/NorteDame_data9.tar.gz</p>\n\n<p>&nbsp;</p>\n\n<p><strong>See the following visualizations to explore the findings:</strong></p>\n\n<p>&nbsp; *<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-retinal-development-single-cell-rnaseq-analysis~70\">Mouse Retinal Development Single Cell RNAseq analysis</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-nmda-treatment~74\">Retinal Regeneration Single Cell of Mouse retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-nmda-treatment~76\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-light-damage~75\">Retinal Regeneration Single Cell of Mouse Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-light-damage~77\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to light damage</a></p>\n\n<p>&nbsp; *&nbsp;<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-and-zebrafish-retinal-development-rnaseq-with-fpkm~51\">Mouse and Zebrafish retinal development RNAseq with FPKM</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-retina-following-nmdagrowth-factor-gf-insulinfgf-treatment~73\">Retinal Regeneration Single cell of Chick Retina following NMDA/growth factor (GF, insulin+FGF) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-development~78\">Retinal Regeneration Single Cell of Zebrafish Retina development</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-trajectory-of-muller-glia-in-response-to-nmdagf-treatment~83\">Retinal Regeneration Single Cell of Chick Trajectory of Muller glia in response to NMDA/GF treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-light-damage~80\">Retinal Regeneration Single Cell of Zebrafish Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-nmda-treatment~79\">Retinal Regeneration Single Cell of Zebrafish Retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-tnf-and-the-gamma-secretase-inhibitor-ro4929097-tr-treatment~81\">Retinal Regeneration Single Cell of Zebrafish Retina following TNF\u0251 and the gamma secretase inhibitor RO4929097 (T+R) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-trajectory-of-muller-glia-in-response-to-nmda-light-damage-and-tr-treatments~82\">Retinal Regeneration Single Cell of Zebrafish Trajectory of Muller glia in response to NMDA, light damage and T+R treatments</a></p>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.1126/science.abb8598",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Gene regulatory networks controlling vertebrate retinal regeneration"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "David R Hyde, Seth Blackshaw",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-11-20",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>For the remaining split tarballs 2,3,5-9, please see below for each:</strong></p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/XPFkPpYk76kVy7qqvGVx6VjF4KfzGYG4GpvZj3x0/NorteDame_data2.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/jP1YK0vF96JVG7Bf2F8B9BZ50QY45zQ4pV02XPg6/NorteDame_data3.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/JXg3Y4BgjFz9q87Kpvf5J7V2J6B6qzKZYfFJk3vK/NorteDame_data5.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/7Vy4fbqy20BXYF4Gj8zk3kp2321q0xBqbgqf9V0g/NorteDame_data6.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/KZ0YXp405XZgv023G6pBzgPjYx8K9Q743XK81PG6/NorteDame_data7.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/xK9VBPYxPF1VbpzFfpPy8zyg76Bgb2xYKfFpqg5p/NorteDame_data8.tar.gz</p>\n\n<p>https://westus.dl.azure.dnanex.us/F/D/ZYp9v7712Zvp9jZp7k746G6VPqkJ09PvPg3ZZJ69/NorteDame_data9.tar.gz</p>\n\n<p>&nbsp;</p>\n\n<p><strong>See the following visualizations to explore the findings:</strong></p>\n\n<p>&nbsp; *<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-retinal-development-single-cell-rnaseq-analysis~70\">Mouse Retinal Development Single Cell RNAseq analysis</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-nmda-treatment~74\">Retinal Regeneration Single Cell of Mouse retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-nmda-treatment~76\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-retina-following-light-damage~75\">Retinal Regeneration Single Cell of Mouse Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-mouse-trajectory-of-muller-glia-in-response-to-light-damage~77\">Retinal Regeneration Single Cell of Mouse Trajectory of Muller glia in response to light damage</a></p>\n\n<p>&nbsp; *&nbsp;<a href=\"https://viz.stjude.cloud/hyde-lab/visualization/mouse-and-zebrafish-retinal-development-rnaseq-with-fpkm~51\">Mouse and Zebrafish retinal development RNAseq with FPKM</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-retina-following-nmdagrowth-factor-gf-insulinfgf-treatment~73\">Retinal Regeneration Single cell of Chick Retina following NMDA/growth factor (GF, insulin+FGF) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-development~78\">Retinal Regeneration Single Cell of Zebrafish Retina development</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-chick-trajectory-of-muller-glia-in-response-to-nmdagf-treatment~83\">Retinal Regeneration Single Cell of Chick Trajectory of Muller glia in response to NMDA/GF treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-light-damage~80\">Retinal Regeneration Single Cell of Zebrafish Retina following light damage</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-nmda-treatment~79\">Retinal Regeneration Single Cell of Zebrafish Retina following NMDA treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/colleen-reilly/visualization/retinal-regeneration-single-cell-of-zebrafish-retina-following-tnf-and-the-gamma-secretase-inhibitor-ro4929097-tr-treatment~81\">Retinal Regeneration Single Cell of Zebrafish Retina following TNF\u0251 and the gamma secretase inhibitor RO4929097 (T+R) treatment</a></p>\n\n<p>&nbsp; * <a href=\"https://viz.stjude.cloud/hyde-lab/visualization/retinal-regeneration-single-cell-of-zebrafish-trajectory-of-muller-glia-in-response-to-nmda-light-damage-and-tr-treatments~82\">Retinal Regeneration Single Cell of Zebrafish Trajectory of Muller glia in response to NMDA, light damage and T+R treatments</a></p>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1311.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1375521996,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8102229",
    "created": "1691608071"
  },
  {
    "id": 109,
    "canonicalId": "bejw7y10v1gis0wr3d0a51zg",
    "datasetId": "bejw7y10v1gis0wr3d0a51zg",
    "doi": "10.5281/zenodo.4521044",
    "title": "Topological characterization of the Retinal microvascular nEtwork visualized by portable fuNDus camera (TREND) database",
    "description": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the <strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND</strong>) is a database of digital color eye fundus images created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2020.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 72 color fundus images of healthy young subjects. Each image is associated with a corresponding binarized image of the manually segmented microvascular network. A set of poor quality images is also included. For comparison of the microvascular geometry, an additional set of 10 digital color fundus images of older healthy subjects is included. Each of these images has its corresponding segmented image. In addition, and a set of low quality images captured on healthy older subjects is also included.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion criteria</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>HEALTHY YOUNG</strong></td>\n\t\t\t<td><strong>HEALTHY OLD</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- age between 18 and 25 years</td>\n\t\t\t<td>- age 55 or older</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- no history of <em>any</em> chronic systemic disease requiring medical management</td>\n\t\t\t<td>- negative history of <em>poorly controlled</em> systemic illness (such as diabetes mellitus, coronary artery disease, cerebrovascular disease, or hypertension),</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&nbsp;</td>\n\t\t\t<td>-&nbsp;negative history of cancer, stroke, dementia or other neurologic disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- opacity of clear eye media preventing good quality imaging of the fundus</td>\n\t\t\t<td>\n\t\t\t<p>- opacity of clear eye media preventing good quality imaging of the fundus</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current acute illness</td>\n\t\t\t<td>- current acute illness</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>Files:</p>\n\n<p>1_HEALTHY YOUNG RAW_GOOD QUALITY (72 images in tif format)</p>\n\n<p>2_HEALTHY YOUNG SEGMENTED (72 images in png format)</p>\n\n<p>3_HEALTHY YOUNG RAW _BAD QUALITY (14 images in tif format)</p>\n\n<p>4_HEALTHY OLD RAW_GOOD QUALITY (10 images in tif format)</p>\n\n<p>5_HEALTHY OLD SEGMENTED (10 images in png format)</p>\n\n<p>6_HEALTHY OLD RAW_BAD QUALITY (8 images in tif format)</p>\n\n<p>7_Image quality criteria (xlsx&nbsp;fromat)</p>\n\n<p>8_Associated data&nbsp;(xlsx&nbsp;fromat)</p>\n\n<p>&nbsp;</p>\n\n<p><strong>References</strong></p>\n\n<p>The database can be used freely for research purposes and it is released under Creative Commons 4.0 Attribution License (<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>).</p>\n\n<p>Detailed description of this database is provided in the following article:</p>\n\n<p>Popovic N, Vujosevic S, Radunovi\u0107 M, Radunovi\u0107 M, Popovic T (2021) TREND database: Retinal images of healthy young subjects visualized by a portable digital non-mydriatic fundus camera. PLoS ONE 16(7): e0254918. https://doi.org/ 10.1371/journal.pone.0254918</p>",
    "versionTitle": "1.1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the <strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND</strong>) is a database of digital color eye fundus images created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2020.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 72 color fundus images of healthy young subjects. Each image is associated with a corresponding binarized image of the manually segmented microvascular network. A set of poor quality images is also included. For comparison of the microvascular geometry, an additional set of 10 digital color fundus images of older healthy subjects is included. Each of these images has its corresponding segmented image. In addition, and a set of low quality images captured on healthy older subjects is also included.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion criteria</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>HEALTHY YOUNG</strong></td>\n\t\t\t<td><strong>HEALTHY OLD</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- age between 18 and 25 years</td>\n\t\t\t<td>- age 55 or older</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- no history of <em>any</em> chronic systemic disease requiring medical management</td>\n\t\t\t<td>- negative history of <em>poorly controlled</em> systemic illness (such as diabetes mellitus, coronary artery disease, cerebrovascular disease, or hypertension),</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&nbsp;</td>\n\t\t\t<td>-&nbsp;negative history of cancer, stroke, dementia or other neurologic disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- opacity of clear eye media preventing good quality imaging of the fundus</td>\n\t\t\t<td>\n\t\t\t<p>- opacity of clear eye media preventing good quality imaging of the fundus</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current acute illness</td>\n\t\t\t<td>- current acute illness</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>Files:</p>\n\n<p>1_HEALTHY YOUNG RAW_GOOD QUALITY (72 images in tif format)</p>\n\n<p>2_HEALTHY YOUNG SEGMENTED (72 images in png format)</p>\n\n<p>3_HEALTHY YOUNG RAW _BAD QUALITY (14 images in tif format)</p>\n\n<p>4_HEALTHY OLD RAW_GOOD QUALITY (10 images in tif format)</p>\n\n<p>5_HEALTHY OLD SEGMENTED (10 images in png format)</p>\n\n<p>6_HEALTHY OLD RAW_BAD QUALITY (8 images in tif format)</p>\n\n<p>7_Image quality criteria (xlsx&nbsp;fromat)</p>\n\n<p>8_Associated data&nbsp;(xlsx&nbsp;fromat)</p>\n\n<p>&nbsp;</p>\n\n<p><strong>References</strong></p>\n\n<p>The database can be used freely for research purposes and it is released under Creative Commons 4.0 Attribution License (<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>).</p>\n\n<p>Detailed description of this database is provided in the following article:</p>\n\n<p>Popovic N, Vujosevic S, Radunovi\u0107 M, Radunovi\u0107 M, Popovic T (2021) TREND database: Retinal images of healthy young subjects visualized by a portable digital non-mydriatic fundus camera. PLoS ONE 16(7): e0254918. https://doi.org/ 10.1371/journal.pone.0254918</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.4521044",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Topological characterization of the Retinal microvascular nEtwork visualized by portable fuNDus camera (TREND) database"
          }
        ],
        "version": "1.1",
        "creator": [
          {
            "creatorName": "Popovic, Natasa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro, Podgorica, Montenegro"
              }
            ]
          },
          {
            "creatorName": "Vujosevic, Stela",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Eye Clinic, IRCCS MultiMedica, Milan, Italy"
              }
            ]
          },
          {
            "creatorName": "Radunovic, Miroslav",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro, Podgorica, Montenegro"
              }
            ]
          },
          {
            "creatorName": "Radunovic, Miodrag",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty of Medicine, University of Montenegro, Podgorica, Montenegro"
              }
            ]
          },
          {
            "creatorName": "Popovic, Tomo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Faculty for Information Systems and Technologies, University of Donja Gorica, Podgorica, Montenegro"
              }
            ]
          }
        ],
        "publicationYear": "2021",
        "date": [
          {
            "dateValue": "2021-06-16",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>Introduction</strong></p>\n\n<p><strong>T</strong>opological characterization of the <strong>R</strong>etinal microvascular n<strong>E</strong>twork visualized by portable fu<strong>ND</strong>us camera (<strong>TREND</strong>) is a database of digital color eye fundus images created by medical professionals of the Faculty of Medicine of the University of Montenegro in 2020.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Purpose</strong></p>\n\n<p>1) to provide a standard that defines normal retinal anatomy and microvascular geometry as it appears when visualized by the portable fundus camera</p>\n\n<p>2) to help the development of new methods for stratification of the risk for the development of various eye diseases, as well as systemic diseases that affect microvasculature</p>\n\n<p>3) to aid the development of biomarkers of accelerated aging</p>\n\n<p>4) to provide a standard that can be used to develop software for segmentation of retinal microvasculature, grading the quality of retinal digital images, and computer-aided diagnosis of systemic and chronic diseases.</p>\n\n<p>All color digital images were acquired with a hand-held portable, non-mydriatic MiiS HORUS Scope DEC 200 with 45&ordm; FOV and 2560 X 1920 pixel resolution.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data</strong></p>\n\n<p>The TREND public database contains 72 color fundus images of healthy young subjects. Each image is associated with a corresponding binarized image of the manually segmented microvascular network. A set of poor quality images is also included. For comparison of the microvascular geometry, an additional set of 10 digital color fundus images of older healthy subjects is included. Each of these images has its corresponding segmented image. In addition, and a set of low quality images captured on healthy older subjects is also included.</p>\n\n<table>\n\t<caption>Inclusion and Exclusion criteria</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><strong>HEALTHY YOUNG</strong></td>\n\t\t\t<td><strong>HEALTHY OLD</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t\t<td><strong>Inclusion Criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- age between 18 and 25 years</td>\n\t\t\t<td>- age 55 or older</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- no history of <em>any</em> chronic systemic disease requiring medical management</td>\n\t\t\t<td>- negative history of <em>poorly controlled</em> systemic illness (such as diabetes mellitus, coronary artery disease, cerebrovascular disease, or hypertension),</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&nbsp;</td>\n\t\t\t<td>-&nbsp;negative history of cancer, stroke, dementia or other neurologic disease</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t\t<td><strong>Exclusion criteria</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t\t<td>- presence of high myopia greater than -5 diopters (D)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- opacity of clear eye media preventing good quality imaging of the fundus</td>\n\t\t\t<td>\n\t\t\t<p>- opacity of clear eye media preventing good quality imaging of the fundus</p>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t\t<td>- pathologic changes of retina interfering with the segmentation of retinal microvasculature in the affected area</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current acute illness</td>\n\t\t\t<td>- current acute illness</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t\t<td>- current alcohol or drug abuse</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t\t<td>- uncompensated psychiatric disease</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>Files:</p>\n\n<p>1_HEALTHY YOUNG RAW_GOOD QUALITY (72 images in tif format)</p>\n\n<p>2_HEALTHY YOUNG SEGMENTED (72 images in png format)</p>\n\n<p>3_HEALTHY YOUNG RAW _BAD QUALITY (14 images in tif format)</p>\n\n<p>4_HEALTHY OLD RAW_GOOD QUALITY (10 images in tif format)</p>\n\n<p>5_HEALTHY OLD SEGMENTED (10 images in png format)</p>\n\n<p>6_HEALTHY OLD RAW_BAD QUALITY (8 images in tif format)</p>\n\n<p>7_Image quality criteria (xlsx&nbsp;fromat)</p>\n\n<p>8_Associated data&nbsp;(xlsx&nbsp;fromat)</p>\n\n<p>&nbsp;</p>\n\n<p><strong>References</strong></p>\n\n<p>The database can be used freely for research purposes and it is released under Creative Commons 4.0 Attribution License (<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a>).</p>\n\n<p>Detailed description of this database is provided in the following article:</p>\n\n<p>Popovic N, Vujosevic S, Radunovi\u0107 M, Radunovi\u0107 M, Popovic T (2021) TREND database: Retinal images of healthy young subjects visualized by a portable digital non-mydriatic fundus camera. PLoS ONE 16(7): e0254918. https://doi.org/ 10.1371/journal.pone.0254918</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "healthy retina"
          },
          {
            "subjectValue": "microvascular network"
          },
          {
            "subjectValue": "portable fundus camera"
          },
          {
            "subjectValue": "age"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "361.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 379479654,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4521044",
    "created": "1623832774"
  },
  {
    "id": 110,
    "canonicalId": "ub0niphfhnmyqsqi48db4qa3",
    "datasetId": "ub0niphfhnmyqsqi48db4qa3",
    "doi": "10.5281/zenodo.6325549",
    "title": "RIGA+ Dataset for Unsupervised Domain Adaptation in Medical Image Segmentation",
    "description": "<p>Different from the previous combined multi-domain dataset for unsupervised domain adaptation (UDA) in medical image segmentation, this multi-domain fundus image dataset contains annotations made by&nbsp;the same group of ophthalmologists. Hence the&nbsp;annotator bias&nbsp;among different datasets can be&nbsp;mitigated. Therefore, this dataset can provide a&nbsp;relatively fair benchmark for evaluating UDA methods in fundus image segmentation.</p>\n\n<p>This dataset is based on the RIGA[1] dataset and MESSIDOR[2] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1] and [2].</p>\n\n<p>The six&nbsp;duplicated cases in the&nbsp;RIGA dataset are&nbsp;filtered out according to the&nbsp;<a href=\"https://www.adcis.net/en/third-party/messidor/\">Errata</a>. We also remove the duplicated cases that exist in both the&nbsp;RIGA dataset and the&nbsp;MESSIDOR dataset by hash value matching.</p>\n\n<table align=\"center\">\n\t<caption>Details of the RIGA+ dataset</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Domain</th>\n\t\t\t<th scope=\"col\">Dataset</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Labeled Samples</p>\n\n\t\t\t<p>(Train+Test)</p>\n\t\t\t</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Unlabeled</p>\n\n\t\t\t<p>Samples</p>\n\t\t\t</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>BinRushed</td>\n\t\t\t<td>195 (195+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>Magrabia</td>\n\t\t\t<td>95 (95+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE1</td>\n\t\t\t<td>173 (138+35)</td>\n\t\t\t<td>227</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE2</td>\n\t\t\t<td>148 (118+30)</td>\n\t\t\t<td>238</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE3</td>\n\t\t\t<td>133 (106+27)</td>\n\t\t\t<td>252</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. International Society for Optics and Photonics, 2018, 10579: 105790B.</p>\n\n<p>[2]&nbsp;Decenci&egrave;re E, Zhang X, Cazuguel G, et al. Feedback on a publicly distributed image database: the Messidor database[J]. Image Analysis &amp; Stereology, 2014, 33(3): 231-234.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code>@inproceedings{hu2022domain,\n  title={Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation},\n  author={Shishuai Hu and Zehui Liao and Yong Xia},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  year={2022},\n  organization={Springer}\n}</code></pre>\n\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Different from the previous combined multi-domain dataset for unsupervised domain adaptation (UDA) in medical image segmentation, this multi-domain fundus image dataset contains annotations made by&nbsp;the same group of ophthalmologists. Hence the&nbsp;annotator bias&nbsp;among different datasets can be&nbsp;mitigated. Therefore, this dataset can provide a&nbsp;relatively fair benchmark for evaluating UDA methods in fundus image segmentation.</p>\n\n<p>This dataset is based on the RIGA[1] dataset and MESSIDOR[2] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1] and [2].</p>\n\n<p>The six&nbsp;duplicated cases in the&nbsp;RIGA dataset are&nbsp;filtered out according to the&nbsp;<a href=\"https://www.adcis.net/en/third-party/messidor/\">Errata</a>. We also remove the duplicated cases that exist in both the&nbsp;RIGA dataset and the&nbsp;MESSIDOR dataset by hash value matching.</p>\n\n<table align=\"center\">\n\t<caption>Details of the RIGA+ dataset</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Domain</th>\n\t\t\t<th scope=\"col\">Dataset</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Labeled Samples</p>\n\n\t\t\t<p>(Train+Test)</p>\n\t\t\t</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Unlabeled</p>\n\n\t\t\t<p>Samples</p>\n\t\t\t</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>BinRushed</td>\n\t\t\t<td>195 (195+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>Magrabia</td>\n\t\t\t<td>95 (95+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE1</td>\n\t\t\t<td>173 (138+35)</td>\n\t\t\t<td>227</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE2</td>\n\t\t\t<td>148 (118+30)</td>\n\t\t\t<td>238</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE3</td>\n\t\t\t<td>133 (106+27)</td>\n\t\t\t<td>252</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. International Society for Optics and Photonics, 2018, 10579: 105790B.</p>\n\n<p>[2]&nbsp;Decenci&egrave;re E, Zhang X, Cazuguel G, et al. Feedback on a publicly distributed image database: the Messidor database[J]. Image Analysis &amp; Stereology, 2014, 33(3): 231-234.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code>@inproceedings{hu2022domain,\n  title={Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation},\n  author={Shishuai Hu and Zehui Liao and Yong Xia},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  year={2022},\n  organization={Springer}\n}</code></pre>\n\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6325549",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "RIGA+ Dataset for Unsupervised Domain Adaptation in Medical Image Segmentation"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Hu, Shishuai",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern Polytechnical University"
              }
            ]
          },
          {
            "creatorName": "Liao, Zehui",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern Polytechnical University"
              }
            ]
          },
          {
            "creatorName": "Xia, Yong",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern Polytechnical University"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-03-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Different from the previous combined multi-domain dataset for unsupervised domain adaptation (UDA) in medical image segmentation, this multi-domain fundus image dataset contains annotations made by&nbsp;the same group of ophthalmologists. Hence the&nbsp;annotator bias&nbsp;among different datasets can be&nbsp;mitigated. Therefore, this dataset can provide a&nbsp;relatively fair benchmark for evaluating UDA methods in fundus image segmentation.</p>\n\n<p>This dataset is based on the RIGA[1] dataset and MESSIDOR[2] dataset. We&nbsp;appreciate their&nbsp;efforts&nbsp;devoted by the authors of [1] and [2].</p>\n\n<p>The six&nbsp;duplicated cases in the&nbsp;RIGA dataset are&nbsp;filtered out according to the&nbsp;<a href=\"https://www.adcis.net/en/third-party/messidor/\">Errata</a>. We also remove the duplicated cases that exist in both the&nbsp;RIGA dataset and the&nbsp;MESSIDOR dataset by hash value matching.</p>\n\n<table align=\"center\">\n\t<caption>Details of the RIGA+ dataset</caption>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Domain</th>\n\t\t\t<th scope=\"col\">Dataset</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Labeled Samples</p>\n\n\t\t\t<p>(Train+Test)</p>\n\t\t\t</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Unlabeled</p>\n\n\t\t\t<p>Samples</p>\n\t\t\t</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>BinRushed</td>\n\t\t\t<td>195 (195+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Source</th>\n\t\t\t<td>Magrabia</td>\n\t\t\t<td>95 (95+0)</td>\n\t\t\t<td>0</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE1</td>\n\t\t\t<td>173 (138+35)</td>\n\t\t\t<td>227</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE2</td>\n\t\t\t<td>148 (118+30)</td>\n\t\t\t<td>238</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th scope=\"row\">Target</th>\n\t\t\t<td>MESSIDOR-BASE3</td>\n\t\t\t<td>133 (106+27)</td>\n\t\t\t<td>252</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p>[1]&nbsp;Almazroa A, Alodhayb S, Osman E, et al. Retinal fundus images for glaucoma analysis: the RIGA dataset[C]//Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications. International Society for Optics and Photonics, 2018, 10579: 105790B.</p>\n\n<p>[2]&nbsp;Decenci&egrave;re E, Zhang X, Cazuguel G, et al. Feedback on a publicly distributed image database: the Messidor database[J]. Image Analysis &amp; Stereology, 2014, 33(3): 231-234.</p>\n\n<p>If you find this dataset useful for your research, please consider citing the paper as follows:</p>\n\n<pre><code>@inproceedings{hu2022domain,\n  title={Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation},\n  author={Shishuai Hu and Zehui Liao and Yong Xia},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  year={2022},\n  organization={Springer}\n}</code></pre>\n\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1028.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1078040985,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6325549",
    "created": "1646317014"
  },
  {
    "id": 111,
    "canonicalId": "bvrl6dy4z3ifdq58yu2svime",
    "datasetId": "bvrl6dy4z3ifdq58yu2svime",
    "doi": "10.5061/dryad.rbnzs7hft",
    "title": "Motor recalibration of visual and saccadic maps",
    "description": "<p>How does the brain maintain an accurate visual representation of external space? Movement errors following saccade execution provide sufficient information to recalibrate motor and visual space. Here, we asked whether spatial information for vision and saccades is processed in shared or in separate resources. We used saccade adaptation to modify both saccade amplitudes and visual mislocalization. After saccade adaptation was induced, we compared participants' saccadic and perceptual localization before and after we inserted \"no error\" trials. In these trials, we clamped the post-saccadic error online to the predicted end points of saccades. In separate experiments, we either annulled the retinal or the prediction error. We also varied the number of \"no error\" trials across conditions. In all conditions, we found that saccade adaptation remained undisturbed by the insertion of \"no error\" trials. However, mislocalization decreased as a function of the number of trials in which zero retinal error was displayed. When the prediction error was clamped to zero, no mislocalization was observed at all. The results demonstrate the post-saccadic error is used separately to recalibrate visual and saccadic space.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>How does the brain maintain an accurate visual representation of external space? Movement errors following saccade execution provide sufficient information to recalibrate motor and visual space. Here, we asked whether spatial information for vision and saccades is processed in shared or in separate resources. We used saccade adaptation to modify both saccade amplitudes and visual mislocalization. After saccade adaptation was induced, we compared participants' saccadic and perceptual localization before and after we inserted \"no error\" trials. In these trials, we clamped the post-saccadic error online to the predicted end points of saccades. In separate experiments, we either annulled the retinal or the prediction error. We also varied the number of \"no error\" trials across conditions. In all conditions, we found that saccade adaptation remained undisturbed by the insertion of \"no error\" trials. However, mislocalization decreased as a function of the number of trials in which zero retinal error was displayed. When the prediction error was clamped to zero, no mislocalization was observed at all. The results demonstrate the post-saccadic error is used separately to recalibrate visual and saccadic space.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.rbnzs7hft",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Motor recalibration of visual and saccadic maps"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Tyralla, Sandra",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Heinrich Heine University D\u00fcsseldorf"
              }
            ]
          },
          {
            "creatorName": "Pom\u00e8, Antonella",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Heinrich Heine University D\u00fcsseldorf"
              }
            ]
          },
          {
            "creatorName": "Zimmermann, Eckart",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Heinrich Heine University D\u00fcsseldorf"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-02-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>How does the brain maintain an accurate visual representation of external space? Movement errors following saccade execution provide sufficient information to recalibrate motor and visual space. Here, we asked whether spatial information for vision and saccades is processed in shared or in separate resources. We used saccade adaptation to modify both saccade amplitudes and visual mislocalization. After saccade adaptation was induced, we compared participants' saccadic and perceptual localization before and after we inserted \"no error\" trials. In these trials, we clamped the post-saccadic error online to the predicted end points of saccades. In separate experiments, we either annulled the retinal or the prediction error. We also varied the number of \"no error\" trials across conditions. In all conditions, we found that saccade adaptation remained undisturbed by the insertion of \"no error\" trials. However, mislocalization decreased as a function of the number of trials in which zero retinal error was displayed. When the prediction error was clamped to zero, no mislocalization was observed at all. The results demonstrate the post-saccadic error is used separately to recalibrate visual and saccadic space.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Saccadic adaptation"
          },
          {
            "subjectValue": "visual localization"
          },
          {
            "subjectValue": "independent resources"
          },
          {
            "subjectValue": "shared resources"
          },
          {
            "subjectValue": "retinal error"
          },
          {
            "subjectValue": "prediction error"
          },
          {
            "subjectValue": "Experimental psychology"
          },
          {
            "subjectValue": "Perceptual psychology"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 104857,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7626001",
    "created": "1675961186"
  },
  {
    "id": 112,
    "canonicalId": "sooersuvh89nvwihara2qk25",
    "datasetId": "sooersuvh89nvwihara2qk25",
    "doi": "10.5281/zenodo.7974096",
    "title": "Mammalian animal & human retinal organ culture as pre-clinical model to evaluate oxidative stress and antioxidant intraocular therapeutics",
    "description": "<p>Oxidative stress (OS) is involved in the pathogenesis of retinal neurodegenerative diseases like age-related macular degeneration (AMD) and diabetic retinopathy (DR) and an important target of therapeutic treatments. New therapeutics are tested in vivo despite limits in transferability and ethical concerns. Retina cultures using human tissue can deliver critical information and significantly reduce the number of animal experiments along with increased transferability. We cultured up to 32 retina samples derived from one eye, analyzed models&rsquo; quality, induced OS, and tested efficiency of antioxidative therapeutics. Bovine, porcine, rat, and human retinae were cultured in different experimental settings for 3-14&nbsp;d. OS was induced by high-glucose or hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) and treated by Scutellarin, pigment epithelium-derived factor (PEDF), and/or granulocyte macrophage-colony stimulating factor (GM-CSF). Tissue morphology, cell viability, inflammation, and glutathione level were determined. Retina samples showed only moderate necrosis (23.83&plusmn;5.05 increased to 27.00&plusmn;1.66 AU PI-staining over 14&nbsp;d) after 14 days in culture. OS was successfully induced (reduced ATP content of 288.3&plusmn;59.9 vs. 435.7&plusmn;166.8 nM ATP in controls); antioxidants reduced OS-induced apoptosis (from 124.20&plusmn;51.09 to 60.80&plusmn;319.66 cells/image after Scutellarin-treatment). Enhanced mammalian animal and human retina cultures allow reliable, highly transferable research on OS-triggered age-related diseases and pre-clinical testing during drug development.</p>",
    "versionTitle": "V1.0.MK",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Oxidative stress (OS) is involved in the pathogenesis of retinal neurodegenerative diseases like age-related macular degeneration (AMD) and diabetic retinopathy (DR) and an important target of therapeutic treatments. New therapeutics are tested in vivo despite limits in transferability and ethical concerns. Retina cultures using human tissue can deliver critical information and significantly reduce the number of animal experiments along with increased transferability. We cultured up to 32 retina samples derived from one eye, analyzed models&rsquo; quality, induced OS, and tested efficiency of antioxidative therapeutics. Bovine, porcine, rat, and human retinae were cultured in different experimental settings for 3-14&nbsp;d. OS was induced by high-glucose or hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) and treated by Scutellarin, pigment epithelium-derived factor (PEDF), and/or granulocyte macrophage-colony stimulating factor (GM-CSF). Tissue morphology, cell viability, inflammation, and glutathione level were determined. Retina samples showed only moderate necrosis (23.83&plusmn;5.05 increased to 27.00&plusmn;1.66 AU PI-staining over 14&nbsp;d) after 14 days in culture. OS was successfully induced (reduced ATP content of 288.3&plusmn;59.9 vs. 435.7&plusmn;166.8 nM ATP in controls); antioxidants reduced OS-induced apoptosis (from 124.20&plusmn;51.09 to 60.80&plusmn;319.66 cells/image after Scutellarin-treatment). Enhanced mammalian animal and human retina cultures allow reliable, highly transferable research on OS-triggered age-related diseases and pre-clinical testing during drug development.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7974096",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Mammalian animal & human retinal organ culture as pre-clinical model to evaluate oxidative stress and antioxidant intraocular therapeutics"
          }
        ],
        "version": "V1.0.MK",
        "creator": [
          {
            "creatorName": "Martina Kropp",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Mohit Mohit",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Cristina Ioana Leroy-Ciocanea",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "H\u00f4pital Priv\u00e9 La Louvi\u00e8re & Cabinet Ophtalmologie S\u00e9bastopol, Lille, France"
              }
            ]
          },
          {
            "creatorName": "Laura Schwerm",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, University Hospital Rheinisch-Westf\u00e4lische Technische Hochschule (RWTH) Aachen, Aachen, Germany"
              }
            ]
          },
          {
            "creatorName": "Nina Harmening",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Thais Bascuas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Eline De Clerck",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Andreas J. Kreis",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          },
          {
            "creatorName": "Bojan Pajic",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland; Eye Clinic Orasis, Reinach; Dept. of Physics, University of Novi Sad, Serbia; Faculty of Medicine of the Military Medical Academy, University of Defense, Belgrade, Serbia"
              }
            ]
          },
          {
            "creatorName": "Sandra Johnen",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Ophthalmology, University Hospital Rheinisch-Westf\u00e4lische Technische Hochschule (RWTH) Aachen, Aachen, Germany"
              }
            ]
          },
          {
            "creatorName": "Gabriele Thumann",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Geneva & University Hospitals of Geneva, Switzerland"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-06-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Oxidative stress (OS) is involved in the pathogenesis of retinal neurodegenerative diseases like age-related macular degeneration (AMD) and diabetic retinopathy (DR) and an important target of therapeutic treatments. New therapeutics are tested in vivo despite limits in transferability and ethical concerns. Retina cultures using human tissue can deliver critical information and significantly reduce the number of animal experiments along with increased transferability. We cultured up to 32 retina samples derived from one eye, analyzed models&rsquo; quality, induced OS, and tested efficiency of antioxidative therapeutics. Bovine, porcine, rat, and human retinae were cultured in different experimental settings for 3-14&nbsp;d. OS was induced by high-glucose or hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) and treated by Scutellarin, pigment epithelium-derived factor (PEDF), and/or granulocyte macrophage-colony stimulating factor (GM-CSF). Tissue morphology, cell viability, inflammation, and glutathione level were determined. Retina samples showed only moderate necrosis (23.83&plusmn;5.05 increased to 27.00&plusmn;1.66 AU PI-staining over 14&nbsp;d) after 14 days in culture. OS was successfully induced (reduced ATP content of 288.3&plusmn;59.9 vs. 435.7&plusmn;166.8 nM ATP in controls); antioxidants reduced OS-induced apoptosis (from 124.20&plusmn;51.09 to 60.80&plusmn;319.66 cells/image after Scutellarin-treatment). Enhanced mammalian animal and human retina cultures allow reliable, highly transferable research on OS-triggered age-related diseases and pre-clinical testing during drug development.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "retina organ culture"
          },
          {
            "subjectValue": "neuroretinal degenerative disease"
          },
          {
            "subjectValue": "age-related macular degeneration (AMD)"
          },
          {
            "subjectValue": "diabetic retinopathy (DR)"
          },
          {
            "subjectValue": "oxidative stress"
          },
          {
            "subjectValue": "antioxidant"
          },
          {
            "subjectValue": "Scutellarin"
          },
          {
            "subjectValue": "PEDF"
          },
          {
            "subjectValue": "GM-CSF"
          },
          {
            "subjectValue": "4R"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "5831.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 6115190374,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7974096",
    "created": "1688037846"
  },
  {
    "id": 113,
    "canonicalId": "esrgqyiq7t4171r70din1zk9",
    "datasetId": "esrgqyiq7t4171r70din1zk9",
    "doi": "10.5281/zenodo.6476639",
    "title": "UTHealth - Fundus and Synthetic OCT-A Dataset (UT-FSOCTA)",
    "description": "<p><strong>Introduction</strong></p>\n\n<p>Vessel segmentation in fundus images is essential in the diagnosis and prognosis of retinal diseases and the identification of image-based biomarkers. However, creating a vessel segmentation map can be a tedious and time consuming process, requiring careful delineation of the vasculature, which is especially hard for microcapillary plexi in fundus images. Optical coherence tomography angiography (OCT-A) is a relatively novel modality visualizing blood flow and microcapillary plexi not clearly observed in fundus photography. Unfortunately, current commercial OCT-A cameras have various limitations due to their complex optics making them more expensive, less portable, and with a reduced field of view (FOV) compared to fundus cameras. Moreover, the vast majority of population health data collection efforts do not include OCT-A data.</p>\n\n<p>We believe that strategies able to map fundus images to en-face OCT-A can create precise vascular vessel segmentation with less effort.</p>\n\n<p>In this dataset, called UTHealth - Fundus and Synthetic OCT-A Dataset (UT-FSOCTA), we include fundus images and en-face OCT-A images for 112 subjects. The two modalities have been manually aligned to allow for training of medical imaging machine learning pipelines. This dataset is accompanied by a manuscript that describes an approach to generate fundus vessel segmentations using OCT-A for training (Coronado et al., 2022). We refer to this approach as &quot;Synthetic OCT-A&quot;.</p>\n\n<p><strong>Fundus Imaging</strong></p>\n\n<p>We include 45 degree macula centered fundus images that cover both macula and optic disc. All images were acquired using a OptoVue iVue fundus camera without pupil dilation.</p>\n\n<p>The full images are available at the <code>fov45/fundus</code> directory. In addition, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/fundus/disc</code> and <code>cropped/fundus/macula</code>.</p>\n\n<p><strong>Enface OCT-A</strong></p>\n\n<p>We include the en-face OCT-A images of the superficial capillary plexus. All images were acquired using an OptoVue Avanti OCT camera with OCT-A reconstruction software (AngioVue). Low quality images with errors in the retina layer segmentations were not included.</p>\n\n<p>En-face OCTA images are located in <code>cropped/octa/disc</code> and <code>cropped/octa/macula</code>. In addition, we include a denoised version of these images where only vessels are included. This has been performed automatically using the ROSE algorithm (Ma et al. 2021). These can be found in <code>cropped/GT_OCT_net/noThresh</code> and <code>cropped/GT_OCT_net/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Synthetic OCT-A</strong></p>\n\n<p>We train a custom conditional generative adversarial network (cGAN) to map a fundus image to an en face OCT-A image. Our model consists of a generator synthesizing en face OCT-A images from corresponding areas in fundus photographs and a discriminator judging the resemblance of the synthesized images to the real en face OCT-A samples. This allows us to avoid the use of manual vessel segmentation maps altogether.</p>\n\n<p>The full images are available at the <code>fov45/synthetic_octa</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/synthetic_octa/disc</code> and <code>cropped/synthetic_octa/macula</code>. In addition, we performed the same denoising ROSE algorithm (Ma et al. 2021) used for the original enface OCT-A images, the results are available in <code>cropped/denoised_synthetic_octa/noThresh</code> and <code>cropped/denoised_synthetic_octa/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Other Fundus Vessel Segmentations Included</strong></p>\n\n<p>In this dataset, we have also included the output of two recent vessel segmentation algorithms trained on external datasets with manual vessel segmentations. SA-Unet (Li et. al, 2020) and IterNet (Guo et. al, 2021).</p>\n\n<ul>\n\t<li>\n\t<p>SA-Unet. The full images are available at the <code>fov45/SA_Unet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/SA_Unet/disc</code> and <code>cropped/SA_Unet/macula</code>.</p>\n\t</li>\n\t<li>\n\t<p>IterNet. The full images are available at the <code>fov45/Iternet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/Iternet/disc</code> and <code>cropped/Iternet/macula</code>.</p>\n\t</li>\n</ul>\n\n<p><strong>Train/Validation/Test Replication</strong></p>\n\n<p>In order to replicate or compare your model to the results of our paper, we report below the data split used.</p>\n\n<ul>\n\t<li>\n\t<p>Training subjects IDs: 1 - 25</p>\n\t</li>\n\t<li>\n\t<p>Validation subjects IDs: 26 - 30</p>\n\t</li>\n\t<li>\n\t<p>Testing subjects IDs: 31 - 112</p>\n\t</li>\n</ul>\n\n<p><strong>Data Acquisition</strong></p>\n\n<p>This dataset was acquired at the Texas Medical Center - Memorial Hermann Hospital in accordance with the guidelines from the Helsinki Declaration and it was approved by the UTHealth IRB with protocol HSC-MS-19-0352.</p>\n\n<p><strong>User Agreement</strong></p>\n\n<p>The UT-FSOCTA dataset is free to use for non-commercial scientific research only. In case of any publication the following paper needs to be cited</p>\n\n<pre><code>\nCoronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n</code></pre>\n\n<p><strong>Funding</strong></p>\n\n<p>This work is supported by the Translational Research Institute for Space Health through NASA Cooperative Agreement NNX16AO69A.</p>\n\n<p><strong>Research Team and Acknowledgements</strong></p>\n\n<p>Here are the people behind this data acquisition effort:</p>\n\n<p>Ivan Coronado, Samiksha Pachade, Rania Abdelkhaleq, Juntao Yan, Sergio Salazar-Marioni, Amanda Jagolino, Mozhdeh Bahrainian, Roomasa Channa, Sunil Sheth, Luca Giancardo</p>\n\n<p>We would also like to acknowledge for their support: the Institute for Stroke and Cerebrovascular Diseases at UTHealth, the VAMPIRE team at University of Dundee, UK and Memorial Hermann Hospital System.</p>\n\n<p><strong>References</strong></p>\n\n<pre><code>Coronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n\n\nC. Guo, M. Szemenyei, Y. Yi, W. Wang, B. Chen, and C. Fan, \"SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation,\" in 2020 25th International Conference on Pattern Recognition (ICPR), Jan. 2021, pp. 1236\u20131242. doi: 10.1109/ICPR48806.2021.9413346.\n\nL. Li, M. Verma, Y. Nakashima, H. Nagahara, and R. Kawasaki, \"IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks,\" 2020 IEEE Winter Conf. Appl. Comput. Vis. WACV, 2020, doi: 10.1109/WACV45572.2020.9093621.\n\nY. Ma et al., \"ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model,\" IEEE Trans. Med. Imaging, vol. 40, no. 3, pp. 928\u2013939, Mar. 2021, doi: 10.1109/TMI.2020.3042802.\n</code></pre>",
    "versionTitle": "1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p><strong>Introduction</strong></p>\n\n<p>Vessel segmentation in fundus images is essential in the diagnosis and prognosis of retinal diseases and the identification of image-based biomarkers. However, creating a vessel segmentation map can be a tedious and time consuming process, requiring careful delineation of the vasculature, which is especially hard for microcapillary plexi in fundus images. Optical coherence tomography angiography (OCT-A) is a relatively novel modality visualizing blood flow and microcapillary plexi not clearly observed in fundus photography. Unfortunately, current commercial OCT-A cameras have various limitations due to their complex optics making them more expensive, less portable, and with a reduced field of view (FOV) compared to fundus cameras. Moreover, the vast majority of population health data collection efforts do not include OCT-A data.</p>\n\n<p>We believe that strategies able to map fundus images to en-face OCT-A can create precise vascular vessel segmentation with less effort.</p>\n\n<p>In this dataset, called UTHealth - Fundus and Synthetic OCT-A Dataset (UT-FSOCTA), we include fundus images and en-face OCT-A images for 112 subjects. The two modalities have been manually aligned to allow for training of medical imaging machine learning pipelines. This dataset is accompanied by a manuscript that describes an approach to generate fundus vessel segmentations using OCT-A for training (Coronado et al., 2022). We refer to this approach as &quot;Synthetic OCT-A&quot;.</p>\n\n<p><strong>Fundus Imaging</strong></p>\n\n<p>We include 45 degree macula centered fundus images that cover both macula and optic disc. All images were acquired using a OptoVue iVue fundus camera without pupil dilation.</p>\n\n<p>The full images are available at the <code>fov45/fundus</code> directory. In addition, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/fundus/disc</code> and <code>cropped/fundus/macula</code>.</p>\n\n<p><strong>Enface OCT-A</strong></p>\n\n<p>We include the en-face OCT-A images of the superficial capillary plexus. All images were acquired using an OptoVue Avanti OCT camera with OCT-A reconstruction software (AngioVue). Low quality images with errors in the retina layer segmentations were not included.</p>\n\n<p>En-face OCTA images are located in <code>cropped/octa/disc</code> and <code>cropped/octa/macula</code>. In addition, we include a denoised version of these images where only vessels are included. This has been performed automatically using the ROSE algorithm (Ma et al. 2021). These can be found in <code>cropped/GT_OCT_net/noThresh</code> and <code>cropped/GT_OCT_net/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Synthetic OCT-A</strong></p>\n\n<p>We train a custom conditional generative adversarial network (cGAN) to map a fundus image to an en face OCT-A image. Our model consists of a generator synthesizing en face OCT-A images from corresponding areas in fundus photographs and a discriminator judging the resemblance of the synthesized images to the real en face OCT-A samples. This allows us to avoid the use of manual vessel segmentation maps altogether.</p>\n\n<p>The full images are available at the <code>fov45/synthetic_octa</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/synthetic_octa/disc</code> and <code>cropped/synthetic_octa/macula</code>. In addition, we performed the same denoising ROSE algorithm (Ma et al. 2021) used for the original enface OCT-A images, the results are available in <code>cropped/denoised_synthetic_octa/noThresh</code> and <code>cropped/denoised_synthetic_octa/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Other Fundus Vessel Segmentations Included</strong></p>\n\n<p>In this dataset, we have also included the output of two recent vessel segmentation algorithms trained on external datasets with manual vessel segmentations. SA-Unet (Li et. al, 2020) and IterNet (Guo et. al, 2021).</p>\n\n<ul>\n\t<li>\n\t<p>SA-Unet. The full images are available at the <code>fov45/SA_Unet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/SA_Unet/disc</code> and <code>cropped/SA_Unet/macula</code>.</p>\n\t</li>\n\t<li>\n\t<p>IterNet. The full images are available at the <code>fov45/Iternet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/Iternet/disc</code> and <code>cropped/Iternet/macula</code>.</p>\n\t</li>\n</ul>\n\n<p><strong>Train/Validation/Test Replication</strong></p>\n\n<p>In order to replicate or compare your model to the results of our paper, we report below the data split used.</p>\n\n<ul>\n\t<li>\n\t<p>Training subjects IDs: 1 - 25</p>\n\t</li>\n\t<li>\n\t<p>Validation subjects IDs: 26 - 30</p>\n\t</li>\n\t<li>\n\t<p>Testing subjects IDs: 31 - 112</p>\n\t</li>\n</ul>\n\n<p><strong>Data Acquisition</strong></p>\n\n<p>This dataset was acquired at the Texas Medical Center - Memorial Hermann Hospital in accordance with the guidelines from the Helsinki Declaration and it was approved by the UTHealth IRB with protocol HSC-MS-19-0352.</p>\n\n<p><strong>User Agreement</strong></p>\n\n<p>The UT-FSOCTA dataset is free to use for non-commercial scientific research only. In case of any publication the following paper needs to be cited</p>\n\n<pre><code>\nCoronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n</code></pre>\n\n<p><strong>Funding</strong></p>\n\n<p>This work is supported by the Translational Research Institute for Space Health through NASA Cooperative Agreement NNX16AO69A.</p>\n\n<p><strong>Research Team and Acknowledgements</strong></p>\n\n<p>Here are the people behind this data acquisition effort:</p>\n\n<p>Ivan Coronado, Samiksha Pachade, Rania Abdelkhaleq, Juntao Yan, Sergio Salazar-Marioni, Amanda Jagolino, Mozhdeh Bahrainian, Roomasa Channa, Sunil Sheth, Luca Giancardo</p>\n\n<p>We would also like to acknowledge for their support: the Institute for Stroke and Cerebrovascular Diseases at UTHealth, the VAMPIRE team at University of Dundee, UK and Memorial Hermann Hospital System.</p>\n\n<p><strong>References</strong></p>\n\n<pre><code>Coronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n\n\nC. Guo, M. Szemenyei, Y. Yi, W. Wang, B. Chen, and C. Fan, \"SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation,\" in 2020 25th International Conference on Pattern Recognition (ICPR), Jan. 2021, pp. 1236\u20131242. doi: 10.1109/ICPR48806.2021.9413346.\n\nL. Li, M. Verma, Y. Nakashima, H. Nagahara, and R. Kawasaki, \"IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks,\" 2020 IEEE Winter Conf. Appl. Comput. Vis. WACV, 2020, doi: 10.1109/WACV45572.2020.9093621.\n\nY. Ma et al., \"ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model,\" IEEE Trans. Med. Imaging, vol. 40, no. 3, pp. 928\u2013939, Mar. 2021, doi: 10.1109/TMI.2020.3042802.\n</code></pre>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6476639",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "UTHealth - Fundus and Synthetic OCT-A Dataset (UT-FSOCTA)"
          }
        ],
        "version": "1.0",
        "creator": [
          {
            "creatorName": "Coronado, Ivan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Pachade, Samiksha",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Abdelkhaleq, Rania",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Yan, Juntao",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Salazar-Marioni, Sergio",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Jagolino, Amanda",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Bahrainian, Mozhdeh",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsis-Madison"
              }
            ]
          },
          {
            "creatorName": "Channa, Roomasa",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsis-Madison"
              }
            ]
          },
          {
            "creatorName": "Sheth, Sunil",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          },
          {
            "creatorName": "Giancardo, Luca",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Texas Health Science Center at Houston"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-04-21",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p><strong>Introduction</strong></p>\n\n<p>Vessel segmentation in fundus images is essential in the diagnosis and prognosis of retinal diseases and the identification of image-based biomarkers. However, creating a vessel segmentation map can be a tedious and time consuming process, requiring careful delineation of the vasculature, which is especially hard for microcapillary plexi in fundus images. Optical coherence tomography angiography (OCT-A) is a relatively novel modality visualizing blood flow and microcapillary plexi not clearly observed in fundus photography. Unfortunately, current commercial OCT-A cameras have various limitations due to their complex optics making them more expensive, less portable, and with a reduced field of view (FOV) compared to fundus cameras. Moreover, the vast majority of population health data collection efforts do not include OCT-A data.</p>\n\n<p>We believe that strategies able to map fundus images to en-face OCT-A can create precise vascular vessel segmentation with less effort.</p>\n\n<p>In this dataset, called UTHealth - Fundus and Synthetic OCT-A Dataset (UT-FSOCTA), we include fundus images and en-face OCT-A images for 112 subjects. The two modalities have been manually aligned to allow for training of medical imaging machine learning pipelines. This dataset is accompanied by a manuscript that describes an approach to generate fundus vessel segmentations using OCT-A for training (Coronado et al., 2022). We refer to this approach as &quot;Synthetic OCT-A&quot;.</p>\n\n<p><strong>Fundus Imaging</strong></p>\n\n<p>We include 45 degree macula centered fundus images that cover both macula and optic disc. All images were acquired using a OptoVue iVue fundus camera without pupil dilation.</p>\n\n<p>The full images are available at the <code>fov45/fundus</code> directory. In addition, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/fundus/disc</code> and <code>cropped/fundus/macula</code>.</p>\n\n<p><strong>Enface OCT-A</strong></p>\n\n<p>We include the en-face OCT-A images of the superficial capillary plexus. All images were acquired using an OptoVue Avanti OCT camera with OCT-A reconstruction software (AngioVue). Low quality images with errors in the retina layer segmentations were not included.</p>\n\n<p>En-face OCTA images are located in <code>cropped/octa/disc</code> and <code>cropped/octa/macula</code>. In addition, we include a denoised version of these images where only vessels are included. This has been performed automatically using the ROSE algorithm (Ma et al. 2021). These can be found in <code>cropped/GT_OCT_net/noThresh</code> and <code>cropped/GT_OCT_net/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Synthetic OCT-A</strong></p>\n\n<p>We train a custom conditional generative adversarial network (cGAN) to map a fundus image to an en face OCT-A image. Our model consists of a generator synthesizing en face OCT-A images from corresponding areas in fundus photographs and a discriminator judging the resemblance of the synthesized images to the real en face OCT-A samples. This allows us to avoid the use of manual vessel segmentation maps altogether.</p>\n\n<p>The full images are available at the <code>fov45/synthetic_octa</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/synthetic_octa/disc</code> and <code>cropped/synthetic_octa/macula</code>. In addition, we performed the same denoising ROSE algorithm (Ma et al. 2021) used for the original enface OCT-A images, the results are available in <code>cropped/denoised_synthetic_octa/noThresh</code> and <code>cropped/denoised_synthetic_octa/Thresh</code>, the former contains the probabilities of the ROSE algorithm the latter a binary map.</p>\n\n<p><strong>Other Fundus Vessel Segmentations Included</strong></p>\n\n<p>In this dataset, we have also included the output of two recent vessel segmentation algorithms trained on external datasets with manual vessel segmentations. SA-Unet (Li et. al, 2020) and IterNet (Guo et. al, 2021).</p>\n\n<ul>\n\t<li>\n\t<p>SA-Unet. The full images are available at the <code>fov45/SA_Unet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/SA_Unet/disc</code> and <code>cropped/SA_Unet/macula</code>.</p>\n\t</li>\n\t<li>\n\t<p>IterNet. The full images are available at the <code>fov45/Iternet</code> directory. Then, we extracted the FOVs corresponding to the en-face OCT-A images collected in <code>cropped/Iternet/disc</code> and <code>cropped/Iternet/macula</code>.</p>\n\t</li>\n</ul>\n\n<p><strong>Train/Validation/Test Replication</strong></p>\n\n<p>In order to replicate or compare your model to the results of our paper, we report below the data split used.</p>\n\n<ul>\n\t<li>\n\t<p>Training subjects IDs: 1 - 25</p>\n\t</li>\n\t<li>\n\t<p>Validation subjects IDs: 26 - 30</p>\n\t</li>\n\t<li>\n\t<p>Testing subjects IDs: 31 - 112</p>\n\t</li>\n</ul>\n\n<p><strong>Data Acquisition</strong></p>\n\n<p>This dataset was acquired at the Texas Medical Center - Memorial Hermann Hospital in accordance with the guidelines from the Helsinki Declaration and it was approved by the UTHealth IRB with protocol HSC-MS-19-0352.</p>\n\n<p><strong>User Agreement</strong></p>\n\n<p>The UT-FSOCTA dataset is free to use for non-commercial scientific research only. In case of any publication the following paper needs to be cited</p>\n\n<pre><code>\nCoronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n</code></pre>\n\n<p><strong>Funding</strong></p>\n\n<p>This work is supported by the Translational Research Institute for Space Health through NASA Cooperative Agreement NNX16AO69A.</p>\n\n<p><strong>Research Team and Acknowledgements</strong></p>\n\n<p>Here are the people behind this data acquisition effort:</p>\n\n<p>Ivan Coronado, Samiksha Pachade, Rania Abdelkhaleq, Juntao Yan, Sergio Salazar-Marioni, Amanda Jagolino, Mozhdeh Bahrainian, Roomasa Channa, Sunil Sheth, Luca Giancardo</p>\n\n<p>We would also like to acknowledge for their support: the Institute for Stroke and Cerebrovascular Diseases at UTHealth, the VAMPIRE team at University of Dundee, UK and Memorial Hermann Hospital System.</p>\n\n<p><strong>References</strong></p>\n\n<pre><code>Coronado I, Pachade S, Trucco E, Abdelkhaleq R, Yan J, Salazar-Marioni S, Jagolino-Cole A, Bahrainian M, Channa R, Sheth SA, Giancardo L. Synthetic OCT-A blood vessel maps using fundus images and generative adversarial networks. Sci Rep 2023;13:15325. https://doi.org/10.1038/s41598-023-42062-9.\n\n\nC. Guo, M. Szemenyei, Y. Yi, W. Wang, B. Chen, and C. Fan, \"SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation,\" in 2020 25th International Conference on Pattern Recognition (ICPR), Jan. 2021, pp. 1236\u20131242. doi: 10.1109/ICPR48806.2021.9413346.\n\nL. Li, M. Verma, Y. Nakashima, H. Nagahara, and R. Kawasaki, \"IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks,\" 2020 IEEE Winter Conf. Appl. Comput. Vis. WACV, 2020, doi: 10.1109/WACV45572.2020.9093621.\n\nY. Ma et al., \"ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model,\" IEEE Trans. Med. Imaging, vol. 40, no. 3, pp. 928\u2013939, Mar. 2021, doi: 10.1109/TMI.2020.3042802.\n</code></pre>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1098.1 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1151441305,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6476639",
    "created": "1650990011"
  },
  {
    "id": 114,
    "canonicalId": "pvdmwifa4bp1vdweqn8povt1",
    "datasetId": "pvdmwifa4bp1vdweqn8povt1",
    "doi": "10.5281/zenodo.8289533",
    "title": "2023 IEEE SPS Video and Image Processing (VIP) Cup: Ophthalmic Biomarker Detection Phase 2",
    "description": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been&nbsp;<em>generalization</em>&nbsp;and&nbsp;<em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>\n\n<p>&nbsp;</p>\n\n<p>These files constitute the second phase of the VIP CUP 2023 Challenge at ICIP 2023. This test set has a more general patient base than the first one and as such is a better indicator of the performance of models. This test set was created by taking a subset of the data from a publicly available OCT dataset and then asking our medical partners to provide fine-grained biomarker labels for the competition. We provide the citation for the source of these images below:&nbsp;</p>\n\n<p>Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010.</p>\n\n<p>&nbsp;</p>\n\n<p>This zenodo repository contains the images and submission template file needed for the second phase of the competition.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been&nbsp;<em>generalization</em>&nbsp;and&nbsp;<em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>\n\n<p>&nbsp;</p>\n\n<p>These files constitute the second phase of the VIP CUP 2023 Challenge at ICIP 2023. This test set has a more general patient base than the first one and as such is a better indicator of the performance of models. This test set was created by taking a subset of the data from a publicly available OCT dataset and then asking our medical partners to provide fine-grained biomarker labels for the competition. We provide the citation for the source of these images below:&nbsp;</p>\n\n<p>Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010.</p>\n\n<p>&nbsp;</p>\n\n<p>This zenodo repository contains the images and submission template file needed for the second phase of the competition.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.8289533",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "2023 IEEE SPS Video and Image Processing (VIP) Cup: Ophthalmic Biomarker Detection Phase 2"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ghassan AlRegib",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Mohit Prabhushankar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Prithwijit  Chowdhury",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Zoe Fowler",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          },
          {
            "creatorName": "Kiran Kokilepersaud",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Georgia Institute of Technology"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-08-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Ophthalmic clinical trials that study treatment efficacy of eye diseases are performed with a specific purpose and a set of procedures that are predetermined before trial initiation. Hence, they result in a controlled data collection process with gradual changes in the state of a diseased eye. In general, these data include 1D clinical measurements and 3D optical coherence tomography (OCT) imagery. Physicians interpret structural biomarkers for every patient using the 3D OCT images and clinical measurements to make personalized decisions for every patient.</p>\n\n<p>Two main challenges in medical image processing has been&nbsp;<em>generalization</em>&nbsp;and&nbsp;<em>personalization</em>.</p>\n\n<p>Generalization aims to develop algorithms that work well across diverse patients and scenarios, providing standardized and widely applicable solutions. Personalization, in contrast, tailors algorithms to individual patients based on their unique characteristics, optimizing diagnosis and treatment planning. Generalization offers broad applicability but may overlook individual variations. Personalization provides tailored solutions but requires patient-specific data. While deep learning has shown an affinity towards generalization, it is lacking in personalization.</p>\n\n<p>The presence and absence of biomarkers is a personalization challenge rather than a generalization challenge. The variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Morphological, texture, statistical and fuzzy image processing techniques through adaptive thresholds and preprocessing may prove substantial to overcome these fine-grained challenges. This challenge provides the data and application to address personalization.</p>\n\n<p>&nbsp;</p>\n\n<p>These files constitute the second phase of the VIP CUP 2023 Challenge at ICIP 2023. This test set has a more general patient base than the first one and as such is a better indicator of the performance of models. This test set was created by taking a subset of the data from a publicly available OCT dataset and then asking our medical partners to provide fine-grained biomarker labels for the competition. We provide the citation for the source of these images below:&nbsp;</p>\n\n<p>Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010.</p>\n\n<p>&nbsp;</p>\n\n<p>This zenodo repository contains the images and submission template file needed for the second phase of the competition.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "20.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 20971520,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8289533",
    "created": "1693225581"
  },
  {
    "id": 115,
    "canonicalId": "nkw51hibgsln08geoc1tu6tz",
    "datasetId": "nkw51hibgsln08geoc1tu6tz",
    "doi": "10.5061/dryad.1c59zw3sc",
    "title": "Data from: Insights in the diagnosis of a retinal arteriovenous malformation",
    "description": "<p class=\"HoofdtekstA\"><b>Objective: </b>To describe a patient with vitreous hemorrhage due to peripheral retinal ischemia and neovascularization, who was diagnosed with an underlying retinal arteriovenous malformation.</p>\n\n<p class=\"HoofdtekstA\"><b>Methods:</b> A 15-year old girl presented with sudden-onset painless visual loss in the right eye. She underwent a full ophthalmologic work-up.</p>\n\n<p><b>Results: </b>BCVA was less than 20/400 in the right eye, 20/20 in the left eye. Intraocular pressure and anterior segment examination were unremarkable. Fundoscopy revealed an impenetrable vitreous hemorrhage in the right eye. The left eye was completely unremarkable. Examination during a 23-gauge pars plana vitrectomy showed dilated, tortuous arteriovenous vessels extending from the optic disc and peripheral neovascularization. A clinical diagnosis of retinal arteriovenous malformation was made. During surgery, a peripheral retinal photocoagulation was completed to avoid rebleeding. During the post-operative period, a fluorescein angiography demonstrated additional macular microangiopathy and diffuse retinal nonperfusion in the periphery. MRI brain revealed no cerebral or orbital vascular anomaly, confirming a retinal arteriovenous malformation group 1.</p>\n\n<p class=\"HoofdtekstA\"><b>Conclusion: </b>Retinal arteriovenous malformations are thought of to be stable over time. However complications such as ischemia, neovascularization and vitreous hemorrhage might occur. Observation is warranted.\u00a0 As such, timely treatment can be offered to avoid complications.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p class=\"HoofdtekstA\"><b>Objective: </b>To describe a patient with vitreous hemorrhage due to peripheral retinal ischemia and neovascularization, who was diagnosed with an underlying retinal arteriovenous malformation.</p>\n\n<p class=\"HoofdtekstA\"><b>Methods:</b> A 15-year old girl presented with sudden-onset painless visual loss in the right eye. She underwent a full ophthalmologic work-up.</p>\n\n<p><b>Results: </b>BCVA was less than 20/400 in the right eye, 20/20 in the left eye. Intraocular pressure and anterior segment examination were unremarkable. Fundoscopy revealed an impenetrable vitreous hemorrhage in the right eye. The left eye was completely unremarkable. Examination during a 23-gauge pars plana vitrectomy showed dilated, tortuous arteriovenous vessels extending from the optic disc and peripheral neovascularization. A clinical diagnosis of retinal arteriovenous malformation was made. During surgery, a peripheral retinal photocoagulation was completed to avoid rebleeding. During the post-operative period, a fluorescein angiography demonstrated additional macular microangiopathy and diffuse retinal nonperfusion in the periphery. MRI brain revealed no cerebral or orbital vascular anomaly, confirming a retinal arteriovenous malformation group 1.</p>\n\n<p class=\"HoofdtekstA\"><b>Conclusion: </b>Retinal arteriovenous malformations are thought of to be stable over time. However complications such as ischemia, neovascularization and vitreous hemorrhage might occur. Observation is warranted.\u00a0 As such, timely treatment can be offered to avoid complications.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.1c59zw3sc",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Insights in the diagnosis of a retinal arteriovenous malformation"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Accou, Geraldine",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ghent University Hospital"
              }
            ]
          },
          {
            "creatorName": "Nerinckx, Fanny",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ghent University Hospital"
              }
            ]
          },
          {
            "creatorName": "De Zaeytijd, Julie",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Ghent University Hospital"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-05-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p class=\"HoofdtekstA\"><b>Objective: </b>To describe a patient with vitreous hemorrhage due to peripheral retinal ischemia and neovascularization, who was diagnosed with an underlying retinal arteriovenous malformation.</p>\n\n<p class=\"HoofdtekstA\"><b>Methods:</b> A 15-year old girl presented with sudden-onset painless visual loss in the right eye. She underwent a full ophthalmologic work-up.</p>\n\n<p><b>Results: </b>BCVA was less than 20/400 in the right eye, 20/20 in the left eye. Intraocular pressure and anterior segment examination were unremarkable. Fundoscopy revealed an impenetrable vitreous hemorrhage in the right eye. The left eye was completely unremarkable. Examination during a 23-gauge pars plana vitrectomy showed dilated, tortuous arteriovenous vessels extending from the optic disc and peripheral neovascularization. A clinical diagnosis of retinal arteriovenous malformation was made. During surgery, a peripheral retinal photocoagulation was completed to avoid rebleeding. During the post-operative period, a fluorescein angiography demonstrated additional macular microangiopathy and diffuse retinal nonperfusion in the periphery. MRI brain revealed no cerebral or orbital vascular anomaly, confirming a retinal arteriovenous malformation group 1.</p>\n\n<p class=\"HoofdtekstA\"><b>Conclusion: </b>Retinal arteriovenous malformations are thought of to be stable over time. However complications such as ischemia, neovascularization and vitreous hemorrhage might occur. Observation is warranted.\u00a0 As such, timely treatment can be offered to avoid complications.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "521.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 547146956,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5002837",
    "created": "1624237991"
  },
  {
    "id": 116,
    "canonicalId": "reyfkeovagczl7dk2hf94p9l",
    "datasetId": "reyfkeovagczl7dk2hf94p9l",
    "doi": "10.5281/zenodo.13916845",
    "title": "Segmentation Dataset for Periorbital Segmentation and Distance Prediction",
    "description": "<p>High quality segmentation of the eyes and lids is an essential step in developing clinically relevant deep learning models for oculoplastic and craniofacial surgery. However, there are currently no publicly available datasets suitable for this purpose. As such, we have developed and validated a novel dataset for oculoplastic segmentation and periorbital distance prediction. Using images from two open-source datasets, we segmented the iris, sclera, lid, caruncle, and brow from cropped eye images. Five trained annotators performed the segmentations, and intergrader reliability was assessed on 100 randomly selected images aftera two-week forgetting period, yielding an average Dice score of 0.82 &plusmn; 0.01. Intragrader reliability on 20 images averaged a Dice score of 0.81 &plusmn; 0.08. To demonstrate the dataset's utility, we trained three DeepLabV3 models following standard procedures. This first-of-its-kind dataset, along with a toolkit for periorbital distance prediction, is publicly available to support the development of clinically useful segmentation models for oculoplastic and craniofacial applications.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>High quality segmentation of the eyes and lids is an essential step in developing clinically relevant deep learning models for oculoplastic and craniofacial surgery. However, there are currently no publicly available datasets suitable for this purpose. As such, we have developed and validated a novel dataset for oculoplastic segmentation and periorbital distance prediction. Using images from two open-source datasets, we segmented the iris, sclera, lid, caruncle, and brow from cropped eye images. Five trained annotators performed the segmentations, and intergrader reliability was assessed on 100 randomly selected images aftera two-week forgetting period, yielding an average Dice score of 0.82 &plusmn; 0.01. Intragrader reliability on 20 images averaged a Dice score of 0.81 &plusmn; 0.08. To demonstrate the dataset's utility, we trained three DeepLabV3 models following standard procedures. This first-of-its-kind dataset, along with a toolkit for periorbital distance prediction, is publicly available to support the development of clinically useful segmentation models for oculoplastic and craniofacial applications.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.13916845",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Segmentation Dataset for Periorbital Segmentation and Distance Prediction"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Nahass, George",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Illinois Chicago"
              }
            ]
          },
          {
            "creatorName": "Yi, Darvin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Illinois Chicago"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-10-10",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>High quality segmentation of the eyes and lids is an essential step in developing clinically relevant deep learning models for oculoplastic and craniofacial surgery. However, there are currently no publicly available datasets suitable for this purpose. As such, we have developed and validated a novel dataset for oculoplastic segmentation and periorbital distance prediction. Using images from two open-source datasets, we segmented the iris, sclera, lid, caruncle, and brow from cropped eye images. Five trained annotators performed the segmentations, and intergrader reliability was assessed on 100 randomly selected images aftera two-week forgetting period, yielding an average Dice score of 0.82 &plusmn; 0.01. Intragrader reliability on 20 images averaged a Dice score of 0.81 &plusmn; 0.08. To demonstrate the dataset's utility, we trained three DeepLabV3 models following standard procedures. This first-of-its-kind dataset, along with a toolkit for periorbital distance prediction, is publicly available to support the development of clinically useful segmentation models for oculoplastic and craniofacial applications.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Segmentation"
          },
          {
            "subjectValue": "Computer Vision"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "181.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 190106828,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/13916845",
    "created": "1728595202"
  },
  {
    "id": 117,
    "canonicalId": "i32nngrnvxxz42iow8artbi3",
    "datasetId": "i32nngrnvxxz42iow8artbi3",
    "doi": "10.5281/zenodo.17958965",
    "title": "Motion enhances camouflage: Retinal evidence for flicker fusion camouflage in jumping spiders",
    "description": "<p>This is the dataset for</p>\n<p>Min Tan, Alex M. Winsor, Long Yu, Eunice J. Tan, Elizabeth M. Jakob, Daiqin Li. (2025). Motion enhances camouflage: Retinal evidence for flicker fusion camouflage in jumping spiders.</p>\n<p><strong>Funding sources</strong></p>\n<p><span lang=\"EN-US\">This study was supported by the grants from the National Natural Science Foundation of China (32270531 to DLi and 32301291 to LYu), the Singapore Ministry of Education Academic Research Fund (AcRF) to DLi (A-8001085-00-00), the National Science Foundation award (1656714 to EMJakob), and the Oscar and Jan Francke Student Research Fund of the International Society of Arachnology to MTan.</span></p>\n<p><strong><span lang=\"EN-US\">Overview for Tan_et_al_data_and_codes</span></strong></p>\n<p><span lang=\"EN-US\">Folder List:</span></p>\n<p><span lang=\"EN-US\">Codes_Tan_et_al: This folder contains the R code (in software) for data analyses and figures.</span></p>\n<p><span lang=\"EN-US\">Data_Tan_et_al: This folder contains the dataset (file type \"csv\").</span></p>\n<p><strong><span lang=\"EN-US\">Methodological information</span></strong></p>\n<p><span lang=\"EN-US\">The data was collected following the protocol described in Tan et al. (2025).</span></p>\n<p><span lang=\"EN-US\">The data was analysed with R 4.4.2.</span></p>\n<p><strong><span lang=\"EN-US\">Specific information</span></strong></p>\n<p><span lang=\"EN-US\">More details for each folder can be found within the folders.</span></p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This is the dataset for</p>\n<p>Min Tan, Alex M. Winsor, Long Yu, Eunice J. Tan, Elizabeth M. Jakob, Daiqin Li. (2025). Motion enhances camouflage: Retinal evidence for flicker fusion camouflage in jumping spiders.</p>\n<p><strong>Funding sources</strong></p>\n<p><span lang=\"EN-US\">This study was supported by the grants from the National Natural Science Foundation of China (32270531 to DLi and 32301291 to LYu), the Singapore Ministry of Education Academic Research Fund (AcRF) to DLi (A-8001085-00-00), the National Science Foundation award (1656714 to EMJakob), and the Oscar and Jan Francke Student Research Fund of the International Society of Arachnology to MTan.</span></p>\n<p><strong><span lang=\"EN-US\">Overview for Tan_et_al_data_and_codes</span></strong></p>\n<p><span lang=\"EN-US\">Folder List:</span></p>\n<p><span lang=\"EN-US\">Codes_Tan_et_al: This folder contains the R code (in software) for data analyses and figures.</span></p>\n<p><span lang=\"EN-US\">Data_Tan_et_al: This folder contains the dataset (file type \"csv\").</span></p>\n<p><strong><span lang=\"EN-US\">Methodological information</span></strong></p>\n<p><span lang=\"EN-US\">The data was collected following the protocol described in Tan et al. (2025).</span></p>\n<p><span lang=\"EN-US\">The data was analysed with R 4.4.2.</span></p>\n<p><strong><span lang=\"EN-US\">Specific information</span></strong></p>\n<p><span lang=\"EN-US\">More details for each folder can be found within the folders.</span></p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.17958965",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Motion enhances camouflage: Retinal evidence for flicker fusion camouflage in jumping spiders"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Tan, Min",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Singapore"
              }
            ]
          },
          {
            "creatorName": "Winsor, Alex",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Massachusetts, Amherst"
              }
            ]
          },
          {
            "creatorName": "Yu, Long",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Hubei University"
              }
            ]
          },
          {
            "creatorName": "Tan, Eunice Jingmei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Singapore"
              }
            ]
          },
          {
            "creatorName": "Jakob, Elizabeth",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Massachusetts Amherst"
              }
            ]
          },
          {
            "creatorName": "Li, Daiqin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "National University of Singapore"
              }
            ]
          }
        ],
        "publicationYear": "2025",
        "date": [
          {
            "dateValue": "2025-12-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This is the dataset for</p>\n<p>Min Tan, Alex M. Winsor, Long Yu, Eunice J. Tan, Elizabeth M. Jakob, Daiqin Li. (2025). Motion enhances camouflage: Retinal evidence for flicker fusion camouflage in jumping spiders.</p>\n<p><strong>Funding sources</strong></p>\n<p><span lang=\"EN-US\">This study was supported by the grants from the National Natural Science Foundation of China (32270531 to DLi and 32301291 to LYu), the Singapore Ministry of Education Academic Research Fund (AcRF) to DLi (A-8001085-00-00), the National Science Foundation award (1656714 to EMJakob), and the Oscar and Jan Francke Student Research Fund of the International Society of Arachnology to MTan.</span></p>\n<p><strong><span lang=\"EN-US\">Overview for Tan_et_al_data_and_codes</span></strong></p>\n<p><span lang=\"EN-US\">Folder List:</span></p>\n<p><span lang=\"EN-US\">Codes_Tan_et_al: This folder contains the R code (in software) for data analyses and figures.</span></p>\n<p><span lang=\"EN-US\">Data_Tan_et_al: This folder contains the dataset (file type \"csv\").</span></p>\n<p><strong><span lang=\"EN-US\">Methodological information</span></strong></p>\n<p><span lang=\"EN-US\">The data was collected following the protocol described in Tan et al. (2025).</span></p>\n<p><span lang=\"EN-US\">The data was analysed with R 4.4.2.</span></p>\n<p><strong><span lang=\"EN-US\">Specific information</span></strong></p>\n<p><span lang=\"EN-US\">More details for each folder can be found within the folders.</span></p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "anti-predation"
          },
          {
            "subjectValue": "crypsis"
          },
          {
            "subjectValue": "eye-tracker"
          },
          {
            "subjectValue": "salticid"
          },
          {
            "subjectValue": "stripe contrast"
          },
          {
            "subjectValue": "biological sciences"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1.5 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1572864,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/17958965",
    "created": "1765947279"
  },
  {
    "id": 118,
    "canonicalId": "cgzh045o89zf8q74saoypjd8",
    "datasetId": "cgzh045o89zf8q74saoypjd8",
    "doi": "10.5061/dryad.xgxd254d1",
    "title": "Systemic paralogy and function of retinal determination network homologs in arachnids",
    "description": "<p>Arachnids are important components of cave ecosystems and display many examples of troglomorphisms, such as blindness, depigmentation, and elongate appendages. Little is known about how the eyes of arachnids are specified genetically, let alone the mechanisms for eye reduction and loss in troglomorphic arachnids. Additionally, paralogy of Retinal Determination Gene Network (RDGN) homologs in spiders has convoluted functional inferences extrapolated from single-copy homologs in pancrustacean models. Here, we investigated a sister species pair of Israeli cave whip spiders (Arachnopulmonata, Amblypygi, <i>Charinus</i>) of which one species has reduced eyes. We generated the first embryonic transcriptomes for Amblypygi, and discovered that several RDGN homologs exhibit duplications. We show that paralogy of RDGN homologs is systemic across arachnopulmonates (arachnid orders that bear book lungs), rather than being a spider-specific phenomenon. A differential gene expression (DGE) analysis comparing the expression of RDGN genes in field-collected embryos of both species identified candidate RDGN genes involved in the formation and reduction of eyes in whip spiders. To ground bioinformatic inference of expression patterns with functional experiments, we interrogated the function of three candidate RDGN genes identified from DGE in a spider, using RNAi in the spider <i>Parasteatoda tepidariorum</i>. We provide functional evidence that one of these paralogs, <i>sine oculis/Six1 A </i>(<i>soA</i>), is necessary for the development of all arachnid eye types. Our results support the conservation of at least one RDGN component across Arthropoda and establish a framework for investigating the role of gene duplications in arachnid eye diversity.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Arachnids are important components of cave ecosystems and display many examples of troglomorphisms, such as blindness, depigmentation, and elongate appendages. Little is known about how the eyes of arachnids are specified genetically, let alone the mechanisms for eye reduction and loss in troglomorphic arachnids. Additionally, paralogy of Retinal Determination Gene Network (RDGN) homologs in spiders has convoluted functional inferences extrapolated from single-copy homologs in pancrustacean models. Here, we investigated a sister species pair of Israeli cave whip spiders (Arachnopulmonata, Amblypygi, <i>Charinus</i>) of which one species has reduced eyes. We generated the first embryonic transcriptomes for Amblypygi, and discovered that several RDGN homologs exhibit duplications. We show that paralogy of RDGN homologs is systemic across arachnopulmonates (arachnid orders that bear book lungs), rather than being a spider-specific phenomenon. A differential gene expression (DGE) analysis comparing the expression of RDGN genes in field-collected embryos of both species identified candidate RDGN genes involved in the formation and reduction of eyes in whip spiders. To ground bioinformatic inference of expression patterns with functional experiments, we interrogated the function of three candidate RDGN genes identified from DGE in a spider, using RNAi in the spider <i>Parasteatoda tepidariorum</i>. We provide functional evidence that one of these paralogs, <i>sine oculis/Six1 A </i>(<i>soA</i>), is necessary for the development of all arachnid eye types. Our results support the conservation of at least one RDGN component across Arthropoda and establish a framework for investigating the role of gene duplications in arachnid eye diversity.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.xgxd254d1",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Systemic paralogy and function of retinal determination network homologs in arachnids"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Gainett, Guilherme",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          },
          {
            "creatorName": "A. Ballesteros, Jes\u00fas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          },
          {
            "creatorName": "R. Kanzler, Charlotte",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          },
          {
            "creatorName": "T. Zehms, Jakob",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          },
          {
            "creatorName": "M. Zern, John",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          },
          {
            "creatorName": "Aharon, Shlomi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Hebrew University of Jerusalem"
              }
            ]
          },
          {
            "creatorName": "Gavish-Regev, Efrat",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Hebrew University of Jerusalem"
              }
            ]
          },
          {
            "creatorName": "P. Sharma, Prashant",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Wisconsin-Madison"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-11-30",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Arachnids are important components of cave ecosystems and display many examples of troglomorphisms, such as blindness, depigmentation, and elongate appendages. Little is known about how the eyes of arachnids are specified genetically, let alone the mechanisms for eye reduction and loss in troglomorphic arachnids. Additionally, paralogy of Retinal Determination Gene Network (RDGN) homologs in spiders has convoluted functional inferences extrapolated from single-copy homologs in pancrustacean models. Here, we investigated a sister species pair of Israeli cave whip spiders (Arachnopulmonata, Amblypygi, <i>Charinus</i>) of which one species has reduced eyes. We generated the first embryonic transcriptomes for Amblypygi, and discovered that several RDGN homologs exhibit duplications. We show that paralogy of RDGN homologs is systemic across arachnopulmonates (arachnid orders that bear book lungs), rather than being a spider-specific phenomenon. A differential gene expression (DGE) analysis comparing the expression of RDGN genes in field-collected embryos of both species identified candidate RDGN genes involved in the formation and reduction of eyes in whip spiders. To ground bioinformatic inference of expression patterns with functional experiments, we interrogated the function of three candidate RDGN genes identified from DGE in a spider, using RNAi in the spider <i>Parasteatoda tepidariorum</i>. We provide functional evidence that one of these paralogs, <i>sine oculis/Six1 A </i>(<i>soA</i>), is necessary for the development of all arachnid eye types. Our results support the conservation of at least one RDGN component across Arthropoda and establish a framework for investigating the role of gene duplications in arachnid eye diversity.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "57.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 60083404,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4299195",
    "created": "1606776409"
  },
  {
    "id": 119,
    "canonicalId": "p199ggmjyvt9c4uvskawma1v",
    "datasetId": "p199ggmjyvt9c4uvskawma1v",
    "doi": "10.5061/dryad.kv3j7",
    "title": "Data from: Cambrian origin of the CYP27C1-mediated vitamin A1-to-A2 switch, a key mechanism of vertebrate sensory plasticity",
    "description": "The spectral composition of ambient light varies across both space and time. Many species of jawed vertebrates adapt to this variation by tuning the sensitivity of their photoreceptors via the expression of CYP27C1, an enzyme that converts vitamin A1 into vitamin A2, thereby shifting the ratio of vitamin A1-based rhodopsin to red-shifted vitamin A2-based porphyropsin in the eye. Here, we show that the sea lamprey (Petromyzon marinus), a jawless vertebrate that diverged from jawed vertebrates during the Cambrian period (approx. 500\u2005Ma), dynamically shifts its photoreceptor spectral sensitivity via vitamin A1-to-A2 chromophore exchange as it transitions between photically divergent aquatic habitats. We further show that this shift correlates with high-level expression of the lamprey orthologue of CYP27C1, specifically in the retinal pigment epithelium as in jawed vertebrates. Our results suggest that the CYP27C1-mediated vitamin A1-to-A2 switch is an evolutionarily ancient mechanism of sensory plasticity that appeared not long after the origin of vertebrates.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "The spectral composition of ambient light varies across both space and time. Many species of jawed vertebrates adapt to this variation by tuning the sensitivity of their photoreceptors via the expression of CYP27C1, an enzyme that converts vitamin A1 into vitamin A2, thereby shifting the ratio of vitamin A1-based rhodopsin to red-shifted vitamin A2-based porphyropsin in the eye. Here, we show that the sea lamprey (Petromyzon marinus), a jawless vertebrate that diverged from jawed vertebrates during the Cambrian period (approx. 500\u2005Ma), dynamically shifts its photoreceptor spectral sensitivity via vitamin A1-to-A2 chromophore exchange as it transitions between photically divergent aquatic habitats. We further show that this shift correlates with high-level expression of the lamprey orthologue of CYP27C1, specifically in the retinal pigment epithelium as in jawed vertebrates. Our results suggest that the CYP27C1-mediated vitamin A1-to-A2 switch is an evolutionarily ancient mechanism of sensory plasticity that appeared not long after the origin of vertebrates.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.kv3j7",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Cambrian origin of the CYP27C1-mediated vitamin A1-to-A2 switch, a key mechanism of vertebrate sensory plasticity"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Morshedian, Ala",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Toomey, Matthew B.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Washington University School of Medicine"
              }
            ]
          },
          {
            "creatorName": "Pollock, Gabriel E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Frederiksen, Rikard",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Boston University School of Medicine"
              }
            ]
          },
          {
            "creatorName": "Enright, Jennifer M.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Washington University School of Medicine"
              }
            ]
          },
          {
            "creatorName": "McCormick, Stephen D.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Washington University in St. Louis School of Medicine"
              }
            ]
          },
          {
            "creatorName": "Cornwall, M. Carter",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Boston University School of Medicine"
              }
            ]
          },
          {
            "creatorName": "Fain, Gordon L.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Los Angeles"
              }
            ]
          },
          {
            "creatorName": "Corbo, Joseph C.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Washington University School of Medicine"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-06-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "The spectral composition of ambient light varies across both space and time. Many species of jawed vertebrates adapt to this variation by tuning the sensitivity of their photoreceptors via the expression of CYP27C1, an enzyme that converts vitamin A1 into vitamin A2, thereby shifting the ratio of vitamin A1-based rhodopsin to red-shifted vitamin A2-based porphyropsin in the eye. Here, we show that the sea lamprey (Petromyzon marinus), a jawless vertebrate that diverged from jawed vertebrates during the Cambrian period (approx. 500\u2005Ma), dynamically shifts its photoreceptor spectral sensitivity via vitamin A1-to-A2 chromophore exchange as it transitions between photically divergent aquatic habitats. We further show that this shift correlates with high-level expression of the lamprey orthologue of CYP27C1, specifically in the retinal pigment epithelium as in jawed vertebrates. Our results suggest that the CYP27C1-mediated vitamin A1-to-A2 switch is an evolutionarily ancient mechanism of sensory plasticity that appeared not long after the origin of vertebrates.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "visual ecology"
          },
          {
            "subjectValue": "Photoreceptor"
          },
          {
            "subjectValue": "Petromyzon marinus"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 314572,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4985378",
    "created": "1624029210"
  },
  {
    "id": 120,
    "canonicalId": "pbpliab3p91hggz2peg290zk",
    "datasetId": "pbpliab3p91hggz2peg290zk",
    "doi": "10.5061/dryad.6djh9w17h",
    "title": "Data from: ASHS-OAP atlas for automatic entorhinal cortex segmentation",
    "description": "<p>Early stages of Alzheimer's disease (AD) are associated with volume reductions in specific subregions of the medial temporal lobe (MTL). Using a manual segmentation method\u2014the Olsen-Amaral-Palombo (OAP) protocol\u2014 previous work in healthy older adults showed that reductions in grey matter volumes in MTL subregions were associated with lower scores on the Montreal Cognitive Assessment (MoCA), suggesting atrophy may occur prior to diagnosis of mild cognitive impairment, a condition that often progresses to AD. However, current manual segmentation methods are labour intensive and time consuming. Here, we examined the utility of Automatic Segmentation of Hippocampal Subfields (ASHS) to detect volumetric differences in MTL subregions of healthy older adults who varied in cognitive status as determined by the MoCA. We trained ASHS on the OAP protocol to create the ASHS-OAP atlas, and then examined how well automated segmentation replicated the ground truth of manual segmentation. Volumetric measures obtained from the ASHS-OAP atlas were also contrasted against those from the ASHS-PMC atlas, a widely used atlas provided by the ASHS team. Volumetrics from the ASHS-OAP atlas aligned well with those from manual segmentation, suggesting ASHS-OAP is a viable alternative to current manual segmentation methods. In addition, while some subtle differences were observed, results from the ASHS-PMC and ASHS-OAP atlases aligned well with each other overall. Our findings highlight the utility of automated segmentation methods but still underscore the need for a unified and harmonized MTL segmentation atlas.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Early stages of Alzheimer's disease (AD) are associated with volume reductions in specific subregions of the medial temporal lobe (MTL). Using a manual segmentation method\u2014the Olsen-Amaral-Palombo (OAP) protocol\u2014 previous work in healthy older adults showed that reductions in grey matter volumes in MTL subregions were associated with lower scores on the Montreal Cognitive Assessment (MoCA), suggesting atrophy may occur prior to diagnosis of mild cognitive impairment, a condition that often progresses to AD. However, current manual segmentation methods are labour intensive and time consuming. Here, we examined the utility of Automatic Segmentation of Hippocampal Subfields (ASHS) to detect volumetric differences in MTL subregions of healthy older adults who varied in cognitive status as determined by the MoCA. We trained ASHS on the OAP protocol to create the ASHS-OAP atlas, and then examined how well automated segmentation replicated the ground truth of manual segmentation. Volumetric measures obtained from the ASHS-OAP atlas were also contrasted against those from the ASHS-PMC atlas, a widely used atlas provided by the ASHS team. Volumetrics from the ASHS-OAP atlas aligned well with those from manual segmentation, suggesting ASHS-OAP is a viable alternative to current manual segmentation methods. In addition, while some subtle differences were observed, results from the ASHS-PMC and ASHS-OAP atlases aligned well with each other overall. Our findings highlight the utility of automated segmentation methods but still underscore the need for a unified and harmonized MTL segmentation atlas.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.6djh9w17h",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: ASHS-OAP atlas for automatic entorhinal cortex segmentation"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Mazloum-Farzaghi, Negar",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          },
          {
            "creatorName": "Barense, Morgan",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          },
          {
            "creatorName": "Ryan, Jennifer",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          },
          {
            "creatorName": "Stark, Craig",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California Irvine"
              }
            ]
          },
          {
            "creatorName": "Olsen, Rosanna",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-02-27",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Early stages of Alzheimer's disease (AD) are associated with volume reductions in specific subregions of the medial temporal lobe (MTL). Using a manual segmentation method\u2014the Olsen-Amaral-Palombo (OAP) protocol\u2014 previous work in healthy older adults showed that reductions in grey matter volumes in MTL subregions were associated with lower scores on the Montreal Cognitive Assessment (MoCA), suggesting atrophy may occur prior to diagnosis of mild cognitive impairment, a condition that often progresses to AD. However, current manual segmentation methods are labour intensive and time consuming. Here, we examined the utility of Automatic Segmentation of Hippocampal Subfields (ASHS) to detect volumetric differences in MTL subregions of healthy older adults who varied in cognitive status as determined by the MoCA. We trained ASHS on the OAP protocol to create the ASHS-OAP atlas, and then examined how well automated segmentation replicated the ground truth of manual segmentation. Volumetric measures obtained from the ASHS-OAP atlas were also contrasted against those from the ASHS-PMC atlas, a widely used atlas provided by the ASHS team. Volumetrics from the ASHS-OAP atlas aligned well with those from manual segmentation, suggesting ASHS-OAP is a viable alternative to current manual segmentation methods. In addition, while some subtle differences were observed, results from the ASHS-PMC and ASHS-OAP atlases aligned well with each other overall. Our findings highlight the utility of automated segmentation methods but still underscore the need for a unified and harmonized MTL segmentation atlas.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "automatic segmentation"
          },
          {
            "subjectValue": "medial temporal lobe"
          },
          {
            "subjectValue": "Alzheimer's disease"
          },
          {
            "subjectValue": "mild cognitive impairment"
          },
          {
            "subjectValue": "neurodegeneration"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "3820.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 4006189465,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/10716020",
    "created": "1709033450"
  },
  {
    "id": 121,
    "canonicalId": "jg13ajk3j3eqmpjp4mp6qxew",
    "datasetId": "jg13ajk3j3eqmpjp4mp6qxew",
    "doi": "10.5281/zenodo.6590774",
    "title": "Dataset TrainBatch2 for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)",
    "description": "<p>[Attention]: ATM_164_0000.nii.gz has misaligned label to CT image, please discard this case!</p>\n\n<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch2.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>[Attention]: ATM_164_0000.nii.gz has misaligned label to CT image, please discard this case!</p>\n\n<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch2.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6590774",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset TrainBatch2 for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Minghui Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yangqian Wu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Hanxiao Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Weihao Yu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yun Gu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-05-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>[Attention]: ATM_164_0000.nii.gz has misaligned label to CT image, please discard this case!</p>\n\n<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch2.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "26204.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 27477829222,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6590774",
    "created": "1653838144"
  },
  {
    "id": 122,
    "canonicalId": "h1z2tha5sl9i07dt0ykbgwms",
    "datasetId": "h1z2tha5sl9i07dt0ykbgwms",
    "doi": "10.5061/dryad.q6ft5",
    "title": "Data from: Development of machine learning models for diagnosis of glaucoma",
    "description": "The study aimed to develop machine learning models that have strong prediction power and interpretability for diagnosis of glaucoma based on retinal nerve fiber layer (RNFL) thickness and visual field (VF). We collected various candidate features from the examination of retinal nerve fiber layer (RNFL) thickness and visual field (VF). We also developed synthesized features from original features. We then selected the best features proper for classification (diagnosis) through feature evaluation. We used 100 cases of data as a test dataset and 399 cases of data as a training and validation dataset. To develop the glaucoma prediction model, we considered four machine learning algorithms: C5.0, random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). We repeatedly composed a learning model using the training dataset and evaluated it by using the validation dataset. Finally, we got the best learning model that produces the highest validation accuracy. We analyzed quality of the models using several measures. The random forest model shows best performance and C5.0, SVM, and KNN models show similar accuracy. In the random forest model, the classification accuracy is 0.98, sensitivity is 0.983, specificity is 0.975, and AUC is 0.979. The developed prediction models show high accuracy, sensitivity, specificity, and AUC in classifying among glaucoma and healthy eyes. It will be used for predicting glaucoma against unknown examination records. Clinicians may reference the prediction results and be able to make better decisions. We may combine multiple learning models to increase prediction accuracy. The C5.0 model includes decision rules for prediction. It can be used to explain the reasons for specific predictions.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "The study aimed to develop machine learning models that have strong prediction power and interpretability for diagnosis of glaucoma based on retinal nerve fiber layer (RNFL) thickness and visual field (VF). We collected various candidate features from the examination of retinal nerve fiber layer (RNFL) thickness and visual field (VF). We also developed synthesized features from original features. We then selected the best features proper for classification (diagnosis) through feature evaluation. We used 100 cases of data as a test dataset and 399 cases of data as a training and validation dataset. To develop the glaucoma prediction model, we considered four machine learning algorithms: C5.0, random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). We repeatedly composed a learning model using the training dataset and evaluated it by using the validation dataset. Finally, we got the best learning model that produces the highest validation accuracy. We analyzed quality of the models using several measures. The random forest model shows best performance and C5.0, SVM, and KNN models show similar accuracy. In the random forest model, the classification accuracy is 0.98, sensitivity is 0.983, specificity is 0.975, and AUC is 0.979. The developed prediction models show high accuracy, sensitivity, specificity, and AUC in classifying among glaucoma and healthy eyes. It will be used for predicting glaucoma against unknown examination records. Clinicians may reference the prediction results and be able to make better decisions. We may combine multiple learning models to increase prediction accuracy. The C5.0 model includes decision rules for prediction. It can be used to explain the reasons for specific predictions.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.q6ft5",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Development of machine learning models for diagnosis of glaucoma"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Kim, Seongjae",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Gyeongsang National University"
              }
            ]
          },
          {
            "creatorName": "Cho, Kyung Jin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Dankook University"
              }
            ]
          },
          {
            "creatorName": "Oh, Sejong",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Dankook University"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-05-05",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "The study aimed to develop machine learning models that have strong prediction power and interpretability for diagnosis of glaucoma based on retinal nerve fiber layer (RNFL) thickness and visual field (VF). We collected various candidate features from the examination of retinal nerve fiber layer (RNFL) thickness and visual field (VF). We also developed synthesized features from original features. We then selected the best features proper for classification (diagnosis) through feature evaluation. We used 100 cases of data as a test dataset and 399 cases of data as a training and validation dataset. To develop the glaucoma prediction model, we considered four machine learning algorithms: C5.0, random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). We repeatedly composed a learning model using the training dataset and evaluated it by using the validation dataset. Finally, we got the best learning model that produces the highest validation accuracy. We analyzed quality of the models using several measures. The random forest model shows best performance and C5.0, SVM, and KNN models show similar accuracy. In the random forest model, the classification accuracy is 0.98, sensitivity is 0.983, specificity is 0.975, and AUC is 0.979. The developed prediction models show high accuracy, sensitivity, specificity, and AUC in classifying among glaucoma and healthy eyes. It will be used for predicting glaucoma against unknown examination records. Clinicians may reference the prediction results and be able to make better decisions. We may combine multiple learning models to increase prediction accuracy. The C5.0 model includes decision rules for prediction. It can be used to explain the reasons for specific predictions.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "cornea thickness"
          },
          {
            "subjectValue": "RNFL"
          },
          {
            "subjectValue": "glaucoma"
          },
          {
            "subjectValue": "ocular pressure"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 0,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4957619",
    "created": "1623766631"
  },
  {
    "id": 123,
    "canonicalId": "gpkf01a3x0na9kxqsqnqz5f2",
    "datasetId": "gpkf01a3x0na9kxqsqnqz5f2",
    "doi": "10.5061/dryad.6np02kb",
    "title": "Pinopsin evolved as the ancestral dim-light visual opsin in vertebrates",
    "description": "<p>Pinopsin is the opsin most closely related to vertebrate visual pigments on the phylogenetic tree. This opsin has been discovered among many vertebrates, except mammals and teleosts, and was thought to exclusively function in their brain for extraocular photoreception. Here, we show the possibility that pinopsin also contributes to scotopic vision in some vertebrate species. Pinopsin is distributed in the retina of non-teleost fishes and frogs, especially in their rod photoreceptor cells, in addition to their brain. Moreover, the retinal chromophore of pinopsin exhibits a thermal isomerization rate considerably lower than those of cone visual pigments, but comparable to that of rhodopsin. Therefore, pinopsin can function as a rhodopsin-like visual pigment in the retinas of these lower vertebrates. Since pinopsin diversified before the branching of rhodopsin on the phylogenetic tree, two-step adaptation to scotopic vision would have occurred through the independent acquisition of pinopsin and rhodopsin by the vertebrate lineage.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Pinopsin is the opsin most closely related to vertebrate visual pigments on the phylogenetic tree. This opsin has been discovered among many vertebrates, except mammals and teleosts, and was thought to exclusively function in their brain for extraocular photoreception. Here, we show the possibility that pinopsin also contributes to scotopic vision in some vertebrate species. Pinopsin is distributed in the retina of non-teleost fishes and frogs, especially in their rod photoreceptor cells, in addition to their brain. Moreover, the retinal chromophore of pinopsin exhibits a thermal isomerization rate considerably lower than those of cone visual pigments, but comparable to that of rhodopsin. Therefore, pinopsin can function as a rhodopsin-like visual pigment in the retinas of these lower vertebrates. Since pinopsin diversified before the branching of rhodopsin on the phylogenetic tree, two-step adaptation to scotopic vision would have occurred through the independent acquisition of pinopsin and rhodopsin by the vertebrate lineage.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.6np02kb",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Pinopsin evolved as the ancestral dim-light visual opsin in vertebrates"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Sato, Keita",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Okayama University"
              }
            ]
          },
          {
            "creatorName": "Yamashita, Takahiro",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          },
          {
            "creatorName": "Kojima, Keiichi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          },
          {
            "creatorName": "Sakai, Kazumi",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          },
          {
            "creatorName": "Matsutani, Yuki",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          },
          {
            "creatorName": "Yanagawa, Masataka",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "RIKEN"
              }
            ]
          },
          {
            "creatorName": "Yamano, Yumiko",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kobe Pharmaceutical University"
              }
            ]
          },
          {
            "creatorName": "Wada, Akimori",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kobe Pharmaceutical University"
              }
            ]
          },
          {
            "creatorName": "Iwabe, Naoyuki",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          },
          {
            "creatorName": "Ohuchi, Hideyo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Okayama University"
              }
            ]
          },
          {
            "creatorName": "Shichida, Yoshinori",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Kyoto University"
              }
            ]
          }
        ],
        "publicationYear": "2018",
        "date": [
          {
            "dateValue": "2018-10-01",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Pinopsin is the opsin most closely related to vertebrate visual pigments on the phylogenetic tree. This opsin has been discovered among many vertebrates, except mammals and teleosts, and was thought to exclusively function in their brain for extraocular photoreception. Here, we show the possibility that pinopsin also contributes to scotopic vision in some vertebrate species. Pinopsin is distributed in the retina of non-teleost fishes and frogs, especially in their rod photoreceptor cells, in addition to their brain. Moreover, the retinal chromophore of pinopsin exhibits a thermal isomerization rate considerably lower than those of cone visual pigments, but comparable to that of rhodopsin. Therefore, pinopsin can function as a rhodopsin-like visual pigment in the retinas of these lower vertebrates. Since pinopsin diversified before the branching of rhodopsin on the phylogenetic tree, two-step adaptation to scotopic vision would have occurred through the independent acquisition of pinopsin and rhodopsin by the vertebrate lineage.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "biochemistry"
          },
          {
            "subjectValue": "Proteins"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "172.8 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 181193932,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/5013594",
    "created": "1624373215"
  },
  {
    "id": 124,
    "canonicalId": "llh9q4bdyuez91zf1d6bmwto",
    "datasetId": "llh9q4bdyuez91zf1d6bmwto",
    "doi": "10.5281/zenodo.13734306",
    "title": "AquaXAI: Visual and Textual Dataset for XAI in Aquatic Environments",
    "description": "<p>This Dataset is for the publication: \"Interactive Simulator Framework for XAI Applications in Aquatic Environments\"<br><br>AquaXAI is a synthetic dataset aiming to explain boat skippers' actions in aquatic environments, using Unity Simulator.</p>\n<p>The dataset consists of:</p>\n<p>Image: RGB + Semantic Segmentation +&nbsp; Instance Segmentation + Bounding Box</p>\n<p>Textual Explanations: Action + Explanation</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This Dataset is for the publication: \"Interactive Simulator Framework for XAI Applications in Aquatic Environments\"<br><br>AquaXAI is a synthetic dataset aiming to explain boat skippers' actions in aquatic environments, using Unity Simulator.</p>\n<p>The dataset consists of:</p>\n<p>Image: RGB + Semantic Segmentation +&nbsp; Instance Segmentation + Bounding Box</p>\n<p>Textual Explanations: Action + Explanation</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.13734306",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "AquaXAI: Visual and Textual Dataset for XAI in Aquatic Environments"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Elsayed, Ahmed H.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          },
          {
            "creatorName": "El-Mihoub, Tarek A.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          },
          {
            "creatorName": "Manss, Christoph",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          },
          {
            "creatorName": "Miedtank, Andre",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          },
          {
            "creatorName": "Nolle, Lars",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          },
          {
            "creatorName": "Stahl, Frederic",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "German Research Centre for Artificial Intelligence"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-09-09",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This Dataset is for the publication: \"Interactive Simulator Framework for XAI Applications in Aquatic Environments\"<br><br>AquaXAI is a synthetic dataset aiming to explain boat skippers' actions in aquatic environments, using Unity Simulator.</p>\n<p>The dataset consists of:</p>\n<p>Image: RGB + Semantic Segmentation +&nbsp; Instance Segmentation + Bounding Box</p>\n<p>Textual Explanations: Action + Explanation</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "499.0 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 523239424,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/13734306",
    "created": "1725871745"
  },
  {
    "id": 125,
    "canonicalId": "ixdst6va862dfu9tt4zl1zd5",
    "datasetId": "ixdst6va862dfu9tt4zl1zd5",
    "doi": "10.5061/dryad.3cp54",
    "title": "Data from: Analysis of potential ischemic effect of intravitreal bevacizumab on unaffected retina in treatment-na\u00efve macular edema due to branch retinal vein occlusion: a prospective, interventional case-series",
    "description": "Background: To study potential ischemic effects of intravitreal Bevacizumab (IVB) on unaffected retina in treatment-naive eyes with macular edema secondary to branch retinal vein occlusion (BRVO) and contralateral eyes secondary to systemic absorption. Methods and Findings: Prospective, interventional series included 27 treatment-naive eyes with BRVO and macular edema. Exclusion criteria: Eyes with diabetic retinopathy, glaucoma, vasculitides, papilledema or systemic neurologic condition. Subjects underwent complete ophthalmological examination including fluoroscein angiography (FA), optical coherence tomography (OCT) and multifocal electroretinogram (mf-ERG). All subjects received single 1.25 mg/0.05ml IVB injection. Two observers measured all parameters; inter-observer agreements were expressed as kappa values. Paired t-test was used to compare values at baseline and follow-up. The statistical analysis was done using SPSS for Windows, Version 14.0. (Chicago, SPSS Inc.) Presenting mean CFT (central foveal thickness) was 499.5(+/-229.7) \u03bcm, mean BCVA (best corrected visual acuity) was 0.64(+/-0.41) logMAR. At last follow-up, mean CFT was 267.9(+/-159.3) \u03bcm (P&lt;0.001), 95% CI [127.18, 422.32]; mean BCVA was 0.28(+/-0.24) logMAR. Respectively, mean N1 and P1 amplitudes of mfERG in 'unaffected quadrant' at presentation were -6.10(+/-4.00) nV/deg2 and 17.17(+/-11.54)nV/deg2; and -5.33(+/-1.30)nV/deg2 and 15.29(+/-4.69)nV/deg2 at final follow-up (P = 0.631 and 0.197, respectively), (95% CIs [-0.93, 1.42] and [-4.22, 1.08] respectively). In fundus quadrant of fellow eyes corresponding to unaffected quadrant in treated eyes, mean N1 and P1 amplitudes at presentation were -5.39(+/-1.56)nV/deg2 and 15.89(+/-3.89)nV/deg2; and -5.39(+/-1.90)nV/deg2 and 15.9(+/-5.52)nV/deg2 (P = 0.380 and 0.208), (95% CIs [-0.57, 1.28] and [-4.1, 1.1]) at last follow-up, respectively. Limitations: This study analysed the effects with a single injection of bevacizumab. However, whether ischemic adverse effects will emerge with repeated IVB injections as a consequence of cumulative dosing needs further investigation. The setting of our study being a tertiary care centre, the numbers of fresh BRVO cases without prior intervention were limited. Thus, the limitations of our study include a small sample size with a small follow-up period. No major ocular/systemic adverse event was observed in the study period. Conclusion: No evidence of progressive ischaemia attributable to single bevacizumab treatment was observed in this study. However, a larger prospective study involving subjects with cumulative dosing of bevacizumab and a longer follow-up could provide a better understanding of the potential ischaemic effects of bevacizumab or other anti-VEGF agents.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Background: To study potential ischemic effects of intravitreal Bevacizumab (IVB) on unaffected retina in treatment-naive eyes with macular edema secondary to branch retinal vein occlusion (BRVO) and contralateral eyes secondary to systemic absorption. Methods and Findings: Prospective, interventional series included 27 treatment-naive eyes with BRVO and macular edema. Exclusion criteria: Eyes with diabetic retinopathy, glaucoma, vasculitides, papilledema or systemic neurologic condition. Subjects underwent complete ophthalmological examination including fluoroscein angiography (FA), optical coherence tomography (OCT) and multifocal electroretinogram (mf-ERG). All subjects received single 1.25 mg/0.05ml IVB injection. Two observers measured all parameters; inter-observer agreements were expressed as kappa values. Paired t-test was used to compare values at baseline and follow-up. The statistical analysis was done using SPSS for Windows, Version 14.0. (Chicago, SPSS Inc.) Presenting mean CFT (central foveal thickness) was 499.5(+/-229.7) \u03bcm, mean BCVA (best corrected visual acuity) was 0.64(+/-0.41) logMAR. At last follow-up, mean CFT was 267.9(+/-159.3) \u03bcm (P&lt;0.001), 95% CI [127.18, 422.32]; mean BCVA was 0.28(+/-0.24) logMAR. Respectively, mean N1 and P1 amplitudes of mfERG in 'unaffected quadrant' at presentation were -6.10(+/-4.00) nV/deg2 and 17.17(+/-11.54)nV/deg2; and -5.33(+/-1.30)nV/deg2 and 15.29(+/-4.69)nV/deg2 at final follow-up (P = 0.631 and 0.197, respectively), (95% CIs [-0.93, 1.42] and [-4.22, 1.08] respectively). In fundus quadrant of fellow eyes corresponding to unaffected quadrant in treated eyes, mean N1 and P1 amplitudes at presentation were -5.39(+/-1.56)nV/deg2 and 15.89(+/-3.89)nV/deg2; and -5.39(+/-1.90)nV/deg2 and 15.9(+/-5.52)nV/deg2 (P = 0.380 and 0.208), (95% CIs [-0.57, 1.28] and [-4.1, 1.1]) at last follow-up, respectively. Limitations: This study analysed the effects with a single injection of bevacizumab. However, whether ischemic adverse effects will emerge with repeated IVB injections as a consequence of cumulative dosing needs further investigation. The setting of our study being a tertiary care centre, the numbers of fresh BRVO cases without prior intervention were limited. Thus, the limitations of our study include a small sample size with a small follow-up period. No major ocular/systemic adverse event was observed in the study period. Conclusion: No evidence of progressive ischaemia attributable to single bevacizumab treatment was observed in this study. However, a larger prospective study involving subjects with cumulative dosing of bevacizumab and a longer follow-up could provide a better understanding of the potential ischaemic effects of bevacizumab or other anti-VEGF agents.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.3cp54",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Analysis of potential ischemic effect of intravitreal bevacizumab on unaffected retina in treatment-na\u00efve macular edema due to branch retinal vein occlusion: a prospective, interventional case-series"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Rishi, Pukhraj",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Raka, Neha",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sankara Nethralaya"
              }
            ]
          },
          {
            "creatorName": "Rishi, Ekta",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sankara Nethralaya"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-08-30",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Background: To study potential ischemic effects of intravitreal Bevacizumab (IVB) on unaffected retina in treatment-naive eyes with macular edema secondary to branch retinal vein occlusion (BRVO) and contralateral eyes secondary to systemic absorption. Methods and Findings: Prospective, interventional series included 27 treatment-naive eyes with BRVO and macular edema. Exclusion criteria: Eyes with diabetic retinopathy, glaucoma, vasculitides, papilledema or systemic neurologic condition. Subjects underwent complete ophthalmological examination including fluoroscein angiography (FA), optical coherence tomography (OCT) and multifocal electroretinogram (mf-ERG). All subjects received single 1.25 mg/0.05ml IVB injection. Two observers measured all parameters; inter-observer agreements were expressed as kappa values. Paired t-test was used to compare values at baseline and follow-up. The statistical analysis was done using SPSS for Windows, Version 14.0. (Chicago, SPSS Inc.) Presenting mean CFT (central foveal thickness) was 499.5(+/-229.7) \u03bcm, mean BCVA (best corrected visual acuity) was 0.64(+/-0.41) logMAR. At last follow-up, mean CFT was 267.9(+/-159.3) \u03bcm (P&lt;0.001), 95% CI [127.18, 422.32]; mean BCVA was 0.28(+/-0.24) logMAR. Respectively, mean N1 and P1 amplitudes of mfERG in 'unaffected quadrant' at presentation were -6.10(+/-4.00) nV/deg2 and 17.17(+/-11.54)nV/deg2; and -5.33(+/-1.30)nV/deg2 and 15.29(+/-4.69)nV/deg2 at final follow-up (P = 0.631 and 0.197, respectively), (95% CIs [-0.93, 1.42] and [-4.22, 1.08] respectively). In fundus quadrant of fellow eyes corresponding to unaffected quadrant in treated eyes, mean N1 and P1 amplitudes at presentation were -5.39(+/-1.56)nV/deg2 and 15.89(+/-3.89)nV/deg2; and -5.39(+/-1.90)nV/deg2 and 15.9(+/-5.52)nV/deg2 (P = 0.380 and 0.208), (95% CIs [-0.57, 1.28] and [-4.1, 1.1]) at last follow-up, respectively. Limitations: This study analysed the effects with a single injection of bevacizumab. However, whether ischemic adverse effects will emerge with repeated IVB injections as a consequence of cumulative dosing needs further investigation. The setting of our study being a tertiary care centre, the numbers of fresh BRVO cases without prior intervention were limited. Thus, the limitations of our study include a small sample size with a small follow-up period. No major ocular/systemic adverse event was observed in the study period. Conclusion: No evidence of progressive ischaemia attributable to single bevacizumab treatment was observed in this study. However, a larger prospective study involving subjects with cumulative dosing of bevacizumab and a longer follow-up could provide a better understanding of the potential ischaemic effects of bevacizumab or other anti-VEGF agents.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "58.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 61446553,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4950918",
    "created": "1623723289"
  },
  {
    "id": 126,
    "canonicalId": "m7bs6uu57raw7kzzhh8zzctd",
    "datasetId": "m7bs6uu57raw7kzzhh8zzctd",
    "doi": "10.5281/zenodo.7949582",
    "title": "Dataset TrainBatch1 for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)",
    "description": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch1.&nbsp;</p>\n\n<p>We provide the casename correspondence information in the TrainBatch1_CaseInfo.csv file.&nbsp; It is convinient for researchers to locate the original cases in LIDC-IDRI with our provided full airway annotation for other research purposes.</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the <strong>References </strong>below !!!</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch1.&nbsp;</p>\n\n<p>We provide the casename correspondence information in the TrainBatch1_CaseInfo.csv file.&nbsp; It is convinient for researchers to locate the original cases in LIDC-IDRI with our provided full airway annotation for other research purposes.</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the <strong>References </strong>below !!!</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.7949582",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset TrainBatch1 for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Minghui Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yangqian Wu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Hanxiao Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Weihao Yu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yun Gu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-05-29",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;TrainBatch1.&nbsp;</p>\n\n<p>We provide the casename correspondence information in the TrainBatch1_CaseInfo.csv file.&nbsp; It is convinient for researchers to locate the original cases in LIDC-IDRI with our provided full airway annotation for other research purposes.</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the <strong>References </strong>below !!!</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "30785.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 32280831590,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/7949582",
    "created": "1684485855"
  },
  {
    "id": 127,
    "canonicalId": "l2baroxn6vv67gjih3qvqeit",
    "datasetId": "l2baroxn6vv67gjih3qvqeit",
    "doi": "10.5281/zenodo.292994",
    "title": "Concept detection scores for the IACC.3 dataset (TRECVID AVS Task)",
    "description": "<p>We provide concept detection scores for the IACC.3 dataset (600 hr internet archive videos), which is used in the TRECVID Ad-hoc Video Search (AVS) task [1]. Concept detection scores for 1345 concepts (1000 ImageNet concepts provided for the ILSVRC challenge [2] and 345 TRECVID SIN concepts [3]) have been generated as follows:<br>\n1) To generate scores for the ImageNet concepts, 5 pre-trained ImageNet networks were applied on the IACC.3 dataset and their output was fused in terms of arithmetic mean.<br>\n2) To generate scores for the TRECVID SIN concepts, two pre-trained ImageNet networks were fine-tuned on these concepts using a combination of our methods presented in the following papers: [4], [5]. We provide two different sets of concept scores for the TRECVID SIN concepts: a) The output of the two fine-tuned networks was fused in terms of arithmetic mean in order to return a single score for each concept. b) The last fully-connected layer was used as feature to train SVM classifiers separately for each fine-tuned network and each concept. Then, the SVM classifiers were applied on the IACC.3 dataset and the prediction scores of the SVMs for the same concept were fused in terms of arithmetic mean in order to return a single score for each concept. We evaluated the two different sets of concepts in terms of MXInfAP on a subset of 38 TRECVID SIN concepts for which ground-truth annotation exists, and the MXInfAP of each set of concept scores is: a) 30.04% for the networks' direct output, b) 35.81% for the SVM classifiers.</p>\n\n<p>Three different files of concept detection scores can be downloaded (after unpacking the compressed file):<br>\n1) scores_ImageNet.txt<br>\n2a) scores_SIN_direct.txt<br>\n2b) scores_SIN_svm.txt<br>\nIn total there are 335944 rows in each file; 1002 columns in the first file and 347 columns in each of the other two. Each row in any of these files corresponds to a different video shot; the video shot IDs appear in the first two columns. (Note: the shot IDs are the ones from the mp7 files in the TRECVID AVS master shot reference, with the format shotFILENUMBER_SHOTNUMBER). Then, each column (except for the fist two) corresponds to a different concept, with all concept scores being in [0,1] range. The higher the score the more likely that the corresponding concept appears in the video shot. Files \u201cconcept_names_ImageNet.txt\u201d and \u201cconcept_names_SIN.txt\u201d indicate the order of the concepts that is used in the concept score files.\u00a0</p>\n\n<p>[1] G. Awad, J. Fiscus, M. Michel et al. 2016. TRECVID 2016: Evaluating Video Search, Video Event Detection, Localization, and Hyperlinking. In TRECVID 2016 Workshop. NIST, USA.<br>\n[2] O. Russakovsky, J. Deng, H. Su et al. 2015. ImageNet Large Scale Visual Recognition Challenge. Int. Journal of Computer Vision (IJCV) 115, 211\u2013252.<br>\n[3] G. Awad, C. Snoek, A. Smeaton, and G. Qu\u00e9not. 2016. TRECVid semantic indexing of video: a 6-year retrospective. ITE Transactions on Media Technology and Applications, 4 (3). pp. 187-208.<br>\n[4] N. Pittaras, F. Markatopoulou, V. Mezaris, I. Patras. 2017. Comparison of Fine-tuning and Extension Strategies for Deep Convolutional Neural Networks, Proc. 23rd Int. Conf. on MultiMedia Modeling (MMM'17), Reykjavik, Iceland, Springer LNCS vol. 10132, pp. 102-114, Jan. 2017.<br>\n[5] F. Markatopoulou, V. Mezaris, and I. Patras. 2016. Deep Multi-task Learning with Label Correlation Constraint for Video Concept Detection, Proc. ACM Multimedia 2016, Amsterdam, Oct. 2016.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>We provide concept detection scores for the IACC.3 dataset (600 hr internet archive videos), which is used in the TRECVID Ad-hoc Video Search (AVS) task [1]. Concept detection scores for 1345 concepts (1000 ImageNet concepts provided for the ILSVRC challenge [2] and 345 TRECVID SIN concepts [3]) have been generated as follows:<br>\n1) To generate scores for the ImageNet concepts, 5 pre-trained ImageNet networks were applied on the IACC.3 dataset and their output was fused in terms of arithmetic mean.<br>\n2) To generate scores for the TRECVID SIN concepts, two pre-trained ImageNet networks were fine-tuned on these concepts using a combination of our methods presented in the following papers: [4], [5]. We provide two different sets of concept scores for the TRECVID SIN concepts: a) The output of the two fine-tuned networks was fused in terms of arithmetic mean in order to return a single score for each concept. b) The last fully-connected layer was used as feature to train SVM classifiers separately for each fine-tuned network and each concept. Then, the SVM classifiers were applied on the IACC.3 dataset and the prediction scores of the SVMs for the same concept were fused in terms of arithmetic mean in order to return a single score for each concept. We evaluated the two different sets of concepts in terms of MXInfAP on a subset of 38 TRECVID SIN concepts for which ground-truth annotation exists, and the MXInfAP of each set of concept scores is: a) 30.04% for the networks' direct output, b) 35.81% for the SVM classifiers.</p>\n\n<p>Three different files of concept detection scores can be downloaded (after unpacking the compressed file):<br>\n1) scores_ImageNet.txt<br>\n2a) scores_SIN_direct.txt<br>\n2b) scores_SIN_svm.txt<br>\nIn total there are 335944 rows in each file; 1002 columns in the first file and 347 columns in each of the other two. Each row in any of these files corresponds to a different video shot; the video shot IDs appear in the first two columns. (Note: the shot IDs are the ones from the mp7 files in the TRECVID AVS master shot reference, with the format shotFILENUMBER_SHOTNUMBER). Then, each column (except for the fist two) corresponds to a different concept, with all concept scores being in [0,1] range. The higher the score the more likely that the corresponding concept appears in the video shot. Files \u201cconcept_names_ImageNet.txt\u201d and \u201cconcept_names_SIN.txt\u201d indicate the order of the concepts that is used in the concept score files.\u00a0</p>\n\n<p>[1] G. Awad, J. Fiscus, M. Michel et al. 2016. TRECVID 2016: Evaluating Video Search, Video Event Detection, Localization, and Hyperlinking. In TRECVID 2016 Workshop. NIST, USA.<br>\n[2] O. Russakovsky, J. Deng, H. Su et al. 2015. ImageNet Large Scale Visual Recognition Challenge. Int. Journal of Computer Vision (IJCV) 115, 211\u2013252.<br>\n[3] G. Awad, C. Snoek, A. Smeaton, and G. Qu\u00e9not. 2016. TRECVid semantic indexing of video: a 6-year retrospective. ITE Transactions on Media Technology and Applications, 4 (3). pp. 187-208.<br>\n[4] N. Pittaras, F. Markatopoulou, V. Mezaris, I. Patras. 2017. Comparison of Fine-tuning and Extension Strategies for Deep Convolutional Neural Networks, Proc. 23rd Int. Conf. on MultiMedia Modeling (MMM'17), Reykjavik, Iceland, Springer LNCS vol. 10132, pp. 102-114, Jan. 2017.<br>\n[5] F. Markatopoulou, V. Mezaris, and I. Patras. 2016. Deep Multi-task Learning with Label Correlation Constraint for Video Concept Detection, Proc. ACM Multimedia 2016, Amsterdam, Oct. 2016.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.292994",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Concept detection scores for the IACC.3 dataset (TRECVID AVS Task)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Markatopoulou, Foteini",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "CERTH-ITI"
              }
            ]
          },
          {
            "creatorName": "Mezaris, Vasileios",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "CERTH-ITI"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-02-17",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>We provide concept detection scores for the IACC.3 dataset (600 hr internet archive videos), which is used in the TRECVID Ad-hoc Video Search (AVS) task [1]. Concept detection scores for 1345 concepts (1000 ImageNet concepts provided for the ILSVRC challenge [2] and 345 TRECVID SIN concepts [3]) have been generated as follows:<br>\n1) To generate scores for the ImageNet concepts, 5 pre-trained ImageNet networks were applied on the IACC.3 dataset and their output was fused in terms of arithmetic mean.<br>\n2) To generate scores for the TRECVID SIN concepts, two pre-trained ImageNet networks were fine-tuned on these concepts using a combination of our methods presented in the following papers: [4], [5]. We provide two different sets of concept scores for the TRECVID SIN concepts: a) The output of the two fine-tuned networks was fused in terms of arithmetic mean in order to return a single score for each concept. b) The last fully-connected layer was used as feature to train SVM classifiers separately for each fine-tuned network and each concept. Then, the SVM classifiers were applied on the IACC.3 dataset and the prediction scores of the SVMs for the same concept were fused in terms of arithmetic mean in order to return a single score for each concept. We evaluated the two different sets of concepts in terms of MXInfAP on a subset of 38 TRECVID SIN concepts for which ground-truth annotation exists, and the MXInfAP of each set of concept scores is: a) 30.04% for the networks' direct output, b) 35.81% for the SVM classifiers.</p>\n\n<p>Three different files of concept detection scores can be downloaded (after unpacking the compressed file):<br>\n1) scores_ImageNet.txt<br>\n2a) scores_SIN_direct.txt<br>\n2b) scores_SIN_svm.txt<br>\nIn total there are 335944 rows in each file; 1002 columns in the first file and 347 columns in each of the other two. Each row in any of these files corresponds to a different video shot; the video shot IDs appear in the first two columns. (Note: the shot IDs are the ones from the mp7 files in the TRECVID AVS master shot reference, with the format shotFILENUMBER_SHOTNUMBER). Then, each column (except for the fist two) corresponds to a different concept, with all concept scores being in [0,1] range. The higher the score the more likely that the corresponding concept appears in the video shot. Files \u201cconcept_names_ImageNet.txt\u201d and \u201cconcept_names_SIN.txt\u201d indicate the order of the concepts that is used in the concept score files.\u00a0</p>\n\n<p>[1] G. Awad, J. Fiscus, M. Michel et al. 2016. TRECVID 2016: Evaluating Video Search, Video Event Detection, Localization, and Hyperlinking. In TRECVID 2016 Workshop. NIST, USA.<br>\n[2] O. Russakovsky, J. Deng, H. Su et al. 2015. ImageNet Large Scale Visual Recognition Challenge. Int. Journal of Computer Vision (IJCV) 115, 211\u2013252.<br>\n[3] G. Awad, C. Snoek, A. Smeaton, and G. Qu\u00e9not. 2016. TRECVid semantic indexing of video: a 6-year retrospective. ITE Transactions on Media Technology and Applications, 4 (3). pp. 187-208.<br>\n[4] N. Pittaras, F. Markatopoulou, V. Mezaris, I. Patras. 2017. Comparison of Fine-tuning and Extension Strategies for Deep Convolutional Neural Networks, Proc. 23rd Int. Conf. on MultiMedia Modeling (MMM'17), Reykjavik, Iceland, Springer LNCS vol. 10132, pp. 102-114, Jan. 2017.<br>\n[5] F. Markatopoulou, V. Mezaris, and I. Patras. 2016. Deep Multi-task Learning with Label Correlation Constraint for Video Concept Detection, Proc. ACM Multimedia 2016, Amsterdam, Oct. 2016.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "concept detection"
          },
          {
            "subjectValue": "ad-hoc video search"
          },
          {
            "subjectValue": "TRECVID AVS task"
          },
          {
            "subjectValue": "video analysis"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "1622.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 1701419417,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/292994",
    "created": "1487346680"
  },
  {
    "id": 128,
    "canonicalId": "o5egqb476mail8rtdob1hmjd",
    "datasetId": "o5egqb476mail8rtdob1hmjd",
    "doi": "10.5281/zenodo.6616655",
    "title": "Data for: Regularly occurring bouts of retinal movements suggest an REM sleep\u2013like state in jumping spiders",
    "description": "<p>Sleep and sleep-like states are present across the animal kingdom, with recent studies convincingly demonstrating sleep-like states in arthropods, nematodes, and even cnidarians. However, the existence of different sleep phases across taxa is as yet unclear. In particular, the study of rapid eye movement (REM) sleep is still largely centered on terrestrial vertebrates, particularly mammals and birds. The most salient indicator of REM sleep is the movement of eyes during this phase. Movable eyes, however, have evolved only in a limited number of lineages&mdash;an adaptation notably absent in insects and most&nbsp;terrestrial&nbsp;arthropods&mdash;restricting cross-species comparisons. Jumping spiders, however, possess movable retinal tubes to redirect gaze, and in newly emerged spiderlings, these movements can be directly observed through their temporarily translucent exoskeleton. Here, we report evidence for an REM sleep&ndash;like state in a terrestrial invertebrate: periodic bouts of retinal movements coupled with limb twitching and stereotyped leg curling behaviors during nocturnal resting in a jumping spider. Observed retinal movement bouts were consistent, including regular durations and intervals, with both increasing over the course of the night. That these characteristic REM sleep&ndash;like behaviors exist in a highly visual, long-diverged lineage further challenges our understanding of this sleep state. Comparisons across such long-diverged lineages likely hold important questions and answers about the visual brain as well as the origin, evolution, and function of REM sleep.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Sleep and sleep-like states are present across the animal kingdom, with recent studies convincingly demonstrating sleep-like states in arthropods, nematodes, and even cnidarians. However, the existence of different sleep phases across taxa is as yet unclear. In particular, the study of rapid eye movement (REM) sleep is still largely centered on terrestrial vertebrates, particularly mammals and birds. The most salient indicator of REM sleep is the movement of eyes during this phase. Movable eyes, however, have evolved only in a limited number of lineages&mdash;an adaptation notably absent in insects and most&nbsp;terrestrial&nbsp;arthropods&mdash;restricting cross-species comparisons. Jumping spiders, however, possess movable retinal tubes to redirect gaze, and in newly emerged spiderlings, these movements can be directly observed through their temporarily translucent exoskeleton. Here, we report evidence for an REM sleep&ndash;like state in a terrestrial invertebrate: periodic bouts of retinal movements coupled with limb twitching and stereotyped leg curling behaviors during nocturnal resting in a jumping spider. Observed retinal movement bouts were consistent, including regular durations and intervals, with both increasing over the course of the night. That these characteristic REM sleep&ndash;like behaviors exist in a highly visual, long-diverged lineage further challenges our understanding of this sleep state. Comparisons across such long-diverged lineages likely hold important questions and answers about the visual brain as well as the origin, evolution, and function of REM sleep.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6616655",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data for: Regularly occurring bouts of retinal movements suggest an REM sleep\u2013like state in jumping spiders"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "R\u00f6\u00dfler, Daniela C.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Konstanz"
              }
            ]
          },
          {
            "creatorName": "Kim, Kris",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Harvard University"
              }
            ]
          },
          {
            "creatorName": "De Agr\u00f2, Massimo",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Florence"
              }
            ]
          },
          {
            "creatorName": "Jordan, Alex",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Max Planck Institute of Animal Behavior"
              }
            ]
          },
          {
            "creatorName": "Galizia, C. Giovanni",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Konstanz"
              }
            ]
          },
          {
            "creatorName": "Shamble, Paul S.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Harvard University"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-06-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Sleep and sleep-like states are present across the animal kingdom, with recent studies convincingly demonstrating sleep-like states in arthropods, nematodes, and even cnidarians. However, the existence of different sleep phases across taxa is as yet unclear. In particular, the study of rapid eye movement (REM) sleep is still largely centered on terrestrial vertebrates, particularly mammals and birds. The most salient indicator of REM sleep is the movement of eyes during this phase. Movable eyes, however, have evolved only in a limited number of lineages&mdash;an adaptation notably absent in insects and most&nbsp;terrestrial&nbsp;arthropods&mdash;restricting cross-species comparisons. Jumping spiders, however, possess movable retinal tubes to redirect gaze, and in newly emerged spiderlings, these movements can be directly observed through their temporarily translucent exoskeleton. Here, we report evidence for an REM sleep&ndash;like state in a terrestrial invertebrate: periodic bouts of retinal movements coupled with limb twitching and stereotyped leg curling behaviors during nocturnal resting in a jumping spider. Observed retinal movement bouts were consistent, including regular durations and intervals, with both increasing over the course of the night. That these characteristic REM sleep&ndash;like behaviors exist in a highly visual, long-diverged lineage further challenges our understanding of this sleep state. Comparisons across such long-diverged lineages likely hold important questions and answers about the visual brain as well as the origin, evolution, and function of REM sleep.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "571.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 599680614,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6616655",
    "created": "1658129562"
  },
  {
    "id": 129,
    "canonicalId": "w0lfjh5bxg7d7we3q7nk9iwd",
    "datasetId": "w0lfjh5bxg7d7we3q7nk9iwd",
    "doi": "10.6078/D1313P",
    "title": "Data from: Functional modules for visual scene segmentation in macaque visual cortex",
    "description": "<p>Segmentation, the computation of object boundaries, is one of the most important steps in intermediate visual processing. Previous studies have reported cells across visual cortex that are modulated by segmentation features, but the functional role of these cells remains unclear. First, it is unclear whether these cells encode segmentation consistently since most studies used only a limited variety of stimulus types. Second, it is unclear whether these cells are organized into specialized modules or instead randomly scattered across the visual cortex: the former would lend credence to a functional role for putative segmentation cells. Here, we used fMRI-guided electrophysiology to systematically characterize the consistency and spatial organization of segmentation-encoding cells across the visual cortex. Using fMRI, we identified a set of patches in V2, V3, V3A, V4, and V4A that were more active for stimuli containing figures compared to ground, regardless of whether figures were defined by texture, motion, luminance, or disparity. We targeted these patches for single-unit recordings and found that cells inside segmentation patches were tuned to both figure-ground and borders more consistently across types of stimuli than cells in the visual cortex outside the patches. Remarkably, we found clusters of cells inside segmentation patches that showed the same border-ownership preference across all stimulus types. Finally, using a population decoding approach, we found that segmentation could be decoded with higher accuracy from segmentation patches than from either color-selective or control regions. Overall, our results suggest that segmentation signals are preferentially encoded in spatially discrete patches.</p>\n<p>The dataset uploaded here contains spike rasters for all cells and stimuli analyzed for the paper, in addition to information about the locations and animals they were recorded from, and information about the different stimuli used. In addition, it contains averaged base-line-subtracted time courses of fMRI activations. Please read the README.md for more detailed information about each variable.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Segmentation, the computation of object boundaries, is one of the most important steps in intermediate visual processing. Previous studies have reported cells across visual cortex that are modulated by segmentation features, but the functional role of these cells remains unclear. First, it is unclear whether these cells encode segmentation consistently since most studies used only a limited variety of stimulus types. Second, it is unclear whether these cells are organized into specialized modules or instead randomly scattered across the visual cortex: the former would lend credence to a functional role for putative segmentation cells. Here, we used fMRI-guided electrophysiology to systematically characterize the consistency and spatial organization of segmentation-encoding cells across the visual cortex. Using fMRI, we identified a set of patches in V2, V3, V3A, V4, and V4A that were more active for stimuli containing figures compared to ground, regardless of whether figures were defined by texture, motion, luminance, or disparity. We targeted these patches for single-unit recordings and found that cells inside segmentation patches were tuned to both figure-ground and borders more consistently across types of stimuli than cells in the visual cortex outside the patches. Remarkably, we found clusters of cells inside segmentation patches that showed the same border-ownership preference across all stimulus types. Finally, using a population decoding approach, we found that segmentation could be decoded with higher accuracy from segmentation patches than from either color-selective or control regions. Overall, our results suggest that segmentation signals are preferentially encoded in spatially discrete patches.</p>\n<p>The dataset uploaded here contains spike rasters for all cells and stimuli analyzed for the paper, in addition to information about the locations and animals they were recorded from, and information about the different stimuli used. In addition, it contains averaged base-line-subtracted time courses of fMRI activations. Please read the README.md for more detailed information about each variable.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.6078/D1313P",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Functional modules for visual scene segmentation in macaque visual cortex"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Hesse, Janis",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          },
          {
            "creatorName": "Tsao, Doris",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of California, Berkeley"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-07-18",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Segmentation, the computation of object boundaries, is one of the most important steps in intermediate visual processing. Previous studies have reported cells across visual cortex that are modulated by segmentation features, but the functional role of these cells remains unclear. First, it is unclear whether these cells encode segmentation consistently since most studies used only a limited variety of stimulus types. Second, it is unclear whether these cells are organized into specialized modules or instead randomly scattered across the visual cortex: the former would lend credence to a functional role for putative segmentation cells. Here, we used fMRI-guided electrophysiology to systematically characterize the consistency and spatial organization of segmentation-encoding cells across the visual cortex. Using fMRI, we identified a set of patches in V2, V3, V3A, V4, and V4A that were more active for stimuli containing figures compared to ground, regardless of whether figures were defined by texture, motion, luminance, or disparity. We targeted these patches for single-unit recordings and found that cells inside segmentation patches were tuned to both figure-ground and borders more consistently across types of stimuli than cells in the visual cortex outside the patches. Remarkably, we found clusters of cells inside segmentation patches that showed the same border-ownership preference across all stimulus types. Finally, using a population decoding approach, we found that segmentation could be decoded with higher accuracy from segmentation patches than from either color-selective or control regions. Overall, our results suggest that segmentation signals are preferentially encoded in spatially discrete patches.</p>\n<p>The dataset uploaded here contains spike rasters for all cells and stimuli analyzed for the paper, in addition to information about the locations and animals they were recorded from, and information about the different stimuli used. In addition, it contains averaged base-line-subtracted time courses of fMRI activations. Please read the README.md for more detailed information about each variable.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Electrophysiology"
          },
          {
            "subjectValue": "Neuroscience"
          },
          {
            "subjectValue": "Vision"
          },
          {
            "subjectValue": "segmentation"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "99.2 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 104018739,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8162076",
    "created": "1689738059"
  },
  {
    "id": 130,
    "canonicalId": "jvs317x21get5cbzyu59csjz",
    "datasetId": "jvs317x21get5cbzyu59csjz",
    "doi": "10.5281/zenodo.6590026",
    "title": "Dataset Validation Images for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)",
    "description": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;Validation Image part.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;Validation Image part.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.6590026",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Dataset Validation Images for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM'22)"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Minghui Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yangqian Wu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Hanxiao Zhang",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Weihao Yu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          },
          {
            "creatorName": "Yun Gu",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-05-28",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset for the MICCAI-2022-Challenge: Airway Tree Modeling (ATM&#39;22)</p>\n\n<p>This is the&nbsp;Validation Image part.&nbsp;</p>\n\n<p>If you use&nbsp;this dataset in your research, you must cite the papers in the References below !!!</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "8351.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 8757287321,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6590026",
    "created": "1653838221"
  },
  {
    "id": 131,
    "canonicalId": "bflae58nkts3a8m1tjfdg9h9",
    "datasetId": "bflae58nkts3a8m1tjfdg9h9",
    "doi": "10.5061/dryad.r2280gbj2",
    "title": "Visual opsin gene expression evolution in the adaptive radiation of cichlid fishes of Lake Tanganyika",
    "description": "<p>Tuning the visual sensory system to the ambient light is essential for survival in many animal species. This is often achieved through duplication, functional diversification, and/or differential expression of visual opsin genes. Here, we examined 753 new retinal transcriptomes from 112 species of cichlid fishes from Lake Tanganyika to unravel adaptive changes in gene expression at the macro-evolutionary and ecosystem level of one of the largest vertebrate adaptive radiations. We found that, across the radiation, all seven cone opsins \u2013 but not the rhodopsin \u2013 rank among the most differentially expressed genes in the retina, together with other vision-, circadian-rhythm-, and haemoglobin-related genes. We propose two new visual palettes characteristic of very shallow- and deep-water living species, respectively, and show that visual system adaptations along two major ecological axes, macro-habitat and diet, occur primarily via gene expression variation in a subset of cone opsin genes.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Tuning the visual sensory system to the ambient light is essential for survival in many animal species. This is often achieved through duplication, functional diversification, and/or differential expression of visual opsin genes. Here, we examined 753 new retinal transcriptomes from 112 species of cichlid fishes from Lake Tanganyika to unravel adaptive changes in gene expression at the macro-evolutionary and ecosystem level of one of the largest vertebrate adaptive radiations. We found that, across the radiation, all seven cone opsins \u2013 but not the rhodopsin \u2013 rank among the most differentially expressed genes in the retina, together with other vision-, circadian-rhythm-, and haemoglobin-related genes. We propose two new visual palettes characteristic of very shallow- and deep-water living species, respectively, and show that visual system adaptations along two major ecological axes, macro-habitat and diet, occur primarily via gene expression variation in a subset of cone opsin genes.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.r2280gbj2",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Visual opsin gene expression evolution in the adaptive radiation of cichlid fishes of Lake Tanganyika"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Ricci, Virginie",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Basel"
              }
            ]
          },
          {
            "creatorName": "Ronco, Fabrizia",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Oslo"
              }
            ]
          },
          {
            "creatorName": "Boileau, Nicolas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Basel"
              }
            ]
          },
          {
            "creatorName": "Salzburger, Walter",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Basel"
              }
            ]
          }
        ],
        "publicationYear": "2023",
        "date": [
          {
            "dateValue": "2023-08-03",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Tuning the visual sensory system to the ambient light is essential for survival in many animal species. This is often achieved through duplication, functional diversification, and/or differential expression of visual opsin genes. Here, we examined 753 new retinal transcriptomes from 112 species of cichlid fishes from Lake Tanganyika to unravel adaptive changes in gene expression at the macro-evolutionary and ecosystem level of one of the largest vertebrate adaptive radiations. We found that, across the radiation, all seven cone opsins \u2013 but not the rhodopsin \u2013 rank among the most differentially expressed genes in the retina, together with other vision-, circadian-rhythm-, and haemoglobin-related genes. We propose two new visual palettes characteristic of very shallow- and deep-water living species, respectively, and show that visual system adaptations along two major ecological axes, macro-habitat and diet, occur primarily via gene expression variation in a subset of cone opsin genes.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "visual opsins"
          },
          {
            "subjectValue": "retinal transcriptome"
          },
          {
            "subjectValue": "Cichlid fishes"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "105.9 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 111044198,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/8212083",
    "created": "1691076642"
  },
  {
    "id": 132,
    "canonicalId": "b4ldfzk62oa6uy29s1qtjyrm",
    "datasetId": "b4ldfzk62oa6uy29s1qtjyrm",
    "doi": "10.5061/dryad.5k0s6",
    "title": "Data from: Epistatic interactions influence terrestrial-marine functional shifts in cetacean rhodopsin",
    "description": "Like many aquatic vertebrates, whales have blue-shifting spectral tuning substitutions in the dim-light visual pigment, rhodopsin, that are thought to increase photosensitivity in underwater environments. We have discovered that known spectral tuning substitutions also have surprising epistatic effects on another function of rhodopsin, the kinetic rates associated with light-activated intermediates. By using absorbance spectroscopy and fluorescence-based retinal release assays on heterologously expressed rhodopsin, we assessed both spectral and kinetic differences between cetaceans (killer whale) and terrestrial outgroups (hippo, bovine). Mutation experiments revealed that killer whale rhodopsin is unusually resilient to pleiotropic effects on retinal release from key blue-shifting substitutions (D83N and A292S), largely due to a surprisingly specific epistatic interaction between D83N and the background residue, S299. Ancestral sequence reconstruction indicated that S299 is an ancestral residue that predates the evolution of blue-shifting substitutions at the origins of Cetacea. Based on these results, we hypothesize that intramolecular epistasis helped to conserve rhodopsin's kinetic properties while enabling blue-shifting spectral tuning substitutions as cetaceans adapted to aquatic environments. Trade-offs between different aspects of molecular function are rarely considered in protein evolution, but in cetacean and other vertebrate rhodopsins, may underlie multiple evolutionary scenarios for the selection of specific amino acid substitutions.",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "Like many aquatic vertebrates, whales have blue-shifting spectral tuning substitutions in the dim-light visual pigment, rhodopsin, that are thought to increase photosensitivity in underwater environments. We have discovered that known spectral tuning substitutions also have surprising epistatic effects on another function of rhodopsin, the kinetic rates associated with light-activated intermediates. By using absorbance spectroscopy and fluorescence-based retinal release assays on heterologously expressed rhodopsin, we assessed both spectral and kinetic differences between cetaceans (killer whale) and terrestrial outgroups (hippo, bovine). Mutation experiments revealed that killer whale rhodopsin is unusually resilient to pleiotropic effects on retinal release from key blue-shifting substitutions (D83N and A292S), largely due to a surprisingly specific epistatic interaction between D83N and the background residue, S299. Ancestral sequence reconstruction indicated that S299 is an ancestral residue that predates the evolution of blue-shifting substitutions at the origins of Cetacea. Based on these results, we hypothesize that intramolecular epistasis helped to conserve rhodopsin's kinetic properties while enabling blue-shifting spectral tuning substitutions as cetaceans adapted to aquatic environments. Trade-offs between different aspects of molecular function are rarely considered in protein evolution, but in cetacean and other vertebrate rhodopsins, may underlie multiple evolutionary scenarios for the selection of specific amino acid substitutions.",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.5k0s6",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Data from: Epistatic interactions influence terrestrial-marine functional shifts in cetacean rhodopsin"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Dungan, Sarah Z.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          },
          {
            "creatorName": "Chang, Belinda S. W.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "University of Toronto"
              }
            ]
          }
        ],
        "publicationYear": "2017",
        "date": [
          {
            "dateValue": "2017-02-06",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "Like many aquatic vertebrates, whales have blue-shifting spectral tuning substitutions in the dim-light visual pigment, rhodopsin, that are thought to increase photosensitivity in underwater environments. We have discovered that known spectral tuning substitutions also have surprising epistatic effects on another function of rhodopsin, the kinetic rates associated with light-activated intermediates. By using absorbance spectroscopy and fluorescence-based retinal release assays on heterologously expressed rhodopsin, we assessed both spectral and kinetic differences between cetaceans (killer whale) and terrestrial outgroups (hippo, bovine). Mutation experiments revealed that killer whale rhodopsin is unusually resilient to pleiotropic effects on retinal release from key blue-shifting substitutions (D83N and A292S), largely due to a surprisingly specific epistatic interaction between D83N and the background residue, S299. Ancestral sequence reconstruction indicated that S299 is an ancestral residue that predates the evolution of blue-shifting substitutions at the origins of Cetacea. Based on these results, we hypothesize that intramolecular epistasis helped to conserve rhodopsin's kinetic properties while enabling blue-shifting spectral tuning substitutions as cetaceans adapted to aquatic environments. Trade-offs between different aspects of molecular function are rarely considered in protein evolution, but in cetacean and other vertebrate rhodopsins, may underlie multiple evolutionary scenarios for the selection of specific amino acid substitutions.",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "protein evolution"
          },
          {
            "subjectValue": "epistasis"
          },
          {
            "subjectValue": "evolution of protein structure-function"
          },
          {
            "subjectValue": "Meta II stability"
          },
          {
            "subjectValue": "Cetacea"
          },
          {
            "subjectValue": "spectral tuning"
          },
          {
            "subjectValue": "Orcinus orca"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "0.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 419430,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/4944325",
    "created": "1623667721"
  },
  {
    "id": 133,
    "canonicalId": "s8azz3ft558kqhcuu8jcga1o",
    "datasetId": "s8azz3ft558kqhcuu8jcga1o",
    "doi": "10.5281/zenodo.3708064",
    "title": "Type-specific dendritic integration in mouse retinal ganglion cells",
    "description": "<p>Dataset for <a href=\"https://www.biorxiv.org/content/10.1101/753335v1.abstract\">Ran et al (2020)</a>. These data can be read by Python (&gt;3.6) and Pandas (&gt;1.0.0):</p>\n\n<pre><code class=\"language-python\">import pickle\ndata = pickle.load(open('./data_raw.pickle', 'rb'))\n</code></pre>\n\n<p>Or use our custom scripts. For more details, see <a href=\"https://github.com/berenslab/rgc_dendrites\">https://github.com/berenslab/rgc_dendrites</a>.&nbsp;</p>",
    "versionTitle": "v1.0",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Dataset for <a href=\"https://www.biorxiv.org/content/10.1101/753335v1.abstract\">Ran et al (2020)</a>. These data can be read by Python (&gt;3.6) and Pandas (&gt;1.0.0):</p>\n\n<pre><code class=\"language-python\">import pickle\ndata = pickle.load(open('./data_raw.pickle', 'rb'))\n</code></pre>\n\n<p>Or use our custom scripts. For more details, see <a href=\"https://github.com/berenslab/rgc_dendrites\">https://github.com/berenslab/rgc_dendrites</a>.&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.3708064",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Type-specific dendritic integration in mouse retinal ganglion cells"
          }
        ],
        "version": "v1.0",
        "creator": [
          {
            "creatorName": "Ran, Yanli",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Huang, Ziwei",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Baden, Tom",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Sussex Neuroscience, University of Sussex"
              }
            ]
          },
          {
            "creatorName": "Schubert, Timm",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Baayen, Harald",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Department of Linguistics, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Berens, Philipp",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Franke, Katrin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          },
          {
            "creatorName": "Euler, Thomas",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Institute for Ophthalmic Research, University of T\u00fcbingen"
              }
            ]
          }
        ],
        "publicationYear": "2020",
        "date": [
          {
            "dateValue": "2020-03-12",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Dataset for <a href=\"https://www.biorxiv.org/content/10.1101/753335v1.abstract\">Ran et al (2020)</a>. These data can be read by Python (&gt;3.6) and Pandas (&gt;1.0.0):</p>\n\n<pre><code class=\"language-python\">import pickle\ndata = pickle.load(open('./data_raw.pickle', 'rb'))\n</code></pre>\n\n<p>Or use our custom scripts. For more details, see <a href=\"https://github.com/berenslab/rgc_dendrites\">https://github.com/berenslab/rgc_dendrites</a>.&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "308.3 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 323275980,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/3708064",
    "created": "1584029983"
  },
  {
    "id": 134,
    "canonicalId": "zuwluz0lygk0olna35msxcvm",
    "datasetId": "zuwluz0lygk0olna35msxcvm",
    "doi": "10.5281/zenodo.12172322",
    "title": "Sample tilting for speckle suppression through angular compounding: Dataset",
    "description": "<p>This data file contains an angular series of 61 OCT intensity tomograms covering a range of 60 degrees (from -30&deg; to +30&deg;) with a step angle of 1&deg; of chicken muscle tissue to generate angulary compounded speckled suppressed tomogram using the physics-informed affine mapping described in \"Sample tilting for speckle suppression through angular compounding\" by Chintada et al. (2024). The source code for the affine mapping is available on GitHub at https://github.com/bhaskarachintada/OCT_Angular_Compounding.&nbsp;</p>\n<p>&nbsp;</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>This data file contains an angular series of 61 OCT intensity tomograms covering a range of 60 degrees (from -30&deg; to +30&deg;) with a step angle of 1&deg; of chicken muscle tissue to generate angulary compounded speckled suppressed tomogram using the physics-informed affine mapping described in \"Sample tilting for speckle suppression through angular compounding\" by Chintada et al. (2024). The source code for the affine mapping is available on GitHub at https://github.com/bhaskarachintada/OCT_Angular_Compounding.&nbsp;</p>\n<p>&nbsp;</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5281/zenodo.12172322",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Sample tilting for speckle suppression through angular compounding: Dataset"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "Bhaskara Rao, Chintada",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Harvard University"
              }
            ]
          },
          {
            "creatorName": "Keahey, Pelham",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Uribe-Patarroyo, N\u00e9stor",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Bouma, Brett E.",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": null
              }
            ]
          },
          {
            "creatorName": "Villiger, Martin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Massachusetts General Hospital"
              }
            ]
          }
        ],
        "publicationYear": "2024",
        "date": [
          {
            "dateValue": "2024-06-19",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>This data file contains an angular series of 61 OCT intensity tomograms covering a range of 60 degrees (from -30&deg; to +30&deg;) with a step angle of 1&deg; of chicken muscle tissue to generate angulary compounded speckled suppressed tomogram using the physics-informed affine mapping described in \"Sample tilting for speckle suppression through angular compounding\" by Chintada et al. (2024). The source code for the affine mapping is available on GitHub at https://github.com/bhaskarachintada/OCT_Angular_Compounding.&nbsp;</p>\n<p>&nbsp;</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "657.6 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 689543577,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/12172322",
    "created": "1722647316"
  },
  {
    "id": 135,
    "canonicalId": "zfg57ri839y2373ya799iqn0",
    "datasetId": "zfg57ri839y2373ya799iqn0",
    "doi": "10.5061/dryad.f4qrfj6wp",
    "title": "Emergence of a geometric pattern of cell fates from tissue-scale mechanics in the Drosophila eye",
    "description": "<p>Pattern formation of biological structures involves the arrangement of different types of cells in an ordered spatial configuration. In this study, we investigate the mechanism of patterning the <em>Drosophila</em> eye into a precise triangular grid of photoreceptor clusters called ommatidia. Previous studies had led to a long-standing biochemical model whereby a reaction-diffusion process is templated by recently formed ommatidia to propagate a molecular prepattern across the eye epithelium. Here, we find that the templating mechanism is instead, mechanical in origin; newly born columns of ommatidia serve as a template to spatially pattern cell flows that move the cells in the epithelium into position to form each new column of ommatidia. Cell flow is generated by a pressure gradient that is caused by a narrow zone of cell dilation precisely positioned behind the growing wavefront of ommatidia. The newly formed lattice grid of ommatidia cells are immobile, deflecting and focusing the flow of other cells. Thus, the self-organization of a regular pattern of cell fates in an epithelium is mechanically driven.</p>",
    "versionTitle": "1",
    "studyTitle": "",
    "publishedMetadata": {
      "studyDescription": {},
      "readme": "<p>Pattern formation of biological structures involves the arrangement of different types of cells in an ordered spatial configuration. In this study, we investigate the mechanism of patterning the <em>Drosophila</em> eye into a precise triangular grid of photoreceptor clusters called ommatidia. Previous studies had led to a long-standing biochemical model whereby a reaction-diffusion process is templated by recently formed ommatidia to propagate a molecular prepattern across the eye epithelium. Here, we find that the templating mechanism is instead, mechanical in origin; newly born columns of ommatidia serve as a template to spatially pattern cell flows that move the cells in the epithelium into position to form each new column of ommatidia. Cell flow is generated by a pressure gradient that is caused by a narrow zone of cell dilation precisely positioned behind the growing wavefront of ommatidia. The newly formed lattice grid of ommatidia cells are immobile, deflecting and focusing the flow of other cells. Thus, the self-organization of a regular pattern of cell fates in an epithelium is mechanically driven.</p>",
      "datasetDescription": {
        "schema": "https://schema.envisionportal.io/v0.2.0/study_description.json",
        "identifier": {
          "identifierValue": "10.5061/dryad.f4qrfj6wp",
          "identifierType": "DOI"
        },
        "title": [
          {
            "titleValue": "Emergence of a geometric pattern of cell fates from tissue-scale mechanics in the Drosophila eye"
          }
        ],
        "version": "1",
        "creator": [
          {
            "creatorName": "W. Carthew, Richard",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern University"
              }
            ]
          },
          {
            "creatorName": "Gallagher, Kevin",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern University"
              }
            ]
          },
          {
            "creatorName": "Mani, Madhav",
            "nameType": "Personal",
            "affiliation": [
              {
                "affiliationName": "Northwestern University"
              }
            ]
          }
        ],
        "publicationYear": "2022",
        "date": [
          {
            "dateValue": "2022-03-22",
            "dateType": "Available",
            "dateInformation": "Date dataset made available on Zenodo"
          }
        ],
        "resourceType": {
          "resourceTypeValue": "Dataset",
          "resourceTypeGeneral": "Dataset"
        },
        "datasetDeIdentLevel": {
          "deIdentType": "NoDeIdentification",
          "deIdentDirect": false,
          "deIdentHIPAA": false,
          "deIdentDates": false,
          "deIdentNonarr": false,
          "deIdentKAnon": false,
          "deIdentDetails": "No de-identification details available"
        },
        "datasetConsent": {
          "consentType": "ConsentSpecifiedNotElsewhereCategorised",
          "consentNoncommercial": false,
          "consentGeogRestrict": false,
          "consentResearchType": false,
          "consentGeneticOnly": false,
          "consentNoMethods": false,
          "consentsDetails": ""
        },
        "description": [
          {
            "descriptionValue": "<p>Pattern formation of biological structures involves the arrangement of different types of cells in an ordered spatial configuration. In this study, we investigate the mechanism of patterning the <em>Drosophila</em> eye into a precise triangular grid of photoreceptor clusters called ommatidia. Previous studies had led to a long-standing biochemical model whereby a reaction-diffusion process is templated by recently formed ommatidia to propagate a molecular prepattern across the eye epithelium. Here, we find that the templating mechanism is instead, mechanical in origin; newly born columns of ommatidia serve as a template to spatially pattern cell flows that move the cells in the epithelium into position to form each new column of ommatidia. Cell flow is generated by a pressure gradient that is caused by a narrow zone of cell dilation precisely positioned behind the growing wavefront of ommatidia. The newly formed lattice grid of ommatidia cells are immobile, deflecting and focusing the flow of other cells. Thus, the self-organization of a regular pattern of cell fates in an epithelium is mechanically driven.</p>",
            "descriptionType": "Abstract"
          }
        ],
        "language": "en",
        "relatedIdentifier": [],
        "subject": [
          {
            "subjectValue": "Eye development"
          },
          {
            "subjectValue": "mechanics"
          },
          {
            "subjectValue": "Pattern formation"
          },
          {
            "subjectValue": "retinal development"
          }
        ],
        "managingOrganization": {
          "name": ""
        },
        "accessType": "PublicOnScreenAccessAndDownload",
        "accessDetails": {
          "description": ""
        },
        "rights": [
          {
            "rightsName": "No license available"
          }
        ],
        "publisher": {
          "publisherName": "Zenodo"
        },
        "size": [
          "2060.4 MB"
        ],
        "fundingReference": [
          {
            "funderName": "",
            "awardNumber": {
              "awardNumberValue": "",
              "awardURI": ""
            },
            "awardTitle": ""
          }
        ],
        "format": []
      },
      "datasetStructureDescription": {
        "schema": "https://schema.aireadi.org/v0.1.1/dataset_structure_description.json",
        "directoryList": [],
        "metadataFileList": []
      },
      "healthsheet": {}
    },
    "files": [],
    "data": {
      "size": 2160485990,
      "fileCount": 0,
      "viewCount": 0,
      "labelingMethod": "",
      "validationInfo": ""
    },
    "external": true,
    "externalUrl": "https://zenodo.org/records/6378171",
    "created": "1648010112"
  }
]